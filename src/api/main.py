#!/usr/bin/env python3
"""
FastAPI ê¸°ë°˜ í†µí•© RAG ì‹œìŠ¤í…œ
- CLIì™€ Streamlitì˜ core ë¡œì§ ì¬ì‚¬ìš©
- RESTful API ì œê³µ
- ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
"""

import os
import sys
import time
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(current_dir))

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
from core.config import LLM_MODELS, HUGGINGFACE_EMBEDDINGS_AVAILABLE, UPSTAGE_AVAILABLE, OLLAMA_AVAILABLE
from core.models import ModelFactory
from core.rag import create_rag_chain
from core.chat_history import ChatHistoryManager
from utils.elasticsearch import ElasticsearchManager

# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None


class RAGSystemManager:
    """RAG ì‹œìŠ¤í…œ ê´€ë¦¬ì - FastAPIìš©"""
    
    def __init__(self):
        self.es_manager = None
        self.model_factory = ModelFactory()
        self.rag_chain = None
        self.embedding_model = None
        self.llm_model = None
        self.model_choice = None
        self.top_k = 5
        self.is_initialized = False
        self.initialization_time = None
        
        # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.session_managers = {}
    
    def get_chat_manager(self, session_id: str = "default") -> ChatHistoryManager:
        """ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ë°˜í™˜"""
        if session_id not in self.session_managers:
            self.session_managers[session_id] = ChatHistoryManager(max_history=10)
        return self.session_managers[session_id]
    
    async def check_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± í™•ì¸ (ë¹„ë™ê¸°)"""
        issues = []
        
        if not HUGGINGFACE_EMBEDDINGS_AVAILABLE:
            issues.append("HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Elasticsearch ì„œë²„ ì—°ê²° í™•ì¸
        try:
            es_manager = ElasticsearchManager()
            is_connected, connection_msg = es_manager.check_connection()
            if not is_connected:
                issues.append(f"Elasticsearch ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {connection_msg}")
        except Exception as e:
            issues.append(f"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        
        # Ollama ì„œë²„ ì—°ê²° í™•ì¸
        try:
            from core.rag import check_ollama_connection
            ollama_connected, ollama_message = check_ollama_connection()
            if not ollama_connected:
                issues.append(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {ollama_message}")
        except Exception as e:
            issues.append(f"Ollama ì—°ê²° í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        
        return {
            "status": "ok" if not issues else "error",
            "issues": issues,
            "available_models": self.model_factory.get_available_models()
        }
    
    async def initialize_rag_system(self, model_choice: str, top_k: int = 5) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¹„ë™ê¸°)"""
        start_time = time.time()
        
        try:
            # 1. Elasticsearch ì—°ê²°
            self.es_manager = ElasticsearchManager()
            is_connected, connection_msg = self.es_manager.check_connection()
            if not is_connected:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {connection_msg}")
            
            # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            self.embedding_model = self.model_factory.create_embedding_model()
            if not self.embedding_model:
                raise Exception("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            # 3. LLM ëª¨ë¸ ë¡œë“œ
            self.llm_model, status = self.model_factory.create_llm_model(model_choice)
            if not self.llm_model:
                raise Exception(f"LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {status}")
            
            # 4. RAG ì²´ì¸ ìƒì„±
            self.rag_chain, success = create_rag_chain(
                embeddings=self.embedding_model,
                llm_model=self.llm_model,
                top_k=top_k
            )
            if not self.rag_chain:
                raise Exception(f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {success}")
            
            self.model_choice = model_choice
            self.top_k = top_k
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            return {
                "status": "success",
                "message": "RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ",
                "model": LLM_MODELS[model_choice]['name'],
                "top_k": top_k,
                "initialization_time": self.initialization_time
            }
            
        except Exception as e:
            self.is_initialized = False
            return {
                "status": "error",
                "message": f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            }
    
    async def process_query(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        if not self.is_initialized or not self.rag_chain:
            raise HTTPException(status_code=400, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        
        try:
            # ëŒ€í™” ê¸°ë¡ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            chat_manager = self.get_chat_manager(session_id)
            context_query = chat_manager.build_context_query(query)
            
            # RAG ì²´ì¸ ì‹¤í–‰
            response = self.rag_chain({"query": context_query})
            
            # ì‘ë‹µ ì²˜ë¦¬
            answer = self._extract_answer(response)
            processing_time = time.time() - start_time
            
            if answer:
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                chat_manager.add_chat(query, answer)
                
                return {
                    "status": "success",
                    "query": query,
                    "answer": answer,
                    "processing_time": processing_time,
                    "session_id": session_id,
                    "chat_history_count": chat_manager.get_history_count(),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "error",
                    "message": "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "processing_time": processing_time
                }
                
        except Exception as e:
            return {
                "status": "error",
                "message": f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                "processing_time": time.time() - start_time
            }
    
    def _extract_answer(self, response) -> Optional[str]:
        """ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ"""
        if isinstance(response, dict):
            if 'result' in response:
                return response['result']
            elif 'answer' in response:
                return response['answer']
            else:
                return str(response)
        elif hasattr(response, 'content'):
            return response.content
        elif isinstance(response, str):
            return response
        else:
            return str(response)
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "is_initialized": self.is_initialized,
            "model": LLM_MODELS.get(self.model_choice, {}).get('name') if self.model_choice else None,
            "model_key": self.model_choice,
            "top_k": self.top_k,
            "initialization_time": self.initialization_time,
            "active_sessions": len(self.session_managers),
            "available_models": self.model_factory.get_available_models()
        }


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    global rag_system
    rag_system = RAGSystemManager()
    print("ğŸš€ FastAPI RAG ì‹œìŠ¤í…œ ì‹œì‘")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ FastAPI RAG ì‹œìŠ¤í…œ ì¢…ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í†µí•© RAG ì‹œìŠ¤í…œ API",
    description="BGE-M3 ì„ë² ë”© + Elasticsearch + ë©€í‹° LLMì„ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Pydantic ëª¨ë¸ ì •ì˜
class QueryRequest(BaseModel):
    query: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©", min_length=1)
    session_id: str = Field(default="default", description="ì„¸ì…˜ ID")

class InitRequest(BaseModel):
    model: str = Field(..., description="ì‚¬ìš©í•  LLM ëª¨ë¸")
    top_k: int = Field(default=5, description="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ Kê°œ", ge=1, le=20)

class ChatHistoryResponse(BaseModel):
    history: List[Dict[str, Any]]
    count: int
    session_id: str


# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "í†µí•© RAG ì‹œìŠ¤í…œ API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/dependencies")
async def check_dependencies():
    """ì˜ì¡´ì„± í™•ì¸"""
    result = await rag_system.check_dependencies()
    if result["status"] == "error":
        raise HTTPException(status_code=503, detail=result)
    return result

@app.get("/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return {
        "models": rag_system.model_factory.get_available_models(),
        "status": "success"
    }

@app.post("/initialize")
async def initialize_system(request: InitRequest):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    available_models = rag_system.model_factory.get_available_models()
    
    if request.model not in available_models:
        raise HTTPException(
            status_code=400, 
            detail=f"ëª¨ë¸ '{request.model}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(available_models.keys())}"
        )
    
    result = await rag_system.initialize_rag_system(request.model, request.top_k)
    
    if result["status"] == "error":
        raise HTTPException(status_code=500, detail=result["message"])
    
    return result

@app.get("/status")
async def get_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return rag_system.get_system_info()

@app.post("/query")
async def process_query(request: QueryRequest):
    """ì§ˆì˜ ì²˜ë¦¬"""
    result = await rag_system.process_query(request.query, request.session_id)
    
    if result["status"] == "error":
        raise HTTPException(status_code=500, detail=result["message"])
    
    return result

@app.get("/chat/history/{session_id}")
async def get_chat_history(session_id: str = "default"):
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    chat_manager = rag_system.get_chat_manager(session_id)
    
    return ChatHistoryResponse(
        history=chat_manager.get_history(),
        count=chat_manager.get_history_count(),
        session_id=session_id
    )

@app.delete("/chat/history/{session_id}")
async def clear_chat_history(session_id: str = "default"):
    """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
    chat_manager = rag_system.get_chat_manager(session_id)
    chat_manager.clear_history()
    
    return {
        "status": "success",
        "message": f"ì„¸ì…˜ '{session_id}'ì˜ ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "session_id": session_id
    }

@app.get("/sessions")
async def get_active_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡"""
    return {
        "sessions": list(rag_system.session_managers.keys()),
        "count": len(rag_system.session_managers)
    }

@app.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """ì„¸ì…˜ ì‚­ì œ"""
    if session_id in rag_system.session_managers:
        del rag_system.session_managers[session_id]
        return {
            "status": "success",
            "message": f"ì„¸ì…˜ '{session_id}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    else:
        raise HTTPException(status_code=404, detail=f"ì„¸ì…˜ '{session_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    return JSONResponse(
        status_code=500,
        content={
            "status": "error",
            "message": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {str(exc)}",
            "timestamp": datetime.now().isoformat()
        }
    )


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="FastAPI RAG ì‹œìŠ¤í…œ ì„œë²„")
    parser.add_argument("--host", default="127.0.0.1", help="ì„œë²„ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8000, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--reload", action="store_true", help="ìë™ ë¦¬ë¡œë“œ í™œì„±í™”")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ FastAPI RAG ì„œë²„ ì‹œì‘: http://{args.host}:{args.port}")
    print(f"ğŸ“š API ë¬¸ì„œ: http://{args.host}:{args.port}/docs")
    
    uvicorn.run(
        "main:app",
        host=args.host,
        port=args.port,
        reload=args.reload
    )
