"""
í™˜ê²½ ì„¤ì • ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ë¡œì§
"""

import json
import os
import sys
import time
from typing import Dict, Any
from contextlib import asynccontextmanager

from core.config import LLM_MODELS, HUGGINGFACE_EMBEDDINGS_AVAILABLE, OLLAMA_AVAILABLE
from core.models import ModelFactory
from core.rag import create_llm_chain, create_rag_chain, create_retriever, create_enhanced_retriever, prompt_for_refined_query, prompt_for_query, prompt_for_context_summary
from core.chat_history import ChatHistoryManager
from utils.elasticsearch import ElasticsearchManager


def add_related_links(answer_text, question_text=""):
    """
    ë‹µë³€ì— ê´€ë ¨ ë§í¬ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    # í‚¤ì›Œë“œ-ë§í¬ ë§¤í•‘
    keyword_links = {
        'ì¹´ë“œë°œê¸‰': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0792',
        'ì´ìš©í•œë„': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind1113',
        'ê²°ì œì¼': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0618',
        'ì´ìš©ê¸°ê°„': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0623',
        'ë¦¬ë³¼ë¹™': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind1187',
        'êµí†µì¹´ë“œ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0649',
        'ì‹ ìš©ì¹´ë“œ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0667',
        'í¬ì¸íŠ¸': 'https://isson.bccard.com/3rd/openSigninFormPage.jsp?UURL=https%3A%2F%2Fisson.bccard.com%2Fnls3%2Ffcs&NONCE=tvaQoSYB9J90I5r1z%2Bu2gNqawETc7ThhYPlG%2Fz308%2FoRCuqBsL%2F6dQjzXnAfZ2CjYEisW42xcJTSYKyTiQfcwQ%3D%3D&FORM=777',
        'í˜œíƒ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind1200',
        'ëŒ€ì¶œ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0667',
        'í• ë¶€': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0667',
        'ì—°ì²´': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0671',
        'ì†Œë“ê³µì œ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0670',
        'í•´ì™¸': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0650',
        'ì¥ì• ': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0791',
        'ë¶„ì‹¤': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind0901',
        'ë¶€ê°€ì„œë¹„ìŠ¤': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind1114',
        'ì—°ì²´ ì ˆì°¨': 'https://www.bccard.com/app/card/ContentsLinkActn.do?pgm_id=ind1115'
    }
    
    # ì´ë¯¸ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
    if '---' in answer_text and 'ìì„¸í•œ ì‚¬í•­ì„' in answer_text:
        return answer_text
    
    # ë‹µë³€ê³¼ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
    found_keywords = []
    search_text = (answer_text + " " + question_text).lower()
    
    for keyword, link in keyword_links.items():
        if keyword in search_text:
            found_keywords.append((keyword, link))
    
    # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì„ íƒ (ì§ˆë¬¸ ìš°ì„ ìˆœìœ„)
    if found_keywords:
        # ì§ˆë¬¸ì— ìˆëŠ” í‚¤ì›Œë“œ ìš°ì„ 
        question_keywords = []
        answer_keywords = []
        
        for keyword, link in found_keywords:
            if keyword in question_text.lower():
                question_keywords.append((keyword, link))
            else:
                answer_keywords.append((keyword, link))
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ + ë‹µë³€ í‚¤ì›Œë“œ ì¡°í•©í•´ì„œ ìµœëŒ€ 3ê°œ
        selected_keywords = (question_keywords + answer_keywords)[:3]
        
        if selected_keywords:
            link_section = "\n\n---\nìì„¸í•œ ì‚¬í•­ì„ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:\n"
            for keyword, link in selected_keywords:
                link_section += f"[{keyword}]({link})\n"
            
            return answer_text + link_section
    
    return answer_text


def load_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    try:
        from dotenv import load_dotenv
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env.prod íŒŒì¼ ë¡œë“œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(os.path.dirname(current_dir))
        env_path = os.path.join(root_dir, '.env.prod')
        load_dotenv(env_path)
        print(f"ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: {env_path}")
        print(f"ğŸ”— OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'Not set')}")
        
        # LangSmith íŠ¸ë ˆì´ì‹± ëª…ì‹œì  ë¹„í™œì„±í™”
        os.environ["LANGCHAIN_TRACING_V2"] = "false"
        if "LANGCHAIN_API_KEY" in os.environ:
            del os.environ["LANGCHAIN_API_KEY"]
        if "LANGSMITH_API_KEY" in os.environ:
            del os.environ["LANGSMITH_API_KEY"]
        print("ğŸš« LangSmith íŠ¸ë ˆì´ì‹± ë¹„í™œì„±í™”ë¨")
        
    except ImportError:
        print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")


def setup_paths():
    """í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.dirname(current_dir)
    if src_dir not in sys.path:
        sys.path.append(src_dir)


def get_langfuse_config():
    """Langfuse ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Dockerì—ì„œ ëª¨ë“ˆë¡œ ì‹¤í–‰í•  ë•Œ
        from api.langfuse_config import get_langfuse_manager, get_langfuse_callback
    except ImportError:
        try:
            # ë¡œì»¬ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ
            sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'api'))
            from langfuse_config import get_langfuse_manager, get_langfuse_callback
        except ImportError:
            print("âš ï¸ Langfuse ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
    
    return get_langfuse_manager, get_langfuse_callback


def check_elasticsearch_availability():
    """Elasticsearch ê°€ìš©ì„± í™•ì¸"""
    try:
        from elasticsearch import Elasticsearch
        return True
    except ImportError:
        print("âš ï¸ Elasticsearchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install elasticsearch")
        return False


class FastAPIRAGSystem:
    """FastAPIìš© RAG ì‹œìŠ¤í…œ - unified_rag_cli.pyì™€ ë™ì¼í•œ ë¡œì§"""
    
    def __init__(self):
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸
        self.es_manager = None
        self.model_factory = ModelFactory()
        self.rag_chain = None
        self.embedding_model = None
        self.llm_model = None
        self.model_choice = None
        self.top_k = 5
        self.retriever = None
        self.llm_chain = None
        
        # ë¯¸ë¦¬ ìƒì„±ëœ ì²´ì¸ë“¤ (ìµœì í™”)
        self.refinement_chain = None
        self.qa_chain = None
        self.summary_chain = None
        
        # ì™¸ë¶€ ì„œë¹„ìŠ¤ ë§¤ë‹ˆì €
        get_langfuse_manager, _ = get_langfuse_config()
        self.langfuse_manager = get_langfuse_manager() if get_langfuse_manager else None
        
        # ìƒíƒœ ê´€ë¦¬
        self.session_managers = {}
        self.is_initialized = False
        self.initialization_time = None

        # ì¸ë±ìŠ¤ ê´€ë¦¬
        self.current_index_name = os.getenv("INDEX_NAME", "yang_deotis_rag")
    
    # =============================================================================
    # ì„¸ì…˜ ë° ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œ
    # =============================================================================
    
    def get_chat_manager(self, session_id: str = "default") -> ChatHistoryManager:
        """ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ë°˜í™˜"""
        if session_id not in self.session_managers:
            self.session_managers[session_id] = ChatHistoryManager(max_history=10)
        return self.session_managers[session_id]
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "is_initialized": self.is_initialized,
            "model": LLM_MODELS.get(self.model_choice, {}).get('name') if self.model_choice else None,
            "model_key": self.model_choice,
            "top_k": self.top_k,
            "initialization_time": self.initialization_time,
            "active_sessions": len(self.session_managers),
            "available_models": self.model_factory.get_available_models(),
            "langfuse_status": self.langfuse_manager.get_status() if self.langfuse_manager else {"available": False}
        }
    
    # =============================================================================
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì˜ì¡´ì„± í™•ì¸
    # =============================================================================
    
    async def check_dependencies_async(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± í™•ì¸ (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        
        def _check_dependencies():
            issues = []
            
            if not check_elasticsearch_availability():
                issues.append("Elasticsearch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not HUGGINGFACE_EMBEDDINGS_AVAILABLE:
                issues.append("HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Elasticsearch ì„œë²„ ì—°ê²° í™•ì¸
            try:
                es_manager = ElasticsearchManager()
                is_connected, connection_msg = es_manager.check_connection()
                if not is_connected:
                    issues.append(f"Elasticsearch ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {connection_msg}")
            except Exception as e:
                issues.append(f"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            
            # Ollama ì„œë²„ ì—°ê²° í™•ì¸
            try:
                from core.rag import check_ollama_connection
                ollama_connected, ollama_message = check_ollama_connection()
                if not ollama_connected:
                    issues.append(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {ollama_message}")
            except Exception as e:
                issues.append(f"Ollama ì—°ê²° í™•ì¸ ì˜¤ë¥˜: {str(e)}")
            
            return {
                "status": "ok" if not issues else "error",
                "issues": issues,
                "issue_count": len(issues)
            }
        
        return await asyncio.get_event_loop().run_in_executor(None, _check_dependencies)
    
    async def initialize_rag_system_async(self, model_choice: str, top_k: int = 5) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        import time
        
        def _initialize_rag_system():
            try:
                start_time = time.time()
                
                self.model_choice = model_choice
                self.top_k = top_k
                
                # Elasticsearch ê´€ë¦¬ì ì´ˆê¸°í™”
                self.es_manager = ElasticsearchManager()
                
                # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
                self.embedding_model = self.model_factory.create_embedding_model()
                if not self.embedding_model:
                    return {
                        "status": "error",
                        "message": "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"
                    }
                
                # LLM ëª¨ë¸ ë¡œë“œ
                self.llm_model, status = self.model_factory.create_llm_model(model_choice)
                if not self.llm_model:
                    return {
                        "status": "error",
                        "message": f"LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {status}"
                    }
                
                # RAG ì²´ì¸ ìƒì„± (Langfuse ì½œë°± í¬í•¨)
                _, get_langfuse_callback = get_langfuse_config()
                langfuse_callback = get_langfuse_callback() if get_langfuse_callback else None
                callbacks = [langfuse_callback] if langfuse_callback else None
                
                self.rag_chain, success_or_error = create_rag_chain(
                    embeddings=self.embedding_model,
                    llm_model=self.llm_model,
                    top_k=top_k,
                    callbacks=callbacks
                )

                # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ Retriever ìƒì„±
                self.retriever = create_enhanced_retriever(
                    embedding_model=self.embedding_model,
                    top_k=top_k
                )

                # ë¯¸ë¦¬ ì‚¬ìš©í•  ì²´ì¸ë“¤ ìƒì„± (ìµœì í™”)
                self.refinement_chain = create_llm_chain(
                    self.llm_model, 
                    prompt_for_refined_query,
                    input_variables=["userinfo", "question", "context"]
                )
                
                self.qa_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_query,
                    input_variables=["question", "context"]
                )
                
                self.summary_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_context_summary,
                    input_variables=["context"]
                )

                # LLM ì²´ì¸ ìƒì„±
                try:
                    self.llm_chain = create_llm_chain(
                        llm_model=self.llm_model,
                        prompt_template="""{context}, {question}"""
                    )
                except Exception as e:
                    print(f"âŒ LLM ì²´ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    self.llm_chain = None

                if self.rag_chain:
                    self.is_initialized = True
                    self.initialization_time = time.time() - start_time
                    return {
                        "status": "success",
                        "message": "RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ",
                        "model": LLM_MODELS[model_choice]['name'],
                        "top_k": top_k,
                        "initialization_time": self.initialization_time
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {success_or_error}"
                    }
                    
            except Exception as e:
                self.is_initialized = False
                return {
                    "status": "error",
                    "message": f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                }
        
        return await asyncio.get_event_loop().run_in_executor(None, _initialize_rag_system)
    
    # =============================================================================
    # ë¬¸ì„œ ì²˜ë¦¬ ë° ê²€ìƒ‰ ìœ í‹¸ë¦¬í‹°
    # =============================================================================
    
    def convert_docs_with_table_preservation(self, merged_docs):
        """í‘œ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ” ê³ ê¸‰ ë¬¸ì„œ ë³€í™˜"""
        enhanced_sections = []
        
        for i, doc in enumerate(merged_docs):
            content = getattr(doc, "page_content", str(doc))
            metadata = getattr(doc, "metadata", {})
            
            # ë¬¸ì„œ í—¤ë” ì •ë³´
            doc_header = f"""
    ğŸ“„ **ë¬¸ì„œ {i+1}**: {metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼')}
    ğŸ“Š **í˜ì´ì§€**: {metadata.get('page_number', 'N/A')}
    ğŸ·ï¸ **ì¹´í…Œê³ ë¦¬**: {metadata.get('category', 'ì¼ë°˜')}
    â­ **ê´€ë ¨ì„± ì ìˆ˜**: {getattr(doc, '_score', 'N/A')}
    """
            
            # í‘œ í¬í•¨ ì—¬ë¶€ í™•ì¸ ë° ì²˜ë¦¬
            if self._has_table_structure(content):
                processed_content = self._preserve_table_structure(content)
                doc_section = f"""{doc_header}
    ğŸ“Š **[í‘œ ë°ì´í„° í¬í•¨ ë¬¸ì„œ]**

    {processed_content}

    ğŸ“Š **[í‘œ ë°ì´í„° ë]**
    """
            else:
                doc_section = f"""{doc_header}
    ğŸ“ **[ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ]**

    {content.strip()}

    ğŸ“ **[ë¬¸ì„œ ë]**
    """
            
            enhanced_sections.append(doc_section)
        
        return "\n\n" + "="*80 + "\n\n".join(enhanced_sections)

    def _has_table_structure(self, content):
        """í‘œ êµ¬ì¡° í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        table_indicators = [
            "|" in content and ("---" in content or ":-:" in content),
            content.count("\t") > 5,  # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í‘œ
            re.search(r'\d+\.\s+.*?\s+\d+', content),  # ë²ˆí˜¸ + í…ìŠ¤íŠ¸ + ìˆ«ì íŒ¨í„´
        ]
        return any(table_indicators)

    def _preserve_table_structure(self, content):
        """í‘œ êµ¬ì¡° ë³´ì¡´ ì²˜ë¦¬"""
        # ë§ˆí¬ë‹¤ìš´ í‘œ í˜•íƒœë¡œ ë³€í™˜
        if "|" in content:
            return self._format_markdown_table(content)
        
        # íƒ­ êµ¬ë¶„ í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        if "\t" in content:
            return self._convert_tab_to_markdown(content)
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        return self._structure_plain_text_table(content)

    def _format_markdown_table(self, content):
        """ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ ì •ë¦¬"""
        lines = content.split('\n')
        table_lines = []
        
        for line in lines:
            if '|' in line:
                # í‘œ ë¼ì¸ ì •ë¦¬ ë° í¬ë§·íŒ…
                cells = [cell.strip() for cell in line.split('|')]
                formatted_line = "| " + " | ".join(cells) + " |"
                table_lines.append(formatted_line)
            elif line.strip():
                table_lines.append(line)
        
        return '\n'.join(table_lines)

    # =============================================================================
    # ì§ˆì˜ ë¶„ì„ ë° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
    # =============================================================================
    
    def _extract_json_from_markdown(self, text):
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ"""
        import re
        json_pattern = r'```(?:json)?\s*\n?(.*?)\n?```'
        match = re.search(json_pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return None
    
    def _parse_query_analysis(self, response_str):
        """ìƒˆë¡œìš´ JSON ì‘ë‹µ í˜•ì‹ì„ íŒŒì‹±"""
        # 1. ì§ì ‘ JSON íŒŒì‹± ì‹œë„
        try:
            parsed_json = json.loads(response_str)
            return (
                parsed_json.get('refined_query'),
                parsed_json.get('action'),
                parsed_json.get('classification'),
                parsed_json.get('is_new_topic'),
                parsed_json.get('reasoning')
            )
        except json.JSONDecodeError:
            pass
        
        # 2. ë§ˆí¬ë‹¤ìš´ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
        json_content = self._extract_json_from_markdown(response_str)
        if json_content:
            try:
                parsed_json = json.loads(json_content)
                return (
                    parsed_json.get('refined_query'),
                    parsed_json.get('action'),
                    parsed_json.get('classification'),
                    parsed_json.get('is_new_topic'),
                    parsed_json.get('reasoning')
                )
            except json.JSONDecodeError:
                pass
        
        # 3. í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„ (fallback)
        import re
        
        # refined_query ì¶”ì¶œ
        query_match = re.search(r'refined_query["\s:]*([^,\n}]+)', response_str, re.IGNORECASE)
        extracted_query = query_match.group(1).strip().strip('"\'') if query_match else None
        
        # action ì¶”ì¶œ
        action_match = re.search(r'action["\s:]*([^,\n}]+)', response_str, re.IGNORECASE)
        extracted_action = action_match.group(1).strip().strip('"\'') if action_match else None
        
        # classification ì¶”ì¶œ
        class_match = re.search(r'classification["\s:]*([^,\n}]+)', response_str, re.IGNORECASE)
        extracted_class = class_match.group(1).strip().strip('"\'') if class_match else None
        
        # is_new_topic ì¶”ì¶œ
        topic_match = re.search(r'is_new_topic["\s:]*([^,\n}]+)', response_str, re.IGNORECASE)
        extracted_topic = None
        if topic_match:
            topic_str = topic_match.group(1).strip().strip('"\'').lower()
            extracted_topic = topic_str == 'true'
        
        # reasoning ì¶”ì¶œ
        reason_match = re.search(r'reasoning["\s:]*([^}]+)', response_str, re.IGNORECASE)
        extracted_reason = reason_match.group(1).strip().strip('"\'') if reason_match else None
        
        return extracted_query, extracted_action, extracted_class, extracted_topic, extracted_reason

    def _create_personalized_context(self, docs_text: str, user_data: Dict[str, Any] = None) -> str:
        """ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        personalized_context = docs_text
#         if user_data and isinstance(user_data, dict):
#             user_context = f"""
# ###ì‚¬ìš©ì ì •ë³´:
# - ì´ë¦„: {user_data.get('userName', 'N/A')}
# - ì—°ì†Œë“: {user_data.get('income', 'N/A')}ì›

# ###ê²€ìƒ‰ë¬¸ì„œ:
# """
#             # ë³´ìœ  ì¹´ë“œ ì •ë³´ ì¶”ê°€
#             user_data_obj = user_data.get('data', {})
#             if user_data_obj and isinstance(user_data_obj, dict) and user_data_obj.get('ownCardArr'):
#                 user_context += "\në³´ìœ  ì¹´ë“œ:\n"
#                 for card in user_data_obj['ownCardArr']:
#                     if isinstance(card, dict):
#                         user_context += f"- {card.get('bank', 'N/A')} {card.get('name', 'N/A')} ({card.get('type', 'N/A')}, ê²°ì œì¼: {card.get('paymentDate', 'N/A')}ì¼)\n"
#             else:
#                 print(f"ï¿½ [DEBUG] ì¹´ë“œ ì •ë³´ ì—†ìŒ ë˜ëŠ” ì¡°ê±´ ë¶ˆë§Œì¡±")

#             print(f"ï¿½ğŸ” ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸: {user_context}")
#             personalized_context = user_context + "\n\nê´€ë ¨ ë¬¸ì„œ:\n" + docs_text
#             print(f"ğŸ” ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(personalized_context)} ë¬¸ì")
#         else:
#             print(f"ğŸ› [DEBUG] user_data ì¡°ê±´ ì‹¤íŒ¨! user_dataê°€ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹˜")
        
        return personalized_context

    def _log_to_langfuse(self, trace, operation: str, input_data: str, output_data: str, metadata: Dict[str, Any]):
        """Langfuse ë¡œê¹… í—¬í¼"""
        if trace and self.langfuse_manager:
            self.langfuse_manager.log_generation(
                trace_context=trace.get('trace_context'),
                name=operation,
                input=input_data,
                output=output_data,
                metadata=metadata
            )

    # =============================================================================
    # ë©”ì¸ ì§ˆì˜ ì²˜ë¦¬ ë©”ì„œë“œ
    # =============================================================================



    async def process_query_async(self, query: str, session_id: str = "default", user_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (ë¹„ë™ê¸° ë²„ì „, ì˜ë¯¸+í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©)"""
        if not self.is_initialized or not self.rag_chain:
            return {
                "status": "error",
                "message": "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        import asyncio
        import time

        def _process_query():
            try:
                start_time = time.time()
                
                # user_dataê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©
                current_user_data = user_data
                if not current_user_data or not isinstance(current_user_data, dict):
                    print("ğŸ”§ [DEBUG] user_dataê°€ ì—†ì–´ì„œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©")
                    current_user_data = {  
                        "userId": "bccard",  
                        "userName": "ê¹€ëª…ì •",  
                        "loginTime": "2025-08-27T14:23:45.123Z",
                        "isAuthenticated": True,
                        "age": "27",
                        "income": "77,511,577",
                        "data": {
                            "email": "kmj@deotis.co.kr",
                            "phone": "010-1234-5678",
                            "ownCardArr": [
                                {
                                    "bank": "ìš°ë¦¬ì¹´ë“œ",
                                    "paymentDate": "4",
                                    "type": "ì‹ ìš©ì¹´ë“œ"
                                }
                            ]
                        }
                    }
                else:
                    print("ğŸ”§ [DEBUG] ì „ë‹¬ë°›ì€ user_data ì‚¬ìš©")

                trace = self._create_langfuse_trace(session_id, current_user_data)
                
                # ì§ˆì˜ ë¶„ì„ ë° ì¬ì •ì˜
                analysis_result = self._analyze_and_refine_query(query, session_id, current_user_data)

                # DIRECT_ANSWER ì²˜ë¦¬
                if analysis_result['action'] == "DIRECT_ANSWER":
                    return self._handle_direct_answer(query, analysis_result, session_id, trace, start_time)
                
                # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
                return self._handle_search_answer(query, analysis_result, session_id, current_user_data, trace, start_time)
                
            except Exception as e:
                return self._handle_error(e, trace if 'trace' in locals() else None, start_time)

        return await asyncio.get_event_loop().run_in_executor(None, _process_query)
    
    def _create_langfuse_trace(self, session_id: str, user_data: Dict[str, Any] = None):
        """Langfuse íŠ¸ë ˆì´ìŠ¤ ìƒì„±"""
        trace = None
        if self.langfuse_manager:
            trace_metadata = {
                "session_id": session_id,
                "model": self.model_choice,
                "top_k": self.top_k
            }
            if user_data and isinstance(user_data, dict):
                trace_metadata.update({
                    "user_id": user_data.get("userId"),
                    "user_name": user_data.get("userName"),
                    "user_age": user_data.get("age"),
                    "user_income": user_data.get("income"),
                    "is_authenticated": user_data.get("isAuthenticated"),
                    "login_time": user_data.get("loginTime")
                })
            
            trace = self.langfuse_manager.create_trace(
                name="rag_query",
                metadata=trace_metadata
            )
        return trace

    def _analyze_and_refine_query(self, query: str, session_id: str, user_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì§ˆì˜ ë¶„ì„ ë° ì¬ì •ì˜"""
        chat_manager = self.get_chat_manager(session_id)
        history = chat_manager.build_history()
        
        print(f"ğŸ” ëŒ€í™” ê¸°ë¡: {history}")
        print(f"ğŸ” ì›ë³¸ ì§ˆì˜: {query}")
        
        userinfo_str = json.dumps(user_data, ensure_ascii=False, indent=2)
        
        try:
            refined_query_str = self.refinement_chain.run({
                "question": query, 
                "context": history, 
                "userinfo": userinfo_str
            })
        except Exception as chain_error:
            print(f"âŒ refinement_chain ì‹¤í–‰ ì˜¤ë¥˜: {str(chain_error)}")
            refined_query_str = query
            
        print(f"ğŸ” ì§ˆì˜ ë¶„ì„ ê²°ê³¼ (ì›ë³¸): {refined_query_str}")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        result = {
            "refined_query": query,
            "action": "SEARCH",
            "classification": "GENERAL",
            "is_new_topic": True,
            "reasoning": ""
        }
        
        try:
            parsed_result = self._parse_query_analysis(refined_query_str)
            parsed_refined_query, parsed_action, parsed_classification, parsed_is_new_topic, parsed_reasoning = parsed_result
            
            # íŒŒì‹±ëœ ê°’ë“¤ì„ ì•ˆì „í•˜ê²Œ ì ìš©
            if parsed_refined_query:
                result["refined_query"] = parsed_refined_query
                print(f"ğŸ” ì •ì œëœ ì§ˆì˜: {parsed_refined_query}")
            
            if parsed_action in ["SEARCH", "DIRECT_ANSWER"]:
                result["action"] = parsed_action
                print(f"ğŸ“‹ ì²˜ë¦¬ ë°©ì‹: {parsed_action}")
            
            if parsed_classification in ["GENERAL", "HYBRID", "USER_INFO_ONLY"]:
                result["classification"] = parsed_classification
                print(f"ğŸ·ï¸ ì§ˆì˜ ë¶„ë¥˜: {parsed_classification}")
            
            if parsed_is_new_topic is not None:
                result["is_new_topic"] = parsed_is_new_topic
                print(f"ğŸ†• ìƒˆë¡œìš´ ì£¼ì œ: {parsed_is_new_topic}")
            
            if parsed_reasoning:
                result["reasoning"] = parsed_reasoning
                print(f"ğŸ’­ ë¶„ì„ ê·¼ê±°: {parsed_reasoning}")
            
            # ìƒˆë¡œìš´ ì£¼ì œì¸ ê²½ìš° ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
            if result["is_new_topic"]:
                chat_manager.clear_history()
                print("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨ (ìƒˆë¡œìš´ ì£¼ì œ)")
                
        except Exception as e:
            print(f"âŒ ì§ˆì˜ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬ ì§„í–‰")
        
        return result
    
    def _handle_direct_answer(self, query: str, analysis_result: Dict[str, Any], session_id: str, trace, start_time: float) -> Dict[str, Any]:
        """DIRECT_ANSWER ì•¡ì…˜ ì²˜ë¦¬"""
        processing_time = time.time() - start_time
        direct_answer = analysis_result["refined_query"]
        
        print(f"ğŸ” ì§ì ‘ ë‹µë³€ ëª¨ë“œ (í”„ë¡¬í”„íŠ¸ ìƒì„±): {direct_answer}")
        
        # ğŸ¯ ê´€ë ¨ ë§í¬ ìë™ ì¶”ê°€
        enhanced_answer = add_related_links(direct_answer, query)
    
        # ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ê°€
        chat_manager = self.get_chat_manager(session_id)
        chat_manager.add_chat(query, enhanced_answer)
        
        # Langfuse ë¡œê¹…
        self._log_to_langfuse(trace, "direct_answer_generation", query, direct_answer, {
            "processing_time": processing_time,
            "mode": "direct_answer",
            "classification": analysis_result["classification"],
            "model": self.model_choice
        })
        
        return {
            "status": "success",
            "answer": enhanced_answer,
            "query": query,
            "refined_query": analysis_result["refined_query"],
            "classification": analysis_result["classification"],
            "action": analysis_result["action"],
            "reasoning": analysis_result["reasoning"],
            "session_id": session_id,
            "processing_time": processing_time,
            "retrieved_docs": []
        }
    
    def _handle_search_answer(self, query: str, analysis_result: Dict[str, Any], session_id: str, user_data: Dict[str, Any], trace, start_time: float) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ì²˜ë¦¬"""
        # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        merged_docs = self.retriever.get_relevant_documents(analysis_result["refined_query"])
        print(f"ğŸ” ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(merged_docs)}")

        # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        docs_text = "\n\n---\n\n".join([
            getattr(doc, "page_content", str(doc)) for doc in merged_docs
        ])
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ë³€í™˜ ë‹¤ë¥¸ ì‹œë„
        # docs_text = self.convert_docs_with_table_preservation(merged_docs)

        print(f"ğŸ” ìµœì¢… ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docs_text)} ë¬¸ì")

        # ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        personalized_context = self._create_personalized_context(docs_text, user_data)

        # ê²€ìƒ‰ëœ ìë£Œì™€ ì¬ì •ì˜ ì§ˆë¬¸ì„ LLMì— ë„˜ê²¨ì„œ ë‹µë³€ ìƒì„±
        result = self.qa_chain.invoke({
            "question": analysis_result["refined_query"], 
            "context": personalized_context
        })

        print(f"ğŸ” RAG ì²´ì¸ ì‘ë‹µ êµ¬ì¡°: {result}")
        processing_time = time.time() - start_time

        # ë‹µë³€ ì¶”ì¶œ
        if result and ('answer' in result or 'result' in result or 'text' in result):
            original_answer = result.get('answer') or result.get('result') or result.get('text')
            print(f"ğŸ” ì›ë³¸ ë‹µë³€: {original_answer}")
            
            # ğŸ¯ ê´€ë ¨ ë§í¬ ìë™ ì¶”ê°€
            enhanced_answer = add_related_links(original_answer, query)
            print(f"ğŸ”— ë§í¬ ì¶”ê°€ëœ ìµœì¢… ë‹µë³€: {enhanced_answer}")
            
            # ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ê°€
            chat_manager = self.get_chat_manager(session_id)
            chat_manager.add_chat(query, enhanced_answer)

            # Langfuse ë¡œê¹…
            self._log_to_langfuse(trace, "rag_generation", query, enhanced_answer, {
                "processing_time": processing_time,
                "retrieved_docs_count": len(merged_docs),
                "model": self.model_choice
            })

            # retrieved_docs êµ¬ì„±
            retrieved_docs = []
            for doc in merged_docs:
                retrieved_docs.append({
                    "type": "enhanced_hybrid",
                    "content": getattr(doc, "page_content", str(doc)),
                    "metadata": getattr(doc, "metadata", {})
                })

            return {
                "status": "success",
                "answer": enhanced_answer,
                "query": query,
                "refined_query": analysis_result["refined_query"],
                "classification": analysis_result["classification"],
                "action": analysis_result["action"],
                "reasoning": analysis_result["reasoning"],
                "session_id": session_id,
                "username": user_data.get("userName", "") if user_data and isinstance(user_data, dict) else "",
                "processing_time": processing_time,
                "retrieved_docs": retrieved_docs
            }
        else:
            return self._handle_no_answer_error(result, trace, processing_time)
    
    def _handle_no_answer_error(self, result, trace, processing_time: float) -> Dict[str, Any]:
        """ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì²˜ë¦¬"""
        if trace and self.langfuse_manager:
            self.langfuse_manager.log_event(
                trace_context=trace.get('trace_context'),
                name="rag_error",
                metadata={
                    "error": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ì‘ë‹µ êµ¬ì¡°: {result}",
                    "processing_time": processing_time
                }
            )

        return {
            "status": "error",
            "message": f"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ êµ¬ì¡°: {result}",
            "processing_time": processing_time
        }
    
    def _handle_error(self, error: Exception, trace, start_time: float) -> Dict[str, Any]:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        processing_time = time.time() - start_time
        
        if trace and self.langfuse_manager:
            self.langfuse_manager.log_event(
                trace_context=trace.get('trace_context'),
                name="rag_exception",
                metadata={
                    "error": str(error),
                    "processing_time": processing_time
                }
            )

        return {
            "status": "error",
            "message": f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(error)}",
            "processing_time": processing_time
        }

# =============================================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
# =============================================================================
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "is_initialized": self.is_initialized,
            "model": LLM_MODELS.get(self.model_choice, {}).get('name') if self.model_choice else None,
            "model_key": self.model_choice,
            "top_k": self.top_k,
            "initialization_time": self.initialization_time,
            "active_sessions": len(self.session_managers),
            "available_models": self.model_factory.get_available_models(),
            "langfuse_status": self.langfuse_manager.get_status() if self.langfuse_manager else {"available": False}
        }

    # =============================================================================
    # Elasticsearch ì¸ë±ìŠ¤ ê´€ë¦¬ ë©”ì„œë“œë“¤
    # =============================================================================
    
    def change_elasticsearch_index(self, new_index_name: str) -> bool:
        """Elasticsearch ì¸ë±ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ë©”ì„œë“œ (ì™„ì „í•œ ë²„ì „)"""
        try:
            # ìƒˆ ì¸ë±ìŠ¤ ì´ë¦„ ìœ íš¨ì„± ê²€ì‚¬
            if not new_index_name or not isinstance(new_index_name, str):
                raise ValueError("ì¸ë±ìŠ¤ ì´ë¦„ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # íŠ¹ìˆ˜ë¬¸ì ê²€ì‚¬ (Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„ ê·œì¹™)
            import re
            if not re.match(r'^[a-z0-9_-]+$', new_index_name.lower()):
                raise ValueError("ì¸ë±ìŠ¤ ì´ë¦„ì€ ì†Œë¬¸ì, ìˆ«ì, '_', '-'ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            old_index = self.current_index_name
            print(f"ğŸ”„ ì¸ë±ìŠ¤ ë³€ê²½ ì‹œì‘: '{old_index}' â†’ '{new_index_name}'")
            
            # ğŸ¯ í•µì‹¬: Retriever ì™„ì „ ì¬ìƒì„±
            try:
                # ê¸°ì¡´ retriever ë°±ì—… (ì‹¤íŒ¨ ì‹œ ë³µì›ìš©)
                old_retriever = self.retriever if hasattr(self, 'retriever') else None
                
                # ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if not self.is_initialized or not self.embedding_model:
                    print("âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € initializeë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.")
                    return False
                
                # ìƒˆ ì¸ë±ìŠ¤ë¡œ Enhanced Retriever ì¬ìƒì„±
                from core.rag import create_enhanced_retriever
                
                print(f"ğŸ” ìƒˆ Retriever ìƒì„± ì¤‘... (ì¸ë±ìŠ¤: {new_index_name})")
                
                # index_name íŒŒë¼ë¯¸í„°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                new_retriever = create_enhanced_retriever(
                    embedding_model=self.embedding_model,
                    top_k=self.top_k,
                    index_name=new_index_name  # ğŸ¯ ìƒˆ ì¸ë±ìŠ¤ ëª…ì‹œì  ì „ë‹¬
                )
                
                if new_retriever is None:
                    raise Exception(f"ì¸ë±ìŠ¤ '{new_index_name}'ë¡œ Retriever ìƒì„± ì‹¤íŒ¨")
                
                # ì„±ê³µí•˜ë©´ êµì²´
                self.retriever = new_retriever
                self.current_index_name = new_index_name
                
                print(f"âœ… ì¸ë±ìŠ¤ ë³€ê²½ ì™„ë£Œ: '{new_index_name}'")
                print(f"âœ… Retriever ì¬ìƒì„± ì™„ë£Œ")
                
                return True
                
            except Exception as retriever_error:
                # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ retriever ë³µì›
                print(f"âŒ Retriever ì¬ìƒì„± ì‹¤íŒ¨: {str(retriever_error)}")
                print(f"ğŸ”„ ê¸°ì¡´ ì¸ë±ìŠ¤ '{old_index}'ë¡œ ë³µì›")
                
                if old_retriever is not None:
                    self.retriever = old_retriever
                
                return False
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def get_current_index(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„ì„ ë°˜í™˜"""
        return self.current_index_name
    
    def list_available_indices(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Elasticsearch ì¸ë±ìŠ¤ ëª©ë¡ì„ ë°˜í™˜"""
        try:
            if hasattr(self, 'retriever') and self.retriever and hasattr(self.retriever, 'client'):
                es_client = self.retriever.client
                if es_client and hasattr(es_client, 'indices'):
                    # ëª¨ë“  ì¸ë±ìŠ¤ ì¡°íšŒ
                    indices = es_client.indices.get_alias(index="*")
                    return list(indices.keys())
            return ["test2_rag", "yang_deotis_rag"]  # ê¸°ë³¸ê°’
        except Exception as e:
            print(f"ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return ["test2_rag", "yang_deotis_rag"]  # ê¸°ë³¸ê°’


@asynccontextmanager
async def lifespan(app):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    global rag_system
    rag_system = FastAPIRAGSystem()
    print("ğŸš€ FastAPI RAG ì‹œìŠ¤í…œ ì‹œì‘")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ FastAPI RAG ì‹œìŠ¤í…œ ì¢…ë£Œ")


# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None


# =============================================================================
# Langfuse ì„¤ì • í•¨ìˆ˜ë“¤
# =============================================================================


# Langfuse í•¨ìˆ˜ë“¤ì„ export
def get_langfuse_manager():
    """Langfuse ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°"""
    try:
        get_langfuse_manager_func, _ = get_langfuse_config()
        return get_langfuse_manager_func() if get_langfuse_manager_func else None
    except Exception:
        return None


def get_langfuse_callback():
    """Langfuse ì½œë°± ê°€ì ¸ì˜¤ê¸°"""
    try:
        _, get_langfuse_callback_func = get_langfuse_config()
        return get_langfuse_callback_func() if get_langfuse_callback_func else None
    except Exception:
        return None
