"""
í™˜ê²½ ì„¤ì • ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ë¡œì§
"""

import os
import sys
from typing import Dict, Any
from contextlib import asynccontextmanager

from core.config import LLM_MODELS, HUGGINGFACE_EMBEDDINGS_AVAILABLE, OLLAMA_AVAILABLE
from core.models import ModelFactory
from core.rag import create_llm_chain, create_rag_chain, create_retriever, create_enhanced_retriever, prompt_for_refined_query, prompt_for_query, prompt_for_context_summary
from core.chat_history import ChatHistoryManager
from utils.elasticsearch import ElasticsearchManager


def load_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    try:
        from dotenv import load_dotenv
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env.prod íŒŒì¼ ë¡œë“œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(os.path.dirname(current_dir))
        env_path = os.path.join(root_dir, '.env.prod')
        load_dotenv(env_path)
        print(f"ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: {env_path}")
        print(f"ğŸ”— OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'Not set')}")
    except ImportError:
        print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")


def setup_paths():
    """í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.dirname(current_dir)
    if src_dir not in sys.path:
        sys.path.append(src_dir)


def get_langfuse_config():
    """Langfuse ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Dockerì—ì„œ ëª¨ë“ˆë¡œ ì‹¤í–‰í•  ë•Œ
        from api.langfuse_config import get_langfuse_manager, get_langfuse_callback
    except ImportError:
        try:
            # ë¡œì»¬ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ
            sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'api'))
            from langfuse_config import get_langfuse_manager, get_langfuse_callback
        except ImportError:
            print("âš ï¸ Langfuse ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
    
    return get_langfuse_manager, get_langfuse_callback


def check_elasticsearch_availability():
    """Elasticsearch ê°€ìš©ì„± í™•ì¸"""
    try:
        from elasticsearch import Elasticsearch
        return True
    except ImportError:
        print("âš ï¸ Elasticsearchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install elasticsearch")
        return False


class FastAPIRAGSystem:
    """FastAPIìš© RAG ì‹œìŠ¤í…œ - unified_rag_cli.pyì™€ ë™ì¼í•œ ë¡œì§"""
    
    def __init__(self):
        self.es_manager = None
        self.model_factory = ModelFactory()
        self.rag_chain = None
        self.embedding_model = None
        self.llm_model = None
        self.model_choice = None
        self.top_k = 5
        self.retriever = None
        self.llm_chain = None
        
        # ë¯¸ë¦¬ ìƒì„±ëœ ì²´ì¸ë“¤ (ìµœì í™”)
        self.refinement_chain = None
        self.qa_chain = None
        self.summary_chain = None
        
        # Langfuse ë§¤ë‹ˆì € ì´ˆê¸°í™”
        get_langfuse_manager, _ = get_langfuse_config()
        self.langfuse_manager = get_langfuse_manager() if get_langfuse_manager else None
        
        # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.session_managers = {}
        self.is_initialized = False
        self.initialization_time = None
    
    def get_chat_manager(self, session_id: str = "default") -> ChatHistoryManager:
        """ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ë°˜í™˜"""
        if session_id not in self.session_managers:
            self.session_managers[session_id] = ChatHistoryManager(max_history=10)
        return self.session_managers[session_id]
    
    async def check_dependencies_async(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± í™•ì¸ (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        
        def _check_dependencies():
            issues = []
            
            if not check_elasticsearch_availability():
                issues.append("Elasticsearch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not HUGGINGFACE_EMBEDDINGS_AVAILABLE:
                issues.append("HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Elasticsearch ì„œë²„ ì—°ê²° í™•ì¸
            try:
                es_manager = ElasticsearchManager()
                is_connected, connection_msg = es_manager.check_connection()
                if not is_connected:
                    issues.append(f"Elasticsearch ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {connection_msg}")
            except Exception as e:
                issues.append(f"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            
            # Ollama ì„œë²„ ì—°ê²° í™•ì¸
            try:
                from core.rag import check_ollama_connection
                ollama_connected, ollama_message = check_ollama_connection()
                if not ollama_connected:
                    issues.append(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {ollama_message}")
            except Exception as e:
                issues.append(f"Ollama ì—°ê²° í™•ì¸ ì˜¤ë¥˜: {str(e)}")
            
            return {
                "status": "ok" if not issues else "error",
                "issues": issues,
                "issue_count": len(issues)
            }
        
        return await asyncio.get_event_loop().run_in_executor(None, _check_dependencies)
    
    async def initialize_rag_system_async(self, model_choice: str, top_k: int = 5) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        import time
        
        def _initialize_rag_system():
            try:
                start_time = time.time()
                
                self.model_choice = model_choice
                self.top_k = top_k
                
                # Elasticsearch ê´€ë¦¬ì ì´ˆê¸°í™”
                self.es_manager = ElasticsearchManager()
                
                # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
                self.embedding_model = self.model_factory.create_embedding_model()
                if not self.embedding_model:
                    return {
                        "status": "error",
                        "message": "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"
                    }
                
                # LLM ëª¨ë¸ ë¡œë“œ
                self.llm_model, status = self.model_factory.create_llm_model(model_choice)
                if not self.llm_model:
                    return {
                        "status": "error",
                        "message": f"LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {status}"
                    }
                
                # RAG ì²´ì¸ ìƒì„± (Langfuse ì½œë°± í¬í•¨)
                _, get_langfuse_callback = get_langfuse_config()
                langfuse_callback = get_langfuse_callback() if get_langfuse_callback else None
                callbacks = [langfuse_callback] if langfuse_callback else None
                
                self.rag_chain, success_or_error = create_rag_chain(
                    embeddings=self.embedding_model,
                    llm_model=self.llm_model,
                    top_k=top_k,
                    callbacks=callbacks
                )

                # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ Retriever ìƒì„±
                self.retriever = create_enhanced_retriever(
                    embedding_model=self.embedding_model,
                    top_k=top_k
                )

                # ë¯¸ë¦¬ ì‚¬ìš©í•  ì²´ì¸ë“¤ ìƒì„± (ìµœì í™”)
                self.refinement_chain = create_llm_chain(
                    self.llm_model, 
                    prompt_for_refined_query,
                    input_variables=["question", "context"]
                )
                
                self.qa_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_query,
                    input_variables=["question", "context"]
                )
                
                self.summary_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_context_summary,
                    input_variables=["context"]
                )

                # LLM ì²´ì¸ ìƒì„±
                try:
                    self.llm_chain = create_llm_chain(
                        llm_model=self.llm_model,
                        prompt_template="""{context}, {question}"""
                    )
                except Exception as e:
                    print(f"âŒ LLM ì²´ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    self.llm_chain = None

                if self.rag_chain:
                    self.is_initialized = True
                    self.initialization_time = time.time() - start_time
                    return {
                        "status": "success",
                        "message": "RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ",
                        "model": LLM_MODELS[model_choice]['name'],
                        "top_k": top_k,
                        "initialization_time": self.initialization_time
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {success_or_error}"
                    }
                    
            except Exception as e:
                self.is_initialized = False
                return {
                    "status": "error",
                    "message": f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                }
        
        return await asyncio.get_event_loop().run_in_executor(None, _initialize_rag_system)
    
    async def process_query_async(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (ë¹„ë™ê¸° ë²„ì „, ì˜ë¯¸+í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©)"""
        if not self.is_initialized or not self.rag_chain:
            return {
                "status": "error",
                "message": "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        import asyncio
        import time

        def _process_query():
            try:
                start_time = time.time()

                # Langfuse íŠ¸ë ˆì´ìŠ¤ ìƒì„±
                trace = None
                if self.langfuse_manager:
                    trace = self.langfuse_manager.create_trace(
                        name="rag_query",
                        metadata={
                            "session_id": session_id,
                            "model": self.model_choice,
                            "top_k": self.top_k
                        }
                    )

                # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ê°€ì ¸ì˜¤ê¸°
                chat_manager = self.get_chat_manager(session_id)

                # ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ ì§ˆë¬¸ ì¬ì •ì˜
                history = chat_manager.build_history()
                print(f"ğŸ” ëŒ€í™” ê¸°ë¡: {history}")
                print(f"ğŸ” ì›ë³¸ ì§ˆì˜: {query}")

                # # ì§ˆë¬¸ ì¬ì •ì˜ë¥¼ ìœ„í•œ ì´ˆê¸° ê²€ìƒ‰ (ì˜ë¯¸ + í‚¤ì›Œë“œ)
                # initial_semantic_docs = self.retriever.get_relevant_documents(query)
                # initial_keyword_results = ElasticsearchManager.keyword_search(query, top_k=3)
                
                # # ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© (ì¬ì§ˆì˜ìš©)
                # initial_context = []
                # for doc in initial_semantic_docs[:3]:  # ìƒìœ„ 3ê°œë§Œ
                #     initial_context.append(getattr(doc, "page_content", str(doc)))
                # for kdoc in initial_keyword_results[:2]:  # ìƒìœ„ 2ê°œë§Œ
                #     content = kdoc.get("content", "")
                #     if content and content not in initial_context:
                #         initial_context.append(content)

                # ì§ˆë¬¸ì„ ì•Œë§ê²Œ ë³€ê²½í•˜ê¸°ìœ„í•¨ì´ê¸°ì— historyë§Œì„ contextì— ì‚¬ìš©
                refined_query = self.refinement_chain.run({"question": query, "context": history})
                print(f"ğŸ” ì •ì œëœ ì§ˆì˜: {refined_query}")

                # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì‹œë§¨í‹± + í‚¤ì›Œë“œ + ìŠ¤ì½”ì–´ë§ì´ ëª¨ë‘ í¬í•¨ë¨)
                merged_docs = self.retriever.get_relevant_documents(refined_query)
                print(f"ğŸ” ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(merged_docs)}")

                # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                docs_text = "\n\n---\n\n".join([
                    getattr(doc, "page_content", str(doc)) for doc in merged_docs
                ])
                print(f"ğŸ” ìµœì¢… ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docs_text)} ë¬¸ì")

                # ê²€ìƒ‰ëœ ìë£Œì™€ ì¬ì •ì˜ ì§ˆë¬¸ì„ LLMì— ë„˜ê²¨ì„œ ë‹µë³€ ìƒì„±
                result = self.qa_chain.invoke({"question": refined_query, "context": docs_text})

                # ë””ë²„ê¹…: ì‹¤ì œ ì‘ë‹µ êµ¬ì¡° ì¶œë ¥
                print(f"ğŸ” RAG ì²´ì¸ ì‘ë‹µ êµ¬ì¡°: {result}")
                print(f"ğŸ” ì‘ë‹µ í‚¤ë“¤: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")

                processing_time = time.time() - start_time

                # RetrievalQAëŠ” 'result' í‚¤ë¥¼ ì‚¬ìš©í•¨
                if result and ('answer' in result or 'result' in result or 'text' in result):
                    answer = result.get('answer') or result.get('result') or result.get('text')
                    print(f"ğŸ” ìµœì¢… ë‹µë³€: {answer}")
                    # ë‹µë³€ ìš”ì•½
                    answer_summary = self.summary_chain.run({"context": answer})
                    print(f"ğŸ” ë‹µë³€ ìš”ì•½: {answer_summary}")
                    # ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ê°€
                    chat_manager.add_chat(refined_query, answer_summary)

                    # Langfuseì— ê²°ê³¼ ë¡œê·¸
                    if trace and self.langfuse_manager:
                        self.langfuse_manager.log_generation(
                            trace_context=trace.get('trace_context'),
                            name="rag_generation",
                            input=query,
                            output=answer,
                            metadata={
                                "processing_time": processing_time,
                                "retrieved_docs_count": len(merged_docs),
                                "model": self.model_choice
                            }
                        )

                    # retrieved_docsì— ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
                    retrieved_docs = []
                    for doc in merged_docs:
                        retrieved_docs.append({
                            "type": "enhanced_hybrid",
                            "content": getattr(doc, "page_content", str(doc)),
                            "metadata": getattr(doc, "metadata", {})
                        })

                    return {
                        "status": "success",
                        "answer": answer,
                        "query": query,
                        "session_id": session_id,
                        "processing_time": processing_time,
                        "retrieved_docs": retrieved_docs
                    }
                else:
                    # Langfuseì— ì—ëŸ¬ ë¡œê·¸
                    if trace and self.langfuse_manager:
                        self.langfuse_manager.log_event(
                            trace_context=trace.get('trace_context'),
                            name="rag_error",
                            metadata={
                                "error": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ì‘ë‹µ êµ¬ì¡°: {result}",
                                "processing_time": processing_time
                            }
                        )

                    return {
                        "status": "error",
                        "message": f"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ êµ¬ì¡°: {result}",
                        "processing_time": processing_time
                    }

            except Exception as e:
                # Langfuseì— ì˜ˆì™¸ ë¡œê·¸
                if 'trace' in locals() and trace and self.langfuse_manager:
                    self.langfuse_manager.log_event(
                        trace_context=trace.get('trace_context'),
                        name="rag_exception",
                        metadata={
                            "error": str(e),
                            "processing_time": time.time() - start_time
                        }
                    )

                return {
                    "status": "error",
                    "message": f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                    "processing_time": time.time() - start_time
                }

        return await asyncio.get_event_loop().run_in_executor(None, _process_query)
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "is_initialized": self.is_initialized,
            "model": LLM_MODELS.get(self.model_choice, {}).get('name') if self.model_choice else None,
            "model_key": self.model_choice,
            "top_k": self.top_k,
            "initialization_time": self.initialization_time,
            "active_sessions": len(self.session_managers),
            "available_models": self.model_factory.get_available_models(),
            "langfuse_status": self.langfuse_manager.get_status() if self.langfuse_manager else {"available": False}
        }


@asynccontextmanager
async def lifespan(app):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    global rag_system
    rag_system = FastAPIRAGSystem()
    print("ğŸš€ FastAPI RAG ì‹œìŠ¤í…œ ì‹œì‘")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ FastAPI RAG ì‹œìŠ¤í…œ ì¢…ë£Œ")


# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None


# Langfuse í•¨ìˆ˜ë“¤ì„ export
def get_langfuse_manager():
    """Langfuse ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°"""
    try:
        get_langfuse_manager_func, _ = get_langfuse_config()
        return get_langfuse_manager_func() if get_langfuse_manager_func else None
    except Exception:
        return None


def get_langfuse_callback():
    """Langfuse ì½œë°± ê°€ì ¸ì˜¤ê¸°"""
    try:
        _, get_langfuse_callback_func = get_langfuse_config()
        return get_langfuse_callback_func() if get_langfuse_callback_func else None
    except Exception:
        return None
