"""
í™˜ê²½ ì„¤ì • ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ë¡œì§
"""

import json
import os
import sys
from typing import Dict, Any
from contextlib import asynccontextmanager

from core.config import LLM_MODELS, HUGGINGFACE_EMBEDDINGS_AVAILABLE, OLLAMA_AVAILABLE
from core.models import ModelFactory
from core.rag import create_llm_chain, create_rag_chain, create_retriever, create_enhanced_retriever, prompt_for_refined_query, prompt_for_query, prompt_for_context_summary
from core.chat_history import ChatHistoryManager
from utils.elasticsearch import ElasticsearchManager


def load_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    try:
        from dotenv import load_dotenv
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env.prod íŒŒì¼ ë¡œë“œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(os.path.dirname(current_dir))
        env_path = os.path.join(root_dir, '.env.prod')
        load_dotenv(env_path)
        print(f"ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: {env_path}")
        print(f"ğŸ”— OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'Not set')}")
        
        # LangSmith íŠ¸ë ˆì´ì‹± ëª…ì‹œì  ë¹„í™œì„±í™”
        os.environ["LANGCHAIN_TRACING_V2"] = "false"
        if "LANGCHAIN_API_KEY" in os.environ:
            del os.environ["LANGCHAIN_API_KEY"]
        if "LANGSMITH_API_KEY" in os.environ:
            del os.environ["LANGSMITH_API_KEY"]
        print("ğŸš« LangSmith íŠ¸ë ˆì´ì‹± ë¹„í™œì„±í™”ë¨")
        
    except ImportError:
        print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")


def setup_paths():
    """í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.dirname(current_dir)
    if src_dir not in sys.path:
        sys.path.append(src_dir)


def get_langfuse_config():
    """Langfuse ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Dockerì—ì„œ ëª¨ë“ˆë¡œ ì‹¤í–‰í•  ë•Œ
        from api.langfuse_config import get_langfuse_manager, get_langfuse_callback
    except ImportError:
        try:
            # ë¡œì»¬ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ
            sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'api'))
            from langfuse_config import get_langfuse_manager, get_langfuse_callback
        except ImportError:
            print("âš ï¸ Langfuse ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
    
    return get_langfuse_manager, get_langfuse_callback


def check_elasticsearch_availability():
    """Elasticsearch ê°€ìš©ì„± í™•ì¸"""
    try:
        from elasticsearch import Elasticsearch
        return True
    except ImportError:
        print("âš ï¸ Elasticsearchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install elasticsearch")
        return False


class FastAPIRAGSystem:
    """FastAPIìš© RAG ì‹œìŠ¤í…œ - unified_rag_cli.pyì™€ ë™ì¼í•œ ë¡œì§"""
    
    def __init__(self):
        self.es_manager = None
        self.model_factory = ModelFactory()
        self.rag_chain = None
        self.embedding_model = None
        self.llm_model = None
        self.model_choice = None
        self.top_k = 5
        self.retriever = None
        self.llm_chain = None
        
        # ë¯¸ë¦¬ ìƒì„±ëœ ì²´ì¸ë“¤ (ìµœì í™”)
        self.refinement_chain = None
        self.qa_chain = None
        self.summary_chain = None
        
        # Langfuse ë§¤ë‹ˆì € ì´ˆê¸°í™”
        get_langfuse_manager, _ = get_langfuse_config()
        self.langfuse_manager = get_langfuse_manager() if get_langfuse_manager else None
        
        # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.session_managers = {}
        self.is_initialized = False
        self.initialization_time = None
    
    def get_chat_manager(self, session_id: str = "default") -> ChatHistoryManager:
        """ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ë°˜í™˜"""
        if session_id not in self.session_managers:
            self.session_managers[session_id] = ChatHistoryManager(max_history=10)
        return self.session_managers[session_id]
    
    async def check_dependencies_async(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± í™•ì¸ (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        
        def _check_dependencies():
            issues = []
            
            if not check_elasticsearch_availability():
                issues.append("Elasticsearch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not HUGGINGFACE_EMBEDDINGS_AVAILABLE:
                issues.append("HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Elasticsearch ì„œë²„ ì—°ê²° í™•ì¸
            try:
                es_manager = ElasticsearchManager()
                is_connected, connection_msg = es_manager.check_connection()
                if not is_connected:
                    issues.append(f"Elasticsearch ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {connection_msg}")
            except Exception as e:
                issues.append(f"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            
            # Ollama ì„œë²„ ì—°ê²° í™•ì¸
            try:
                from core.rag import check_ollama_connection
                ollama_connected, ollama_message = check_ollama_connection()
                if not ollama_connected:
                    issues.append(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {ollama_message}")
            except Exception as e:
                issues.append(f"Ollama ì—°ê²° í™•ì¸ ì˜¤ë¥˜: {str(e)}")
            
            return {
                "status": "ok" if not issues else "error",
                "issues": issues,
                "issue_count": len(issues)
            }
        
        return await asyncio.get_event_loop().run_in_executor(None, _check_dependencies)
    
    async def initialize_rag_system_async(self, model_choice: str, top_k: int = 5) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¹„ë™ê¸° ë²„ì „)"""
        import asyncio
        import time
        
        def _initialize_rag_system():
            try:
                start_time = time.time()
                
                self.model_choice = model_choice
                self.top_k = top_k
                
                # Elasticsearch ê´€ë¦¬ì ì´ˆê¸°í™”
                self.es_manager = ElasticsearchManager()
                
                # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
                self.embedding_model = self.model_factory.create_embedding_model()
                if not self.embedding_model:
                    return {
                        "status": "error",
                        "message": "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"
                    }
                
                # LLM ëª¨ë¸ ë¡œë“œ
                self.llm_model, status = self.model_factory.create_llm_model(model_choice)
                if not self.llm_model:
                    return {
                        "status": "error",
                        "message": f"LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {status}"
                    }
                
                # RAG ì²´ì¸ ìƒì„± (Langfuse ì½œë°± í¬í•¨)
                _, get_langfuse_callback = get_langfuse_config()
                langfuse_callback = get_langfuse_callback() if get_langfuse_callback else None
                callbacks = [langfuse_callback] if langfuse_callback else None
                
                self.rag_chain, success_or_error = create_rag_chain(
                    embeddings=self.embedding_model,
                    llm_model=self.llm_model,
                    top_k=top_k,
                    callbacks=callbacks
                )

                # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ Retriever ìƒì„±
                self.retriever = create_enhanced_retriever(
                    embedding_model=self.embedding_model,
                    top_k=top_k
                )

                # ë¯¸ë¦¬ ì‚¬ìš©í•  ì²´ì¸ë“¤ ìƒì„± (ìµœì í™”)
                self.refinement_chain = create_llm_chain(
                    self.llm_model, 
                    prompt_for_refined_query,
                    input_variables=["userinfo", "question", "context"]
                )
                
                self.qa_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_query,
                    input_variables=["question", "context"]
                )
                
                self.summary_chain = create_llm_chain(
                    self.llm_model,
                    prompt_for_context_summary,
                    input_variables=["context"]
                )

                # LLM ì²´ì¸ ìƒì„±
                try:
                    self.llm_chain = create_llm_chain(
                        llm_model=self.llm_model,
                        prompt_template="""{context}, {question}"""
                    )
                except Exception as e:
                    print(f"âŒ LLM ì²´ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                    self.llm_chain = None

                if self.rag_chain:
                    self.is_initialized = True
                    self.initialization_time = time.time() - start_time
                    return {
                        "status": "success",
                        "message": "RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ",
                        "model": LLM_MODELS[model_choice]['name'],
                        "top_k": top_k,
                        "initialization_time": self.initialization_time
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {success_or_error}"
                    }
                    
            except Exception as e:
                self.is_initialized = False
                return {
                    "status": "error",
                    "message": f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                }
        
        return await asyncio.get_event_loop().run_in_executor(None, _initialize_rag_system)
    
    def enhance_docs_for_table_preservation(self, merged_docs):
        """í‘œ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ê³  LLMì˜ ì´í•´ë¥¼ ë•ëŠ” ë¬¸ì„œ ê²°í•© í•¨ìˆ˜"""
        enhanced_docs = []
        
        for i, doc in enumerate(merged_docs):
            content = getattr(doc, "page_content", str(doc))
            metadata = getattr(doc, "metadata", {})
            
            # í‘œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ë§ˆí¬ë‹¤ìš´ í‘œ íŒ¨í„´)
            has_table = bool(
                "|" in content and 
                ("---" in content or ":-:" in content or ":--" in content or "--:" in content)
            )
            
            if has_table:
                # í‘œê°€ í¬í•¨ëœ ë¬¸ì„œëŠ” íŠ¹ë³„í•œ ë§ˆí‚¹ê³¼ í•¨ê»˜ ë³´ì¡´
                enhanced_content = f"""ğŸ“Š **í‘œ ë°ì´í„° ë¬¸ì„œ #{i+1}** (íŒŒì¼: {metadata.get('filename', 'Unknown')})

    {content.strip()}

    ğŸ“Š **í‘œ ë°ì´í„° ë¬¸ì„œ ë**"""
            else:
                # ì¼ë°˜ ë¬¸ì„œ
                enhanced_content = f"""ğŸ“„ **ì°¸ê³  ë¬¸ì„œ #{i+1}**
    (íŒŒì¼: {metadata.get('filename', 'Unknown')})

    {content.strip()}

    ğŸ“„ **ë¬¸ì„œ ë**"""
            
            enhanced_docs.append(enhanced_content)
        
        # ê° ë¬¸ì„œ ì‚¬ì´ì— ëª…í™•í•œ êµ¬ë¶„ìë¥¼ ë„£ì–´ ê²°í•©
        separator = "\n\n" + "="*50 + " ë¬¸ì„œ êµ¬ë¶„ì„  " + "="*50 + "\n\n"
        return separator.join(enhanced_docs)



    async def process_query_async(self, query: str, session_id: str = "default", user_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (ë¹„ë™ê¸° ë²„ì „, ì˜ë¯¸+í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©)"""
        if not self.is_initialized or not self.rag_chain:
            return {
                "status": "error",
                "message": "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        import asyncio
        import time

        def _process_query():
            try:
                start_time = time.time()

                # Langfuse íŠ¸ë ˆì´ìŠ¤ ìƒì„±
                trace = None
                if self.langfuse_manager:
                    trace_metadata = {
                        "session_id": session_id,
                        "model": self.model_choice,
                        "top_k": self.top_k
                    }
                    # ì‚¬ìš©ì ë°ì´í„°ê°€ ìˆìœ¼ë©´ íŠ¸ë ˆì´ìŠ¤ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                    if user_data and isinstance(user_data, dict):
                        trace_metadata.update({
                            "user_id": user_data.get("userId"),
                            "user_name": user_data.get("userName"),
                            "user_age": user_data.get("age"),
                            "user_income": user_data.get("income"),
                            "is_authenticated": user_data.get("isAuthenticated"),
                            "login_time": user_data.get("loginTime")
                        })
                    
                    trace = self.langfuse_manager.create_trace(
                        name="rag_query",
                        metadata=trace_metadata
                    )

                # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ì ê°€ì ¸ì˜¤ê¸°
                chat_manager = self.get_chat_manager(session_id)

                # ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ ì§ˆë¬¸ ì¬ì •ì˜
                history = chat_manager.build_history()
                print(f"ğŸ” ëŒ€í™” ê¸°ë¡: {history}")
                print(f"ğŸ” ì›ë³¸ ì§ˆì˜: {query}")

                # # ì§ˆë¬¸ ì¬ì •ì˜ë¥¼ ìœ„í•œ ì´ˆê¸° ê²€ìƒ‰ (ì˜ë¯¸ + í‚¤ì›Œë“œ)
                # initial_semantic_docs = self.retriever.get_relevant_documents(query)
                # initial_keyword_results = ElasticsearchManager.keyword_search(query, top_k=3)
                
                # # ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© (ì¬ì§ˆì˜ìš©)
                # initial_context = []
                # for doc in initial_semantic_docs[:3]:  # ìƒìœ„ 3ê°œë§Œ
                #     initial_context.append(getattr(doc, "page_content", str(doc)))
                # for kdoc in initial_keyword_results[:2]:  # ìƒìœ„ 2ê°œë§Œ
                #     content = kdoc.get("content", "")
                #     if content and content not in initial_context:
                #         initial_context.append(content)

                # ì§ˆë¬¸ì„ ì•Œë§ê²Œ ë³€ê²½í•˜ê¸°ìœ„í•¨ì´ê¸°ì— historyë§Œì„ contextì— ì‚¬ìš©
                userinfo = {  
                    "userId": "bccard",  
                    "userName": "ê¹€ëª…ì •",  
                    "loginTime": "2025-08-27T14:23:45.123Z",
                    "isAuthenticated": True, # pythonì—ì„œ true -> True ë¡œ ì¹˜í™˜ë¨
                    "age": "27",
                    "income": "77,511,577",
                    "data": {
                        "email": "kmj@deotis.co.kr",
                        "phone": "010-1234-5678",
                        "ownCardArr": [
                            {
                                "bank": "ìš°ë¦¬ì¹´ë“œ",
                                "name": "VVIP ì¹´ë“œ",
                                "paymentDate": "4",
                                "ipn": "VISA",
                                "type": "ì‹ ìš©ì¹´ë“œ"
                            }
                        ]
                    }
                }  # JSON ê°ì²´ í˜•íƒœì˜ ì‚¬ìš©ì ì •ë³´
                
                refined_query_str = self.refinement_chain.run({"question": query, "context": history, "userinfo": userinfo})
                print(f"ğŸ” ì •ì œëœ ì§ˆì˜ (ì›ë³¸): {refined_query_str}")
                
                # JSON ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•íƒœ ì²˜ë¦¬
                refined_query = query  # ê¸°ë³¸ê°’
                action = None
                
                def extract_json_from_markdown(text):
                    """ë§ˆí¬ë‹¤ìš´ì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ"""
                    import re
                    # ```json ... ``` ë˜ëŠ” ```  ... ``` í˜•íƒœì˜ ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
                    json_pattern = r'```(?:json)?\s*\n?(.*?)\n?```'
                    match = re.search(json_pattern, text, re.DOTALL | re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
                    return None
                
                def parse_refined_query(response_str):
                    """JSON ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ refined_query ì¶”ì¶œ"""
                    # 1. ì§ì ‘ JSON íŒŒì‹± ì‹œë„
                    try:
                        parsed_json = json.loads(response_str)
                        return parsed_json.get('refined_query'), parsed_json.get('action')
                    except json.JSONDecodeError:
                        pass
                    
                    # 2. ë§ˆí¬ë‹¤ìš´ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                    json_content = extract_json_from_markdown(response_str)
                    if json_content:
                        try:
                            parsed_json = json.loads(json_content)
                            return parsed_json.get('refined_query'), parsed_json.get('action')
                        except json.JSONDecodeError:
                            pass
                    
                    # 3. í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„ (ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ë‚´ìš©)
                    # refined_query: ë˜ëŠ” ì§ˆë¬¸: ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
                    import re
                    query_patterns = [
                        r'refined_query[:\s]*([^\n]+)',
                        r'ì§ˆë¬¸[:\s]*([^\n]+)',
                        r'ì •ì œëœ\s*ì§ˆì˜[:\s]*([^\n]+)',
                        r'ê°œì„ ëœ\s*ì§ˆë¬¸[:\s]*([^\n]+)'
                    ]
                    
                    for pattern in query_patterns:
                        match = re.search(pattern, response_str, re.IGNORECASE)
                        if match:
                            extracted_query = match.group(1).strip()
                            # ë”°ì˜´í‘œ ì œê±°
                            extracted_query = extracted_query.strip('"\'')
                            return extracted_query, None
                    
                    # 4. action ì¶”ì¶œ ì‹œë„
                    action_patterns = [
                        r'action[:\s]*([^\n]+)',
                        r'ë™ì‘[:\s]*([^\n]+)'
                    ]
                    
                    extracted_action = None
                    for pattern in action_patterns:
                        match = re.search(pattern, response_str, re.IGNORECASE)
                        if match:
                            extracted_action = match.group(1).strip().strip('"\'')
                            break
                    
                    return None, extracted_action
                
                try:
                    refined_query, action = parse_refined_query(refined_query_str)
                    
                    if refined_query:
                        print(f"ğŸ” ì •ì œëœ ì§ˆì˜: {refined_query}")
                    else:
                        print(f"ğŸ” ì •ì œëœ ì§ˆì˜ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ ì§ˆì˜ ì‚¬ìš©: {query}")
                        refined_query = query
                    
                    if action == 'reset':
                        # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
                        chat_manager.clear_history()
                        print("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨")
                    elif action == 'answer':
                        # refined_queryë¥¼ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë°”ë¡œ ë¦¬í„´
                        processing_time = time.time() - start_time
                        print(f"ğŸ” ì§ì ‘ ë‹µë³€ ëª¨ë“œ: {refined_query}")
                        
                        # ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ê°€
                        chat_manager.add_chat(query, refined_query)
                        
                        # Langfuseì— ê²°ê³¼ ë¡œê·¸
                        if trace and self.langfuse_manager:
                            self.langfuse_manager.log_generation(
                                trace_context=trace.get('trace_context'),
                                name="direct_answer_generation",
                                input=query,
                                output=refined_query,
                                metadata={
                                    "processing_time": processing_time,
                                    "mode": "direct_answer",
                                    "model": self.model_choice
                                }
                            )
                        
                        return {
                            "status": "success",
                            "answer": refined_query,
                            "query": query,
                            "refined_query": refined_query,
                            "session_id": session_id,
                            "processing_time": processing_time,
                            "retrieved_docs": []
                        }
                        
                except Exception as e:
                    print(f"âŒ ì •ì œëœ ì§ˆì˜ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    print(f"âŒ ì‘ë‹µ ë‚´ìš©: {refined_query_str}")
                    refined_query = query
                    chat_manager.clear_history()
                    print("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨ (íŒŒì‹± ì‹¤íŒ¨)")

                # ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì‹œë§¨í‹± + í‚¤ì›Œë“œ + ìŠ¤ì½”ì–´ë§ì´ ëª¨ë‘ í¬í•¨ë¨)
                merged_docs = self.retriever.get_relevant_documents(refined_query)
                print(f"ğŸ” ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(merged_docs)}")

                # # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                docs_text = "\n\n---\n\n".join([
                    getattr(doc, "page_content", str(doc)) for doc in merged_docs
                ])
                print(f"ğŸ” ìµœì¢… ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docs_text)} ë¬¸ì")
                # docs_text = self.enhance_docs_for_table_preservation(merged_docs)
                # print(f"ğŸ” ìµœì¢… ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(docs_text)} ë¬¸ì")


                # ê°œì¸ ì •ë³´ ê´€ë ¨ ì§ˆì˜ì¸ì§€ íŒë‹¨ // íŒë‹¨ë§Œ í•˜ê³  ì•„ì§ ì“°ì§„ ì•ŠìŒ
                personal_keywords = ['ë‚´', 'ë‚˜ì˜', 'ë‚´ê°€', 'ë‚´ ì¹´ë“œ', 'ë‚´ ì •ë³´', 'ë‚´ ê²°ì œì¼', 'ë‚´ í˜œíƒ', 'ë‚´ í¬ì¸íŠ¸']
                is_personal_query = any(keyword in query for keyword in personal_keywords)
    

                # ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                personalized_context = docs_text
                if user_data and isinstance(user_data, dict):
                    user_context = f"""
ì‚¬ìš©ì ì •ë³´:
- ì´ë¦„: {user_data.get('userName', 'N/A')}
- ë‚˜ì´: {user_data.get('age', 'N/A')}ì„¸
- ì—°ì†Œë“: {user_data.get('income', 'N/A')}ì›
- ì¸ì¦ ìƒíƒœ: {'ì¸ì¦ë¨' if user_data.get('isAuthenticated') else 'ë¯¸ì¸ì¦'}
"""
                    # ë³´ìœ  ì¹´ë“œ ì •ë³´ ì¶”ê°€
                    user_data_obj = user_data.get('data', {})
                    if user_data_obj and isinstance(user_data_obj, dict) and user_data_obj.get('ownCardArr'):
                        user_context += "\në³´ìœ  ì¹´ë“œ:\n"
                        for card in user_data_obj['ownCardArr']:
                            if isinstance(card, dict):
                                user_context += f"- {card.get('bank', 'N/A')} {card.get('name', 'N/A')} ({card.get('type', 'N/A')}, ê²°ì œì¼: {card.get('paymentDate', 'N/A')}ì¼)\n"
                    
                    personalized_context = user_context + "\n\nê´€ë ¨ ë¬¸ì„œ:\n" + docs_text
                    print(f"ğŸ” ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(personalized_context)} ë¬¸ì")

                # ê²€ìƒ‰ëœ ìë£Œì™€ ì¬ì •ì˜ ì§ˆë¬¸ì„ LLMì— ë„˜ê²¨ì„œ ë‹µë³€ ìƒì„±
                result = self.qa_chain.invoke({"question": refined_query, "context": personalized_context})

                # ë””ë²„ê¹…: ì‹¤ì œ ì‘ë‹µ êµ¬ì¡° ì¶œë ¥
                print(f"ğŸ” RAG ì²´ì¸ ì‘ë‹µ êµ¬ì¡°: {result}")
                print(f"ğŸ” ì‘ë‹µ í‚¤ë“¤: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")

                processing_time = time.time() - start_time

                # RetrievalQAëŠ” 'result' í‚¤ë¥¼ ì‚¬ìš©í•¨
                if result and ('answer' in result or 'result' in result or 'text' in result):
                    answer = result.get('answer') or result.get('result') or result.get('text')
                    print(f"ğŸ” ìµœì¢… ë‹µë³€: {answer}")
                    # ë‹µë³€ ìš”ì•½
                    # answer_summary = self.summary_chain.run({"context": answer})
                    # print(f"ğŸ” ë‹µë³€ ìš”ì•½: {answer_summary}")
                    # ëŒ€í™” ê¸°ë¡ì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ê°€
                    chat_manager.add_chat(query, answer)

                    # Langfuseì— ê²°ê³¼ ë¡œê·¸
                    if trace and self.langfuse_manager:
                        self.langfuse_manager.log_generation(
                            trace_context=trace.get('trace_context'),
                            name="rag_generation",
                            input=query,
                            output=answer,
                            metadata={
                                "processing_time": processing_time,
                                "retrieved_docs_count": len(merged_docs),
                                "model": self.model_choice
                            }
                        )

                    # retrieved_docsì— ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
                    retrieved_docs = []
                    for doc in merged_docs:
                        retrieved_docs.append({
                            "type": "enhanced_hybrid",
                            "content": getattr(doc, "page_content", str(doc)),
                            "metadata": getattr(doc, "metadata", {})
                        })

                    return {
                        "status": "success",
                        "answer": answer,
                        "query": query,
                        "refined_query": refined_query,
                        "session_id": session_id,
                        "username": user_data.get("userName", "") if user_data and isinstance(user_data, dict) else "",  # ì‚¬ìš©ì ë°ì´í„° í¬í•¨
                        "processing_time": processing_time,
                        "retrieved_docs": retrieved_docs
                    }
                else:
                    # Langfuseì— ì—ëŸ¬ ë¡œê·¸
                    if trace and self.langfuse_manager:
                        self.langfuse_manager.log_event(
                            trace_context=trace.get('trace_context'),
                            name="rag_error",
                            metadata={
                                "error": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ì‘ë‹µ êµ¬ì¡°: {result}",
                                "processing_time": processing_time
                            }
                        )

                    return {
                        "status": "error",
                        "message": f"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ êµ¬ì¡°: {result}",
                        "processing_time": processing_time
                    }

            except Exception as e:
                # Langfuseì— ì˜ˆì™¸ ë¡œê·¸
                if 'trace' in locals() and trace and self.langfuse_manager:
                    self.langfuse_manager.log_event(
                        trace_context=trace.get('trace_context'),
                        name="rag_exception",
                        metadata={
                            "error": str(e),
                            "processing_time": time.time() - start_time
                        }
                    )

                return {
                    "status": "error",
                    "message": f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                    "processing_time": time.time() - start_time
                }

        return await asyncio.get_event_loop().run_in_executor(None, _process_query)
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "is_initialized": self.is_initialized,
            "model": LLM_MODELS.get(self.model_choice, {}).get('name') if self.model_choice else None,
            "model_key": self.model_choice,
            "top_k": self.top_k,
            "initialization_time": self.initialization_time,
            "active_sessions": len(self.session_managers),
            "available_models": self.model_factory.get_available_models(),
            "langfuse_status": self.langfuse_manager.get_status() if self.langfuse_manager else {"available": False}
        }


@asynccontextmanager
async def lifespan(app):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    global rag_system
    rag_system = FastAPIRAGSystem()
    print("ğŸš€ FastAPI RAG ì‹œìŠ¤í…œ ì‹œì‘")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    print("ğŸ‘‹ FastAPI RAG ì‹œìŠ¤í…œ ì¢…ë£Œ")


# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None


# Langfuse í•¨ìˆ˜ë“¤ì„ export
def get_langfuse_manager():
    """Langfuse ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°"""
    try:
        get_langfuse_manager_func, _ = get_langfuse_config()
        return get_langfuse_manager_func() if get_langfuse_manager_func else None
    except Exception:
        return None


def get_langfuse_callback():
    """Langfuse ì½œë°± ê°€ì ¸ì˜¤ê¸°"""
    try:
        _, get_langfuse_callback_func = get_langfuse_config()
        return get_langfuse_callback_func() if get_langfuse_callback_func else None
    except Exception:
        return None
