"""
ì„±ëŠ¥ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ
"""
import os
import time
import psutil
from datetime import datetime
from typing import Dict, List, Optional, Any


class PerformanceTracker:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics = {}
        self.current_process = psutil.Process()
        
    def get_elasticsearch_memory(self) -> tuple[float, int]:
        """Elasticsearch í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        es_memory = 0
        es_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
            try:
                if 'elasticsearch' in proc.info['name'].lower() or 'java' in proc.info['name'].lower():
                    # Java í”„ë¡œì„¸ìŠ¤ ì¤‘ Elasticsearch ê´€ë ¨ í™•ì¸
                    try:
                        cmdline = proc.cmdline()
                        if any('elasticsearch' in cmd.lower() for cmd in cmdline):
                            es_processes.append(proc)
                            es_memory += proc.memory_info().rss / 1024 / 1024
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return es_memory, len(es_processes)
    
    def get_ollama_memory(self) -> tuple[float, int]:
        """Ollama í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        ollama_memory = 0
        ollama_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
            try:
                if 'ollama' in proc.info['name'].lower():
                    ollama_processes.append(proc)
                    ollama_memory += proc.memory_info().rss / 1024 / 1024
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return ollama_memory, len(ollama_processes)
    
    def get_total_memory_usage(self) -> Dict[str, float]:
        """ì „ì²´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        python_memory = self.current_process.memory_info().rss / 1024 / 1024
        es_memory, es_count = self.get_elasticsearch_memory()
        ollama_memory, ollama_count = self.get_ollama_memory()
        
        return {
            'python_memory': python_memory,
            'elasticsearch_memory': es_memory,
            'ollama_memory': ollama_memory,
            'total_memory': python_memory + es_memory + ollama_memory,
            'elasticsearch_process_count': es_count,
            'ollama_process_count': ollama_count
        }
        
    def start_timer(self, task_name: str) -> None:
        """ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        memory_info = self.get_total_memory_usage()
        self.metrics[task_name] = {
            'start_time': time.time(),
            'start_python_memory': memory_info['python_memory'],
            'start_elasticsearch_memory': memory_info['elasticsearch_memory'],
            'start_ollama_memory': memory_info['ollama_memory'],
            'start_total_memory': memory_info['total_memory'],
            'start_cpu_percent': self.current_process.cpu_percent()
        }
        
    def end_timer(self, task_name: str) -> Optional[Dict[str, Any]]:
        """ì‘ì—… ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ê²°ê³¼ ë°˜í™˜"""
        if task_name in self.metrics:
            end_time = time.time()
            memory_info = self.get_total_memory_usage()
            
            self.metrics[task_name].update({
                'end_time': end_time,
                'end_python_memory': memory_info['python_memory'],
                'end_elasticsearch_memory': memory_info['elasticsearch_memory'],
                'end_ollama_memory': memory_info['ollama_memory'],
                'end_total_memory': memory_info['total_memory'],
                'duration': end_time - self.metrics[task_name]['start_time'],
                'python_memory_used': memory_info['python_memory'] - self.metrics[task_name]['start_python_memory'],
                'elasticsearch_memory_used': memory_info['elasticsearch_memory'] - self.metrics[task_name]['start_elasticsearch_memory'],
                'ollama_memory_used': memory_info['ollama_memory'] - self.metrics[task_name]['start_ollama_memory'],
                'total_memory_used': memory_info['total_memory'] - self.metrics[task_name]['start_total_memory'],
                'elasticsearch_process_count': memory_info['elasticsearch_process_count'],
                'ollama_process_count': memory_info['ollama_process_count']
            })
            
            return self.metrics[task_name]
        return None
    
    def get_summary(self) -> Dict[str, Dict[str, Any]]:
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        for task, metrics in self.metrics.items():
            if 'duration' in metrics:
                summary[task] = {
                    'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': round(metrics['duration'], 2),
                    'Python ë©”ëª¨ë¦¬ (MB)': round(metrics['python_memory_used'], 2),
                    'Elasticsearch ë©”ëª¨ë¦¬ (MB)': round(metrics['elasticsearch_memory_used'], 2),
                    'Ollama ë©”ëª¨ë¦¬ (MB)': round(metrics['ollama_memory_used'], 2),
                    'ì´ ë©”ëª¨ë¦¬ (MB)': round(metrics['total_memory_used'], 2),
                    'ES í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['elasticsearch_process_count'],
                    'Ollama í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['ollama_process_count'],
                    'ì™„ë£Œì‹œê°„': datetime.fromtimestamp(metrics['end_time']).strftime('%H:%M:%S')
                }
        return summary


class HybridPerformanceTracker:
    """í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.system_tracker = PerformanceTracker()
        self.hybrid_metrics = {
            'system_metrics': {},
            'combined_insights': {}
        }
        
    def get_langsmith_status(self) -> Dict[str, Any]:
        """Langsmith ìƒíƒœ ë°˜í™˜ (ë¹„í™œì„±í™”ë¨)"""
        return {
            'available': False,
            'enabled': False,
            'project': 'disabled'
        }
    
    def track_preprocessing_stage(self, stage_name: str) -> None:
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ì¶”ì """
        return self.system_tracker.start_timer(stage_name)
    
    def end_preprocessing_stage(self, stage_name: str) -> Optional[Dict[str, Any]]:
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ì¢…ë£Œ"""
        metrics = self.system_tracker.end_timer(stage_name)
        if metrics:
            self.hybrid_metrics['system_metrics'][stage_name] = metrics
        return metrics
    
    def track_llm_inference(self, qa_chain, query: str, metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """LLM ì¶”ë¡  í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì """
        import streamlit as st
        
        self.system_tracker.start_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
        
        if metadata is None:
            metadata = {}
        
        enhanced_metadata = {
            **metadata,
            'session_id': st.session_state.get('session_id', 'unknown'),
            'timestamp': datetime.now().isoformat(),
            'query_length': len(query),
            'system': 'unified_rag'
        }
        
        try:
            response = qa_chain({"query": query})
            
            system_metrics = self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            
            combined_result = {
                'response': response,
                'system_metrics': system_metrics,
                'langsmith_enabled': False,
                'metadata': enhanced_metadata
            }
            
            return combined_result
            
        except Exception as e:
            self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            raise
    
    def get_system_summary(self) -> Dict[str, Dict[str, Any]]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìš”ì•½"""
        return self.system_tracker.get_summary()
    
    def get_hybrid_insights(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸"""
        system_summary = self.get_system_summary()
        
        insights = {
            'system_performance': system_summary,
            'langsmith_status': self.get_langsmith_status(),
            'recommendations': self._generate_recommendations(system_summary)
        }
        
        return insights
    
    def _generate_recommendations(self, system_summary: Dict[str, Dict[str, Any]]) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if system_summary:
            slowest_task = max(system_summary.items(), 
                             key=lambda x: x[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'], 
                             default=(None, {'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': 0}))
            
            if slowest_task[0] and slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'] > 10:
                recommendations.append(f"âš ï¸ {slowest_task[0]}ì´ {slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆë¡œ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
                
                if 'Elasticsearch' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •ì„ ìµœì í™”í•˜ê±°ë‚˜ ë” ì ì€ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")
                elif 'LLM_ì¶”ë¡ ' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ ë” ì‘ì€ LLM ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ GPUë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            total_memory = sum(metrics.get('ì´ ë©”ëª¨ë¦¬ (MB)', 0) for metrics in system_summary.values())
            if total_memory > 8000:
                recommendations.append(f"ğŸ”¥ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {total_memory:.1f}MBë¡œ ë†’ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        return recommendations
