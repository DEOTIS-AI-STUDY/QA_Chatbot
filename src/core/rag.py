"""
RAG ì‹œìŠ¤í…œ í•µì‹¬ ë¡œì§
"""
import os
from typing import Tuple, Union, List
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from core.config import ELASTICSEARCH_URL, INDEX_NAME
from utils.elasticsearch import ElasticsearchManager


def check_ollama_connection() -> Tuple[bool, str]:
    """Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        import requests
        import socket
        from urllib.parse import urlparse
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Ollama URL ê°€ì ¸ì˜¤ê¸°
        ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        parsed_url = urlparse(ollama_url)
        host = parsed_url.hostname or 'localhost'
        port = parsed_url.port or 11434
        
        # 1. HTTP ì‘ë‹µ í™•ì¸ (ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°©ë²•)
        try:
            response = requests.get(f"{ollama_url}/api/version", timeout=5)
            if response.status_code == 200:
                version_info = response.json()
                return True, f"Ollama ì„œë²„ ì—°ê²° ì„±ê³µ (ë²„ì „: {version_info.get('version', 'unknown')})"
            else:
                return False, f"Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code})"
        except requests.exceptions.ConnectionError:
            # HTTP ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´ ì†Œì¼“ìœ¼ë¡œ í¬íŠ¸ í™•ì¸
            pass
        except requests.exceptions.Timeout:
            return False, "Ollama ì„œë²„ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ"
        
        # 2. ì†Œì¼“ ì—°ê²° í™•ì¸ (HTTP ì‹¤íŒ¨ ì‹œì—ë§Œ)
        # ì—¬ëŸ¬ ì†Œì¼“ íŒ¨ë°€ë¦¬ ì‹œë„ (IPv4, IPv6)
        for family in [socket.AF_INET, socket.AF_INET6]:
            try:
                sock = socket.socket(family, socket.SOCK_STREAM)
                sock.settimeout(3)
                result = sock.connect_ex((host, port))
                sock.close()
                
                if result == 0:
                    return False, f"Ollama ì„œë²„ í¬íŠ¸ëŠ” ì—´ë ¤ìˆì§€ë§Œ HTTP ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤"
            except Exception:
                continue
        
        return False, f"Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤ (í¬íŠ¸ {port} ë‹«í˜)"
            
    except ImportError:
        return False, "requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    except Exception as e:
        return False, f"Ollama ì—°ê²° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"


def create_rag_chain(embeddings, llm_model, top_k: int = 3, callbacks=None) -> Tuple[Union[RetrievalQA, None], Union[bool, str]]:
    """RAG ì²´ì¸ ìƒì„±"""
    try:
        print("ğŸš€ RAG ì²´ì¸ ìƒì„± ì‹œì‘...")
        
        # 0. Ollama ì„œë²„ ì—°ê²° ì‚¬ì „ í™•ì¸ (LLM ëª¨ë¸ì´ Ollama ê¸°ë°˜ì¸ ê²½ìš°)
        try:
            # LLM ëª¨ë¸ì´ Ollama ê¸°ë°˜ì¸ì§€ í™•ì¸
            if hasattr(llm_model, '_client') or 'ollama' in str(type(llm_model)).lower():
                print("ğŸ” Ollama ê¸°ë°˜ LLM ëª¨ë¸ ê°ì§€, ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
                ollama_connected, ollama_message = check_ollama_connection()
                if not ollama_connected:
                    return None, f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {ollama_message}\n\nğŸ”§ í•´ê²° ë°©ë²•:\n1. Ollama ë‹¤ìš´ë¡œë“œ: https://ollama.ai/download\n2. Ollama ì‹œì‘: ollama serve\n3. ëª¨ë¸ ì„¤ì¹˜: ollama pull qwen2:7b\n4. í™•ì¸: ollama list"
                print(f"âœ… {ollama_message}")
        except Exception as ollama_check_error:
            print(f"âš ï¸ Ollama ì—°ê²° í™•ì¸ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(ollama_check_error)}")
        
        # 1. Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í™•ì¸ - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print("ğŸ” Elasticsearch ì—°ê²° í™•ì¸ ì¤‘...")
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                return None, f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}\ní•´ê²° ë°©ë²•:\n1. Elasticsearch ì„œë²„ ì‹œì‘: docker-compose up -d elasticsearch\n2. í¬íŠ¸ í™•ì¸: {ELASTICSEARCH_URL}\n3. ë°©í™”ë²½ ì„¤ì • í™•ì¸"
            print(f"âœ… Elasticsearch ì—°ê²° ì„±ê³µ: {message}")
        except ConnectionError as es_conn_error:
            return None, f"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(es_conn_error)}\nElasticsearch ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: {ELASTICSEARCH_URL}"
        except Exception as es_error:
            return None, f"Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(es_error)}"
        
        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print(f"ğŸ“‹ ì¸ë±ìŠ¤ '{INDEX_NAME}' ì¡´ì¬ í™•ì¸ ì¤‘...")
            if not es_client.indices.exists(index=INDEX_NAME):
                return None, f"ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\ní•´ê²° ë°©ë²•:\n1. PDF íŒŒì¼ì„ pdf/ ë””ë ‰í† ë¦¬ì— ì¶”ê°€\n2. PDF ì¸ë±ì‹± ì‹¤í–‰\n3. ë˜ëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ë¦„ í™•ì¸"
            print(f"âœ… ì¸ë±ìŠ¤ '{INDEX_NAME}' ì¡´ì¬ í™•ì¸")
        except Exception as idx_error:
            return None, f"ì¸ë±ìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(idx_error)}"
        
        # ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print("ğŸ“Š ì¸ë±ìŠ¤ ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì¤‘...")
            doc_count = es_client.count(index=INDEX_NAME).get("count", 0)
            if doc_count == 0:
                return None, f"ì¸ë±ìŠ¤ '{INDEX_NAME}'ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ë¬¸ì„œ ìˆ˜: {doc_count})\ní•´ê²° ë°©ë²•:\n1. PDF íŒŒì¼ ì¸ë±ì‹± ì‹¤í–‰\n2. ì¸ë±ìŠ¤ ë°ì´í„° í™•ì¸"
            print(f"âœ… ì¸ë±ìŠ¤ì— {doc_count}ê°œ ë¬¸ì„œ ì¡´ì¬")
        except Exception as count_error:
            return None, f"ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(count_error)}"
        
        # Elasticsearch ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print("ğŸ”— Elasticsearch ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì¤‘...")
            # ElasticsearchStoreì— ì¸ì¦ì´ í¬í•¨ëœ URL ì‚¬ìš©
            vectorstore = ElasticsearchStore(
                embedding=embeddings,
                index_name=INDEX_NAME,
                es_url=ELASTICSEARCH_URL
            )
            print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ (es_url ë°©ì‹)")
        except TypeError as type_error:
            print("âš ï¸ es_url íŒŒë¼ë¯¸í„° ì˜¤ë¥˜, elasticsearch_urlë¡œ ì¬ì‹œë„...")
            # íŒŒë¼ë¯¸í„° ì´ë¦„ ë¬¸ì œì¸ ê²½ìš°
            try:
                vectorstore = ElasticsearchStore(
                    embedding=embeddings,
                    index_name=INDEX_NAME,
                    elasticsearch_url=ELASTICSEARCH_URL
                )
                print("âœ… ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ (elasticsearch_url ë°©ì‹)")
            except ConnectionError as conn_error2:
                return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - ì—°ê²° ì˜¤ë¥˜ (elasticsearch_url): {str(conn_error2)}"
            except TimeoutError as timeout_error2:
                return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - íƒ€ì„ì•„ì›ƒ (elasticsearch_url): {str(timeout_error2)}"
            except Exception as vs_error2:
                import traceback
                error_traceback = traceback.format_exc()
                return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ (elasticsearch_url):\nì˜ˆì™¸ íƒ€ì…: {type(vs_error2).__name__}\nì˜¤ë¥˜ ë©”ì‹œì§€: {str(vs_error2)}\nìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}"
        except ConnectionError as conn_error:
            return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - Elasticsearch ì—°ê²° ì˜¤ë¥˜: {str(conn_error)}\nElasticsearch ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: {ELASTICSEARCH_URL}"
        except TimeoutError as timeout_error:
            return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - ì—°ê²° íƒ€ì„ì•„ì›ƒ: {str(timeout_error)}\nElasticsearch ì„œë²„ ì‘ë‹µì´ ëŠë¦½ë‹ˆë‹¤: {ELASTICSEARCH_URL}"
        except ImportError as import_error:
            return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì˜¤ë¥˜: {str(import_error)}\ní•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        except Exception as vs_error:
            import traceback
            error_traceback = traceback.format_exc()
            return None, f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:\nì˜ˆì™¸ íƒ€ì…: {type(vs_error).__name__}\nì˜¤ë¥˜ ë©”ì‹œì§€: {str(vs_error)}\nElasticsearch URL: {ELASTICSEARCH_URL}\nìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}"
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ
        try:
            print("ğŸ”„ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì¤‘...")
            
            # ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ ì¡°í•©
            base_retriever = vectorstore.as_retriever(
                search_kwargs={
                    "k": top_k * 2,  # ë” ë§ì€ í›„ë³´ ë¬¸ì„œ í™•ë³´
                    "fetch_k": min(top_k * 6, 10000)  # ì´ˆê¸° ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
                }
            )
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì—°ê²°
            
            # ë¦¬ë­í‚¹ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§ ì¶”ê°€ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê°•í™”)
            def enhanced_retrieve(query):
                # 1ì°¨: ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰
                semantic_docs = base_retriever.get_relevant_documents(query)
                
                # 2ì°¨: í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                keyword_results = ElasticsearchManager.keyword_search(query, top_k * 2)
                
                # 3ì°¨: ê´€ë ¨ì„± ê²€ì¦ ë° í‚¤ì›Œë“œ ë§¤ì¹­ ê°•í™”
                query_keywords = query.lower().split()
                scored_docs = []
                
                for doc in semantic_docs:
                    content = doc.page_content.lower()
                    
                    # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                    keyword_score = sum(1 for keyword in query_keywords if keyword in content)
                    
                    # ê´€ë ¨ì„± ì„ê³„ê°’ ì„¤ì • (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
                    min_relevance_score = 0.1  # ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜
                    if keyword_score == 0 and len(doc.page_content) < 50:
                        # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì „í˜€ ì—†ê³  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸
                        continue
                    
                    metadata_score = 0
                    
                    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
                    if hasattr(doc, 'metadata'):
                        if doc.metadata.get('structure_type') == 'ì—…ë¬´ì•ˆë‚´ì„œ' and 'ì—…ë¬´' in query:
                            metadata_score += 2
                        if doc.metadata.get('has_tables') and ('í‘œ' in query or 'ëª©ë¡' in query):
                            metadata_score += 1
                        if doc.metadata.get('category') == 'DOCX' and ('ì•ˆë‚´' in query or 'ì ˆì°¨' in query):
                            metadata_score += 1.5
                    
                    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì™€ ë§¤ì¹­ë˜ë©´ ë³´ë„ˆìŠ¤ ì ìˆ˜
                    filename = doc.metadata.get('filename', '')
                    for kw_result in keyword_results:
                        if kw_result.get('metadata', {}).get('filename') == filename:
                            metadata_score += 1
                            break
                    
                    total_score = keyword_score + metadata_score
                    
                    # ìµœì†Œ ì ìˆ˜ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
                    if total_score >= min_relevance_score:
                        scored_docs.append((doc, total_score))
                
                # ìŠ¤ì½”ì–´ ê¸°ë°˜ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
                scored_docs.sort(key=lambda x: x[1], reverse=True)
                final_docs = [doc for doc, _ in scored_docs[:top_k]]
                
                # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
                if not final_docs:
                    print("âš ï¸ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                return final_docs
            
            # ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„ í´ë˜ìŠ¤ ìƒì„±
            class EnhancedRetriever(BaseRetriever):
                def _get_relevant_documents(self, query: str, *, run_manager=None):
                    return enhanced_retrieve(query)
                
                def get_relevant_documents(self, query):
                    return enhanced_retrieve(query)
            
            retriever = EnhancedRetriever()
            print(f"âœ… í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì™„ë£Œ (ì‹œë§¨í‹± + í‚¤ì›Œë“œ ê²€ìƒ‰, top_k: {top_k})")
        except AttributeError as attr_error:
            return None, f"ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì‹¤íŒ¨ - ì†ì„± ì˜¤ë¥˜: {str(attr_error)}\në²¡í„°ìŠ¤í† ì–´ ê°ì²´ê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        except ValueError as value_error:
            return None, f"ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì‹¤íŒ¨ - ê°’ ì˜¤ë¥˜: {str(value_error)}\nsearch_kwargs ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”."
        except Exception as ret_error:
            return None, f"ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì‹¤íŒ¨ - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(ret_error)}"

        prompt_template = """
    ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    
    **[ì¤‘ìš” ê·œì¹™]**
    1. ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ì‚¬ì „ ì§€ì‹ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
    3. ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìœ¼ë©´, "ì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ë¬¸ì„œ ë‚´ì—ì„œëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    4. ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì—¬ëŸ¬ í•´ì„ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë°˜ë“œì‹œ êµ¬ì²´ì  ì¬ì§ˆì˜ë¥¼ ìš”ì²­í•˜ì„¸ìš”.
    
    **[ë‹µë³€ ë°©ì‹]**
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•´, ë¬¸ì„œì—ì„œ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    - ì§ˆë¬¸ì´ í¬ê´„ì ì´ê¸´ í•˜ì§€ë§Œ ì¼ë°˜ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë¨¼ì € ë‹µë³€í•œ ë’¤ ë¬¸ì„œ ê¸°ë°˜ ì¶”ê°€ ê¶ê¸ˆì¦ì´ë‚˜ í›„ì† ì§ˆë¬¸ì„ 2~3ê°œ ì œì•ˆí•©ë‹ˆë‹¤.
    - ë§Œì•½ ì§ˆë¬¸ì´ ë„ˆë¬´ ëª¨í˜¸í•´ ì¼ë°˜ ë‹µë³€ì¡°ì°¨ ì–´ë ¤ìš°ë©´, 'ì¬ì§ˆì˜ ìš”ì²­ ê·œì¹™'ì— ë”°ë¼ ì˜µì…˜ì„ ì œì•ˆí•´ ì§ˆë¬¸ì„ êµ¬ì²´í™”í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
    
    **[ì¬ì§ˆì˜ ìš”ì²­ ê·œì¹™ ë° í˜•ì‹]**
    - ì§ˆë¬¸ì´ ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì— ê±¸ì³ ìˆì„ ë•Œ, í˜¹ì€ í•µì‹¬ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í˜•ì‹: 
    "ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ì ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì•Œê³  ì‹¶ìœ¼ì‹ ì§€ ì„ íƒí•´ ì£¼ì„¸ìš”:
    1. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 1]
    2. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 2]
    3. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 3]"
    
    **[ì œê³µëœ ë¬¸ì„œ ë‚´ìš©]**
    {context}
    
    **[ì§ˆë¬¸]**
    {question}
    
    **[ë‹µë³€]**

"""
        
        try:
            print("ğŸ“ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì¤‘...")
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            print("âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ")
        except ValueError as prompt_value_error:
            return None, f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì‹¤íŒ¨ - ê°’ ì˜¤ë¥˜: {str(prompt_value_error)}\ninput_variables ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”."
        except Exception as prompt_error:
            return None, f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì‹¤íŒ¨: {str(prompt_error)}"
        
        # QA ì²´ì¸ ìƒì„± - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print("âš™ï¸ QA ì²´ì¸ ìƒì„± ì¤‘...")
            
            # callbacksê°€ ì œê³µëœ ê²½ìš° í¬í•¨
            qa_kwargs = {
                "llm": llm_model,
                "chain_type": "stuff",
                "retriever": retriever,
                "chain_type_kwargs": {"prompt": prompt},
                "return_source_documents": True
            }
            
            if callbacks:
                qa_kwargs["callbacks"] = callbacks
                print(f"âœ… Langfuse ì½œë°± ì¶”ê°€ë¨")
            
            qa_chain = RetrievalQA.from_chain_type(**qa_kwargs)
            print("âœ… QA ì²´ì¸ ìƒì„± ì™„ë£Œ")
        except AttributeError as qa_attr_error:
            return None, f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨ - ì†ì„± ì˜¤ë¥˜: {str(qa_attr_error)}\nLLM ëª¨ë¸ì´ë‚˜ ë¦¬íŠ¸ë¦¬ë²„ê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        except ValueError as qa_value_error:
            return None, f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨ - ê°’ ì˜¤ë¥˜: {str(qa_value_error)}\nchain_typeì´ë‚˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        except ImportError as qa_import_error:
            return None, f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨ - import ì˜¤ë¥˜: {str(qa_import_error)}\nLangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        except Exception as qa_error:
            return None, f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨ - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(qa_error)}"
        
        # ìµœì¢… ê²€ì¦
        if qa_chain is None:
            return None, "QA ì²´ì¸ì´ Noneìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„± ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ - ìƒì„¸í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            print("ğŸ” QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_result = qa_chain({"query": "ì¹´ë“œ ì´ìš©í•œë„ ì¡°ì •ì‚¬ìœ "})
            if test_result is None:
                return None, "QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì‘ë‹µì´ Noneì…ë‹ˆë‹¤."
            print("âœ… QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        except ConnectionError as conn_error:
            if "10061" in str(conn_error):
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {str(conn_error)}\n\ní•´ê²° ë°©ë²•:\n1. Ollama ì„¤ì¹˜: https://ollama.ai/download\n2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘: ollama serve\n3. ëª¨ë¸ ì„¤ì¹˜: ollama pull qwen2:7b\n4. ì„œë¹„ìŠ¤ í™•ì¸: ollama list"
            else:
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ì—°ê²° ì˜¤ë¥˜: {str(conn_error)}"
        except TimeoutError as timeout_error:
            return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - íƒ€ì„ì•„ì›ƒ: {str(timeout_error)}\nOllama ì„œë²„ ì‘ë‹µì´ ëŠë¦½ë‹ˆë‹¤. ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
        except ImportError as import_error:
            return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ëª¨ë“ˆ import ì˜¤ë¥˜: {str(import_error)}\ní•„ìš”í•œ íŒ¨í‚¤ì§€: pip install ollama langchain-ollama"
        except AttributeError as attr_error:
            if "ollama" in str(attr_error).lower():
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - Ollama ì†ì„± ì˜¤ë¥˜: {str(attr_error)}\nOllama í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”."
            else:
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ì†ì„± ì˜¤ë¥˜: {str(attr_error)}"
        except ValueError as value_error:
            return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ê°’ ì˜¤ë¥˜: {str(value_error)}\nëª¨ë¸ ì„¤ì •ì´ë‚˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        except KeyError as key_error:
            return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - í‚¤ ì˜¤ë¥˜: {str(key_error)}\nì„¤ì • íŒŒì¼ì´ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        except Exception as test_error:
            import traceback
            error_traceback = traceback.format_exc()
            
            # httpx.ConnectError íŠ¹ë³„ ì²˜ë¦¬
            if "httpx.ConnectError" in error_traceback or "WinError 10061" in str(test_error):
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - Ollama ì—°ê²° ì˜¤ë¥˜:\nì˜¤ë¥˜: {str(test_error)}\n\nğŸ”§ í•´ê²° ë°©ë²•:\n1. Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜: https://ollama.ai/download\n2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘:\n   - Windows: ollama serve (ë˜ëŠ” ìë™ ì‹œì‘)\n   - ë˜ëŠ” Windows ì„œë¹„ìŠ¤ì—ì„œ Ollama ì‹œì‘\n3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:\n   - ollama pull qwen2:7b\n   - ollama pull solar:10.7b\n4. ì„¤ì¹˜ í™•ì¸: ollama list\n5. í¬íŠ¸ í™•ì¸: netstat -an | findstr 11434"
            else:
                return None, f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:\nì˜ˆì™¸ íƒ€ì…: {type(test_error).__name__}\nì˜¤ë¥˜ ë©”ì‹œì§€: {str(test_error)}\nìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}"
        
        return qa_chain, True
        
    except ConnectionError as conn_error:
        return None, f"ì—°ê²° ì˜¤ë¥˜: {str(conn_error)}\nElasticsearch ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    except TimeoutError as timeout_error:
        return None, f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜: {str(timeout_error)}\nElasticsearch ì„œë²„ ì‘ë‹µì´ ëŠë¦½ë‹ˆë‹¤."
    except ImportError as import_error:
        return None, f"ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì˜¤ë¥˜: {str(import_error)}\ní•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install -r requirements.txt"
    except Exception as e:
        import traceback
        error_traceback = traceback.format_exc()
        return None, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:\nì˜ˆì™¸ íƒ€ì…: {type(e).__name__}\nì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}\nìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}"

# ì „ì—­ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ìˆ˜ (ë‚´ìš©ì€ ì§ì ‘ ì‘ì„±)
prompt_for_refined_query = """
ë‹¹ì‹ ì€ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì¬ì •ì˜í•˜ëŠ” ì „ë¬¸ AI ë¹„ì„œì…ë‹ˆë‹¤.

- **ëª©í‘œ**: ì œê³µëœ ëŒ€í™” ê¸°ë¡ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ë‹¤ë“¬ìœ¼ì„¸ìš”.
- **ê·œì¹™ 1**: ì§ˆë¬¸ì´ ë§¥ë½ê³¼ ê´€ë ¨ ìˆë‹¤ë©´, ì´ì „ ëŒ€í™” ì—†ì´ë„ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ë  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¬ì •ì˜í•˜ì„¸ìš”.
- **ê·œì¹™ 2**: ì§ˆë¬¸ì´ ëŒ€í™” ë§¥ë½ê³¼ ì „í˜€ ê´€ë ¨ ì—†ë‹¤ë©´, ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
- **ê·œì¹™ 3**: ì›ë˜ ì§ˆë¬¸ì˜ ì–´ì¡°ì™€ ì˜ë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”. í‰ì„œë¬¸ì„ ì˜ë¬¸ë¬¸ìœ¼ë¡œ ë°”ê¾¸ê±°ë‚˜ ê·¸ ë°˜ëŒ€ë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
- **ê·œì¹™ 4**: ì¬ì •ì˜ëœ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ë‹¨ì–´, ë¬¸êµ¬, ëŒ€í™”í˜• í‘œí˜„ì„ ì¼ì ˆ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- **ê·œì¹™ 5**: ì›ë˜ ì§ˆë¬¸ì´ë‚˜ ëŒ€í™” ê¸°ë¡ì— ì—†ëŠ” ìƒˆë¡œìš´ ì •ë³´ë‚˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•˜ì—¬ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- **ê·œì¹™ 6**: ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ í•©ë‹ˆë‹¤.

ëŒ€í™” ê¸°ë¡: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""

# prompt_for_query = """
# ë‹¤ìŒ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
# ë‹µë³€ì€ í•œêµ­ì–´(í•œê¸€)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
# ì ˆëŒ€ë¡œ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
# ë‹µë³€ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì ìš©í•´ì£¼ì„¸ìš”.
# ë‹µë³€ì„ í‘œë¡œì‘ì„±ê°€ëŠ¥í•˜ë©´ í‘œë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
# í‘œë¡œ ì‘ì„± ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ëŠ” ì‘ì„±í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
# ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.


# ë¬¸ì„œ ë‚´ìš©: {context}

# ì§ˆë¬¸: {question}

# ë‹µë³€:
# """

prompt_for_query = """
ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**[ì¤‘ìš” ê·œì¹™]**
1.  **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€:** ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
2.  **ì™¸ë¶€ ì§€ì‹ ê¸ˆì§€:** ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
3.  **ì •ë³´ ë¶€ì¬ ì‹œ ë‹µë³€:** ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìœ¼ë©´, "ì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ë¬¸ì„œ ë‚´ì—ì„œëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4.  **ëª¨í˜¸ì„± ì²˜ë¦¬:** ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì—¬ëŸ¬ í•´ì„ì´ ê°€ëŠ¥í•˜ë©´, ì•„ë˜ 'ì¬ì§ˆì˜ ìš”ì²­ ê·œì¹™'ì— ë”°ë¼ ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ìš”ì²­í•˜ì„¸ìš”.

**[ë‹µë³€ ë°©ì‹]**
1.  **ë‹µë³€ í˜•ì‹:**
    - ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    - ë‚´ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ë©´ í‘œ(í…Œì´ë¸”)ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ì„œìˆ í˜•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    - ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹µë³€ì— ì ì ˆí•œ ì¤„ ë°”ê¿ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
2.  **í¬ê´„ì  ì§ˆë¬¸ ëŒ€ì‘:**
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ì†Œ í¬ê´„ì ì´ë”ë¼ë„ ì¼ë°˜ì ì¸ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë¨¼ì € ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•©ë‹ˆë‹¤.
    - ê·¸ í›„, ì‚¬ìš©ìê°€ ë” ê¶ê¸ˆí•´í•  ë§Œí•œ ë‚´ìš©ì„ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì—¬ 2~3ê°œì˜ êµ¬ì²´ì ì¸ í›„ì† ì§ˆë¬¸ì„ ì œì•ˆí•©ë‹ˆë‹¤.
3.  **ëª¨í˜¸í•œ ì§ˆë¬¸ ëŒ€ì‘:**
    - ì§ˆë¬¸ì´ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê±°ë‚˜ í•µì‹¬ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì´ ì–´ë ¤ìš¸ ê²½ìš°, 'ì¬ì§ˆì˜ ìš”ì²­ ê·œì¹™'ì— ë”°ë¼ ì§ˆë¬¸ì„ êµ¬ì²´í™”í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

**[ì¬ì§ˆì˜ ìš”ì²­ ê·œì¹™ ë° í˜•ì‹]**
- ì§ˆë¬¸ì´ ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì— ê±¸ì³ ìˆì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- í˜•ì‹:
"ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ì ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì•Œê³  ì‹¶ìœ¼ì‹ ì§€ ì„ íƒí•´ ì£¼ì„¸ìš”:
1. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 1]
2. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 2]
3. [ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ 3]"

**[ì œê³µëœ ë¬¸ì„œ ë‚´ìš©]**
{context}

**[ì§ˆë¬¸]**
{question}

**[ë‹µë³€]**
"""



prompt_for_context_summary = """
ë‹¤ìŒ 'ë‚´ìš©'ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´(í•œê¸€)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì ˆëŒ€ë¡œ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.


ë‚´ìš©: {context}

ë‹µë³€:
"""

def create_llm_chain(llm_model, prompt_template, input_variables=None):
    """
    LLMChain ìƒì„± í•¨ìˆ˜ (ì˜ˆì™¸ì²˜ë¦¬ í¬í•¨)
    :param llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸
    :param prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    :param input_variables: í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["context", "question", "history"])
    :return: LLMChain ê°ì²´ ë˜ëŠ” None

    @Description: LLM + PromptTemplateì„ ê²°í•©í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì²´ì¸ ìƒì„±
    - LLM ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ê²°í•©
    - ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨
    - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ LLMChain ê°ì²´ ë°˜í™˜
    """
    try:
        if input_variables is None:
            input_variables = ["context", "question"]
        try:
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=input_variables
            )
        except Exception as e:
            print(f"âŒ PromptTemplate ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
        try:
            chain = LLMChain(
                llm=llm_model,
                prompt=prompt
            )
        except Exception as e:
            print(f"âŒ LLMChain ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
        return chain
    except Exception as e:
        print(f"âŒ create_llm_chain ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        return None

def create_retriever(embedding_model, top_k=3):
    """
    ê¸°ë³¸ Elasticsearch ê¸°ë°˜ Retriever ìƒì„± í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    :param embedding_model: ì„ë² ë”© ëª¨ë¸ ê°ì²´
    :param top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
    :return: Retriever ê°ì²´ ë˜ëŠ” None
    """
    try:
        from core.config import ELASTICSEARCH_URL, INDEX_NAME
        from langchain_community.vectorstores import ElasticsearchStore
        es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
        if not success:
            print(f"âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            return None
        if not es_client.indices.exists(index=INDEX_NAME):
            print(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”.")
            return None
        doc_count = es_client.count(index=INDEX_NAME).get("count", 0)
        if doc_count == 0:
            print(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”.")
            return None
        vectorstore = ElasticsearchStore(
            embedding=embedding_model,
            index_name=INDEX_NAME,
            es_url=ELASTICSEARCH_URL,
        )
        retriever = vectorstore.as_retriever(
            search_kwargs={
                "k": top_k,
                "fetch_k": min(top_k * 3, 10000)
            }
        )
        print("âœ… ê¸°ë³¸ Retriever ìƒì„± ì„±ê³µ")
        return retriever
    except Exception as e:
        print(f"âŒ Retriever ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

def create_enhanced_retriever(embedding_model, top_k=3):
    """
    ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ Retriever ìƒì„± í•¨ìˆ˜
    - ì‹œë§¨í‹± + í‚¤ì›Œë“œ ê²€ìƒ‰ ì¡°í•©
    - ì •êµí•œ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ
    - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜
    - ê´€ë ¨ì„± ì„ê³„ê°’ìœ¼ë¡œ í™˜ê° ë°©ì§€
    
    :param embedding_model: ì„ë² ë”© ëª¨ë¸ ê°ì²´
    :param top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
    :return: Enhanced Retriever ê°ì²´ ë˜ëŠ” None
    """
    try:
        from core.config import ELASTICSEARCH_URL, INDEX_NAME
        from langchain_community.vectorstores import ElasticsearchStore
        
        # Elasticsearch ì—°ê²° í™•ì¸
        es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
        if not success:
            print(f"âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            return None
            
        if not es_client.indices.exists(index=INDEX_NAME):
            print(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
            
        doc_count = es_client.count(index=INDEX_NAME).get("count", 0)
        if doc_count == 0:
            print(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = ElasticsearchStore(
            embedding=embedding_model,
            index_name=INDEX_NAME,
            es_url=ELASTICSEARCH_URL,
        )
        
        # ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ ë¦¬íŠ¸ë¦¬ë²„ (í™•ì¥ëœ í›„ë³´êµ°)
        base_retriever = vectorstore.as_retriever(
            search_kwargs={
                "k": top_k * 2,  # ë” ë§ì€ í›„ë³´ í™•ë³´
                "fetch_k": min(top_k * 6, 10000)  # ì´ˆê¸° ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
            }
        )
        
        def enhanced_retrieve(query):
            """ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§"""
            try:
                # 1ì°¨: ì‹œë§¨í‹± ê²€ìƒ‰
                semantic_docs = base_retriever.get_relevant_documents(query)
                
                # 2ì°¨: í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                keyword_results = ElasticsearchManager.keyword_search(query, top_k * 2)
                
                # 3ì°¨: ì •êµí•œ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ
                query_keywords = query.lower().split()
                scored_docs = []
                
                for doc in semantic_docs:
                    content = doc.page_content.lower()
                    
                    # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                    keyword_score = sum(1 for keyword in query_keywords if keyword in content)
                    
                    # ê´€ë ¨ì„± ì„ê³„ê°’ ì„¤ì • (í™˜ê° ë°©ì§€)
                    min_relevance_score = 0.1
                    if keyword_score == 0 and len(doc.page_content) < 50:
                        # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì „í˜€ ì—†ê³  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸
                        continue
                    
                    metadata_score = 0
                    
                    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
                    if hasattr(doc, 'metadata') and doc.metadata:
                        # êµ¬ì¡° íƒ€ì… ê¸°ë°˜ ê°€ì¤‘ì¹˜
                        if doc.metadata.get('structure_type') == 'ì—…ë¬´ì•ˆë‚´ì„œ' and 'ì—…ë¬´' in query:
                            metadata_score += 2
                        if doc.metadata.get('content_type') == 'ì ˆì°¨ì•ˆë‚´' and ('ì ˆì°¨' in query or 'ë°©ë²•' in query):
                            metadata_score += 1.5
                        
                        # í‘œ í¬í•¨ ë¬¸ì„œ ê°€ì¤‘ì¹˜
                        if doc.metadata.get('has_tables') and ('í‘œ' in query or 'ëª©ë¡' in query or 'ê¸°ì¤€' in query):
                            metadata_score += 1
                        if doc.metadata.get('has_table') and ('í‘œ' in query or 'ëª©ë¡' in query):
                            metadata_score += 1
                            
                        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                        if doc.metadata.get('category') == 'DOCX' and ('ì•ˆë‚´' in query or 'ì ˆì°¨' in query):
                            metadata_score += 1.5
                        if doc.metadata.get('category') == 'PDF' and ('ê·œì •' in query or 'ì •ì±…' in query):
                            metadata_score += 1.2
                            
                        # ì œëª©/í—¤ë”© ë§¤ì¹­ ê°€ì¤‘ì¹˜
                        title = doc.metadata.get('title', '').lower()
                        heading = doc.metadata.get('heading', '').lower()
                        for keyword in query_keywords:
                            if keyword in title:
                                metadata_score += 2
                            if keyword in heading:
                                metadata_score += 1.5
                                
                        # í‚¤ì›Œë“œ í•„ë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜
                        keywords_field = doc.metadata.get('keywords', '').lower()
                        for keyword in query_keywords:
                            if keyword in keywords_field:
                                metadata_score += 1
                    
                    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì™€ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                    filename = doc.metadata.get('filename', '') if hasattr(doc, 'metadata') else ''
                    for kw_result in keyword_results:
                        if isinstance(kw_result, dict):
                            kw_filename = kw_result.get('metadata', {}).get('filename', '')
                        else:
                            kw_filename = getattr(kw_result, 'metadata', {}).get('filename', '') if hasattr(kw_result, 'metadata') else ''
                        
                        if kw_filename and kw_filename == filename:
                            metadata_score += 1
                            break
                    
                    total_score = keyword_score + metadata_score
                    
                    # ìµœì†Œ ì ìˆ˜ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨ (í™˜ê° ë°©ì§€)
                    if total_score >= min_relevance_score:
                        scored_docs.append((doc, total_score))
                
                # ìŠ¤ì½”ì–´ ê¸°ë°˜ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
                scored_docs.sort(key=lambda x: x[1], reverse=True)
                final_docs = [doc for doc, _ in scored_docs[:top_k]]
                
                # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
                if not final_docs:
                    print("âš ï¸ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                return final_docs
                
            except Exception as e:
                print(f"âŒ ê³ ë„í™”ëœ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                return base_retriever.get_relevant_documents(query)[:top_k]
        
        # ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„ í´ë˜ìŠ¤ ìƒì„±
        class EnhancedRetriever(BaseRetriever):
            def _get_relevant_documents(self, query: str, *, run_manager=None):
                return enhanced_retrieve(query)
                
            def get_relevant_documents(self, query):
                return enhanced_retrieve(query)
        
        enhanced_retriever = EnhancedRetriever()
        print(f"âœ… ê³ ë„í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì™„ë£Œ (top_k: {top_k})")
        return enhanced_retriever
        
    except Exception as e:
        print(f"âŒ ê³ ë„í™”ëœ Retriever ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None