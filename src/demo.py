import streamlit as st
import os
import time
import psutil
from datetime import datetime
from tempfile import NamedTemporaryFile

# --- ë³€ê²½/ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
# LLMê³¼ ì„ë² ë”© ëª¨ë¸ì„ ë¡œì»¬ë¡œ ëŒë¦¬ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings
# -----------------------------

# LangChain ê´€ë ¨ ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- OpenAI API í‚¤ ì„¤ì • ë¶€ë¶„ì€ ì´ì œ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ---

# ì„±ëŠ¥ ì¸¡ì • ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
class PerformanceTracker:
    def __init__(self):
        self.metrics = {}
        self.current_process = psutil.Process()  # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ê°ì²´ ì €ì¥
        
    def get_ollama_processes(self):
        """Ollama ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜"""
        ollama_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent']):
            try:
                if 'ollama' in proc.info['name'].lower():
                    ollama_processes.append(proc)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        return ollama_processes
    
    def get_total_memory_usage(self):
        """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ + Ollama í”„ë¡œì„¸ìŠ¤ë“¤ì˜ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        # í˜„ì¬ Python í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
        python_memory = self.current_process.memory_info().rss / 1024 / 1024
        
        # Ollama í”„ë¡œì„¸ìŠ¤ë“¤ ë©”ëª¨ë¦¬
        ollama_memory = 0
        ollama_processes = self.get_ollama_processes()
        for proc in ollama_processes:
            try:
                ollama_memory += proc.memory_info().rss / 1024 / 1024
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return {
            'python_memory': python_memory,
            'ollama_memory': ollama_memory,
            'total_memory': python_memory + ollama_memory,
            'ollama_process_count': len(ollama_processes)
        }
        
    def start_timer(self, task_name):
        """ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ê°œì„ ëœ ë²„ì „)"""
        memory_info = self.get_total_memory_usage()
        self.metrics[task_name] = {
            'start_time': time.time(),
            'start_python_memory': memory_info['python_memory'],
            'start_ollama_memory': memory_info['ollama_memory'],
            'start_total_memory': memory_info['total_memory'],
            'start_cpu_percent': self.current_process.cpu_percent()  # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ CPU ì‚¬ìš©ë¥ 
        }
        
    def end_timer(self, task_name):
        """ì‘ì—… ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ê²°ê³¼ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        if task_name in self.metrics:
            end_time = time.time()
            memory_info = self.get_total_memory_usage()
            
            self.metrics[task_name].update({
                'end_time': end_time,
                'end_python_memory': memory_info['python_memory'],
                'end_ollama_memory': memory_info['ollama_memory'],
                'end_total_memory': memory_info['total_memory'],
                'duration': end_time - self.metrics[task_name]['start_time'],
                'python_memory_used': memory_info['python_memory'] - self.metrics[task_name]['start_python_memory'],
                'ollama_memory_used': memory_info['ollama_memory'] - self.metrics[task_name]['start_ollama_memory'],
                'total_memory_used': memory_info['total_memory'] - self.metrics[task_name]['start_total_memory'],
                'ollama_process_count': memory_info['ollama_process_count']
            })
            
            return self.metrics[task_name]
        return None
    
    def get_summary(self):
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        summary = {}
        for task, metrics in self.metrics.items():
            if 'duration' in metrics:
                summary[task] = {
                    'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': round(metrics['duration'], 2),
                    'Python ë©”ëª¨ë¦¬ (MB)': round(metrics['python_memory_used'], 2),
                    'Ollama ë©”ëª¨ë¦¬ (MB)': round(metrics['ollama_memory_used'], 2),
                    'ì´ ë©”ëª¨ë¦¬ (MB)': round(metrics['total_memory_used'], 2),
                    'Ollama í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['ollama_process_count'],
                    'ì™„ë£Œì‹œê°„': datetime.fromtimestamp(metrics['end_time']).strftime('%H:%M:%S')
                }
        return summary

# ì „ì—­ ì„±ëŠ¥ ì¶”ì ê¸° ì´ˆê¸°í™”
if "performance_tracker" not in st.session_state:
    st.session_state.performance_tracker = PerformanceTracker()

# 1. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜
def process_pdf_and_create_qa_chain(pdf_file, tracker):
    """PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ QA ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    
    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ì¶”ì  ì‹œì‘
    tracker.start_timer("ì „ì²´_PDF_ì²˜ë¦¬")
    
    # 1. PDF ë¡œë”©
    tracker.start_timer("PDF_ë¡œë”©")
    with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    tracker.end_timer("PDF_ë¡œë”©")
    
    # ë¬¸ì„œ ì •ë³´ ì €ì¥
    doc_count = len(documents)
    total_chars = sum(len(doc.page_content) for doc in documents)

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    tracker.start_timer("í…ìŠ¤íŠ¸_ë¶„í• ")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)
    chunk_count = len(docs)
    tracker.end_timer("í…ìŠ¤íŠ¸_ë¶„í• ")

    # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    tracker.start_timer("ì„ë² ë”©_ëª¨ë¸_ë¡œë”©")
    model_name = "jhgan/ko-sroberta-multitask"
    model_kwargs = {'device': 'cpu'} # CPU ì‚¬ìš©, GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    tracker.end_timer("ì„ë² ë”©_ëª¨ë¸_ë¡œë”©")
    
    # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì„ë² ë”© ìƒì„±)
    tracker.start_timer("ë²¡í„°_ìŠ¤í† ì–´_ìƒì„±")
    vectorstore = FAISS.from_documents(docs, embeddings)
    tracker.end_timer("ë²¡í„°_ìŠ¤í† ì–´_ìƒì„±")

    # 5. QA ì²´ì¸ êµ¬ì„±
    tracker.start_timer("QA_ì²´ì¸_êµ¬ì„±")
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

    prompt_template = """
    ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    {context}

    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # LLM ëª¨ë¸ ë¡œë”©
    llm = ChatOllama(model="llama3:8b", temperature=0)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    tracker.end_timer("QA_ì²´ì¸_êµ¬ì„±")
    
    # ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ
    tracker.end_timer("ì „ì²´_PDF_ì²˜ë¦¬")
    
    # íŒŒì¼ ì •ë¦¬
    os.remove(tmp_file_path)
    
    # ì²˜ë¦¬ í†µê³„ ì €ì¥
    processing_stats = {
        'ë¬¸ì„œ_í˜ì´ì§€_ìˆ˜': doc_count,
        'ì´_ë¬¸ì_ìˆ˜': total_chars,
        'ì²­í¬_ìˆ˜': chunk_count,
        'ê²€ìƒ‰_ë¬¸ì„œ_ìˆ˜': 3
    }
    
    return qa_chain, processing_stats

# 2. Streamlit UI êµ¬ì„±
st.set_page_config(page_title="ë¡œì»¬ PDF QA ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“„ ë¡œì»¬ PDF ê¸°ë°˜ QA ì±—ë´‡ (Ollama)")

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í† ê¸€
show_performance = st.sidebar.checkbox("ğŸ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í™œì„±í™”", value=True)

# --- OpenAI API í‚¤ ì…ë ¥ ë¶€ë¶„ì´ í•„ìš” ì—†ì–´ì¡ŒìŠµë‹ˆë‹¤. ---
st.info("ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ì²˜ë¦¬' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë™ì¼)
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing_stats" not in st.session_state:
    st.session_state.processing_stats = None

# ì‚¬ì´ë“œë°”ì— PDF ì—…ë¡œë” ë°°ì¹˜ (ë™ì¼)
with st.sidebar:
    st.header("PDF ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì§ˆë¬¸í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
    if st.button("PDF ì²˜ë¦¬"):
        if uploaded_file is not None:
            with st.spinner("ë¡œì»¬ ëª¨ë¸ë¡œ PDFë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... (ì²˜ìŒì—ëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                # ì„±ëŠ¥ ì¶”ì ê¸° ì´ˆê¸°í™”
                st.session_state.performance_tracker = PerformanceTracker()
                
                # PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„±
                qa_chain, processing_stats = process_pdf_and_create_qa_chain(
                    uploaded_file, 
                    st.session_state.performance_tracker
                )
                
                st.session_state.qa_chain = qa_chain
                st.session_state.processing_stats = processing_stats
                st.session_state.messages = [{"role": "assistant", "content": "PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]
            
            st.success("PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ì„±ëŠ¥ ê²°ê³¼ í‘œì‹œ
            if show_performance:
                st.subheader("ğŸ“Š PDF ì²˜ë¦¬ ì„±ëŠ¥ ë¶„ì„")
                
                # ì²˜ë¦¬ í†µê³„
                st.write("**ë¬¸ì„œ ì •ë³´:**")
                stats_col1, stats_col2 = st.columns(2)
                with stats_col1:
                    st.metric("í˜ì´ì§€ ìˆ˜", f"{processing_stats['ë¬¸ì„œ_í˜ì´ì§€_ìˆ˜']}ê°œ")
                    st.metric("ì²­í¬ ìˆ˜", f"{processing_stats['ì²­í¬_ìˆ˜']}ê°œ")
                with stats_col2:
                    st.metric("ì´ ë¬¸ì ìˆ˜", f"{processing_stats['ì´_ë¬¸ì_ìˆ˜']:,}ì")
                    st.metric("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜", f"{processing_stats['ê²€ìƒ‰_ë¬¸ì„œ_ìˆ˜']}ê°œ")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                perf_summary = st.session_state.performance_tracker.get_summary()
                st.write("**ì²˜ë¦¬ ì‹œê°„ ë¶„ì„:**")
                
                for task, metrics in perf_summary.items():
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric(f"{task}", f"{metrics['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆ")
                    with col2:
                        st.metric("Python ë©”ëª¨ë¦¬", f"{metrics['Python ë©”ëª¨ë¦¬ (MB)']}MB")
                    with col3:
                        st.metric("Ollama ë©”ëª¨ë¦¬", f"{metrics['Ollama ë©”ëª¨ë¦¬ (MB)']}MB")
                    with col4:
                        st.metric("ì´ ë©”ëª¨ë¦¬", f"{metrics['ì´ ë©”ëª¨ë¦¬ (MB)']}MB")
                
        else:
            st.error("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ ë° ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ë™ì¼)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ (ì‚¬ì´ë“œë°”)
if show_performance and hasattr(st.session_state, 'performance_tracker'):
    with st.sidebar:
        st.header("ğŸ“ˆ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ")
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        perf_summary = st.session_state.performance_tracker.get_summary()
        if perf_summary:
            st.subheader("ì „ì²´ ì‘ì—… ìš”ì•½")
            total_time = sum(metrics['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'] for metrics in perf_summary.values())
            total_python_memory = sum(metrics['Python ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            total_ollama_memory = sum(metrics['Ollama ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            total_memory = sum(metrics['ì´ ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            
            st.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
            st.metric("Python ë©”ëª¨ë¦¬ ì´í•©", f"{total_python_memory:.2f}MB")
            st.metric("Ollama ë©”ëª¨ë¦¬ ì´í•©", f"{total_ollama_memory:.2f}MB")
            st.metric("ì „ì²´ ë©”ëª¨ë¦¬ ì´í•©", f"{total_memory:.2f}MB")
            
            # ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° ì‘ì—…
            if perf_summary:
                slowest_task = max(perf_summary.items(), key=lambda x: x[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'])
                st.metric("ê°€ì¥ ëŠë¦° ì‘ì—…", f"{slowest_task[0]}: {slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆ")
        
        # í˜„ì¬ ì‹œìŠ¤í…œ ì •ë³´ (ê°œì„ ëœ ë²„ì „)
        st.subheader("ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ì •ë³´")
        memory_info = st.session_state.performance_tracker.get_total_memory_usage()
        cpu_percent = psutil.cpu_percent(interval=0.1)  # 0.1ì´ˆë¡œ ë‹¨ì¶•
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Python í”„ë¡œì„¸ìŠ¤", f"{memory_info['python_memory']:.1f}MB")
            st.metric("Ollama í”„ë¡œì„¸ìŠ¤", f"{memory_info['ollama_memory']:.1f}MB")
        with col2:
            st.metric("ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©", f"{memory_info['total_memory']:.1f}MB")
            st.metric("ì‹œìŠ¤í…œ CPU ì‚¬ìš©ë¥ ", f"{cpu_percent:.1f}%")
            
        # Ollama í”„ë¡œì„¸ìŠ¤ ì •ë³´
        if memory_info['ollama_process_count'] > 0:
            st.info(f"ğŸ¤– í™œì„± Ollama í”„ë¡œì„¸ìŠ¤: {memory_info['ollama_process_count']}ê°œ")

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.qa_chain is not None:
            with st.spinner("ë¡œì»¬ LLMì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                # ì§ˆë¬¸ ì‘ë‹µ ì„±ëŠ¥ ì¸¡ì •
                if show_performance:
                    st.session_state.performance_tracker.start_timer("ì§ˆë¬¸_ì‘ë‹µ")
                
                response = st.session_state.qa_chain({"query": prompt})
                
                if show_performance:
                    qa_metrics = st.session_state.performance_tracker.end_timer("ì§ˆë¬¸_ì‘ë‹µ")
                
                st.markdown(response["result"])
                
                # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                if show_performance and qa_metrics:
                    with st.expander("âš¡ ì‘ë‹µ ì„±ëŠ¥ ì •ë³´"):
                        perf_col1, perf_col2, perf_col3 = st.columns(3)
                        with perf_col1:
                            st.metric("ì‘ë‹µ ì‹œê°„", f"{qa_metrics['duration']:.2f}ì´ˆ")
                            st.metric("ì™„ë£Œ ì‹œê°„", datetime.fromtimestamp(qa_metrics['end_time']).strftime('%H:%M:%S'))
                        with perf_col2:
                            st.metric("Python ë©”ëª¨ë¦¬", f"{qa_metrics['python_memory_used']:.2f}MB")
                            st.metric("Ollama ë©”ëª¨ë¦¬", f"{qa_metrics['ollama_memory_used']:.2f}MB")
                        with perf_col3:
                            st.metric("ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©", f"{qa_metrics['total_memory_used']:.2f}MB")
                            # ëŒ€ëµì ì¸ ì²˜ë¦¬ ì†ë„ ê³„ì‚°
                            if qa_metrics['duration'] > 0:
                                chars_per_sec = len(prompt) / qa_metrics['duration']
                                st.metric("ì²˜ë¦¬ ì†ë„", f"{chars_per_sec:.1f} ë¬¸ì/ì´ˆ")
                
                with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                    for i, doc in enumerate(response["source_documents"]):
                        st.write(f"**ë¬¸ì„œ {i+1}:**")
                        st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
                        st.write("---")
        else:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    if st.session_state.qa_chain is not None:
        st.session_state.messages.append({"role": "assistant", "content": response["result"]})
