import streamlit as st
import os
import time
import psutil
import uuid
from datetime import datetime
from tempfile import NamedTemporaryFile

# .env íŒŒì¼ ë¡œë”©
try:
    from dotenv import load_dotenv
    load_dotenv()  # .env íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œ
except ImportError:
    pass  # python-dotenvê°€ ì—†ì–´ë„ ê³„ì† ì§„í–‰

# --- Langsmith ì¶”ì ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
try:
    from langsmith import Client
    from langchain.callbacks.tracers import LangChainTracer
    from langchain.callbacks.manager import CallbackManager
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False

# --- ë³€ê²½/ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
# LLMê³¼ ì„ë² ë”© ëª¨ë¸ì„ ë¡œì»¬ë¡œ ëŒë¦¬ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings
# -----------------------------

# LangChain ê´€ë ¨ ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- OpenAI API í‚¤ ì„¤ì • ë¶€ë¶„ì€ ì´ì œ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ---

# Langsmith ì„¤ì • í•¨ìˆ˜
def setup_langsmith():
    """Langsmith ì¶”ì  ì„¤ì •"""
    if not LANGSMITH_AVAILABLE:
        return None, False
        
    # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Streamlit secretsì—ì„œ Langsmith API í‚¤ ê°€ì ¸ì˜¤ê¸°
    try:
        langsmith_api_key = os.getenv("LANGSMITH_API_KEY") or st.secrets.get("LANGSMITH_API_KEY", "")
        langsmith_project = os.getenv("LANGSMITH_PROJECT", "pdf-qa-hybrid-monitoring")
        langsmith_endpoint = os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")  # ê¸°ë³¸ê°’ í¬í•¨
        
        if langsmith_api_key:
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_ENDPOINT"] = langsmith_endpoint  # .envì—ì„œ ê°€ì ¸ì˜¨ ê°’ ì‚¬ìš©
            os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
            os.environ["LANGCHAIN_PROJECT"] = langsmith_project
            
            # Langsmith tracer ì„¤ì •
            tracer = LangChainTracer(project_name=langsmith_project)
            callback_manager = CallbackManager([tracer])
            return callback_manager, True
        else:
            return None, False
    except Exception as e:
        st.warning(f"âš ï¸ Langsmith ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, False

# í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤
class HybridPerformanceTracker:
    def __init__(self):
        # ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¶”ì 
        self.system_tracker = PerformanceTracker()
        
        # Langsmith ì„¤ì •
        self.langsmith_callback, self.langsmith_enabled = setup_langsmith()
        
        # ì¶”ì  ë°ì´í„° í†µí•©
        self.hybrid_metrics = {
            'system_metrics': {},
            'langsmith_sessions': [],
            'combined_insights': {}
        }
        
    def get_langsmith_status(self):
        """Langsmith ìƒíƒœ ë°˜í™˜"""
        return {
            'available': LANGSMITH_AVAILABLE,
            'enabled': self.langsmith_enabled,
            'project': os.getenv("LANGSMITH_PROJECT", "pdf-qa-hybrid-monitoring")
        }
    
    def track_preprocessing_stage(self, stage_name):
        """ì „ì²˜ë¦¬ ë‹¨ê³„ëŠ” ì»¤ìŠ¤í…€ ì¶”ì  (ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¤‘ì‹¬)"""
        return self.system_tracker.start_timer(stage_name)
    
    def end_preprocessing_stage(self, stage_name):
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ì¢…ë£Œ"""
        metrics = self.system_tracker.end_timer(stage_name)
        if metrics:
            self.hybrid_metrics['system_metrics'][stage_name] = metrics
        return metrics
    
    def track_llm_inference(self, qa_chain, query, metadata=None):
        """LLM ì¶”ë¡ ì€ Langsmith + ì»¤ìŠ¤í…€ ì¶”ì  ì¡°í•©"""
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¶”ì  ì‹œì‘
        self.system_tracker.start_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        if metadata is None:
            metadata = {}
        
        enhanced_metadata = {
            **metadata,
            'session_id': st.session_state.get('session_id', 'unknown'),
            'timestamp': datetime.now().isoformat(),
            'query_length': len(query),
            'model': 'llama3:8b'
        }
        
        try:
            if self.langsmith_enabled and self.langsmith_callback:
                # Langsmith ì¶”ì ê³¼ í•¨ê»˜ ì‹¤í–‰
                response = qa_chain(
                    {"query": query}, 
                    callbacks=self.langsmith_callback.handlers,
                    metadata=enhanced_metadata
                )
            else:
                # Langsmith ì—†ì´ ì‹¤í–‰
                response = qa_chain({"query": query})
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¶”ì  ì¢…ë£Œ
            system_metrics = self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            
            # ê²°ê³¼ í†µí•©
            combined_result = {
                'response': response,
                'system_metrics': system_metrics,
                'langsmith_enabled': self.langsmith_enabled,
                'metadata': enhanced_metadata
            }
            
            return combined_result
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì‹œìŠ¤í…œ ì¶”ì  ì¢…ë£Œ
            self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            st.error(f"LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def get_system_summary(self):
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìš”ì•½ (ê¸°ì¡´ ë°©ì‹)"""
        return self.system_tracker.get_summary()
    
    def get_hybrid_insights(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸"""
        system_summary = self.get_system_summary()
        
        insights = {
            'system_performance': system_summary,
            'langsmith_status': self.get_langsmith_status(),
            'recommendations': self._generate_recommendations(system_summary)
        }
        
        return insights
    
    def _generate_recommendations(self, system_summary):
        """ì„±ëŠ¥ ë°ì´í„° ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if system_summary:
            # ê°€ì¥ ëŠë¦° ì‘ì—… ì‹ë³„
            slowest_task = max(system_summary.items(), 
                             key=lambda x: x[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'], 
                             default=(None, {'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': 0}))
            
            if slowest_task[0] and slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'] > 10:
                recommendations.append(f"âš ï¸ {slowest_task[0]}ì´ {slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆë¡œ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
                
                if 'ë²¡í„°_ìŠ¤í† ì–´_ìƒì„±' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ ë¬¸ì„œ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
                elif 'LLM_ì¶”ë¡ ' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ Ollama ì„¤ì •ì„ ìµœì í™”í•´ë³´ì„¸ìš”.")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            total_memory = sum(metrics.get('ì´ ë©”ëª¨ë¦¬ (MB)', 0) for metrics in system_summary.values())
            if total_memory > 8000:  # 8GB ì´ìƒ
                recommendations.append(f"ğŸ”¥ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {total_memory:.1f}MBë¡œ ë†’ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        return recommendations

# ì„±ëŠ¥ ì¸¡ì • ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
class PerformanceTracker:
    def __init__(self):
        self.metrics = {}
        self.current_process = psutil.Process()  # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ê°ì²´ ì €ì¥
        
    def get_ollama_processes(self):
        """Ollama ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜"""
        ollama_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent']):
            try:
                if 'ollama' in proc.info['name'].lower():
                    ollama_processes.append(proc)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        return ollama_processes
    
    def get_total_memory_usage(self):
        """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ + Ollama í”„ë¡œì„¸ìŠ¤ë“¤ì˜ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        # í˜„ì¬ Python í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
        python_memory = self.current_process.memory_info().rss / 1024 / 1024
        
        # Ollama í”„ë¡œì„¸ìŠ¤ë“¤ ë©”ëª¨ë¦¬
        ollama_memory = 0
        ollama_processes = self.get_ollama_processes()
        for proc in ollama_processes:
            try:
                ollama_memory += proc.memory_info().rss / 1024 / 1024
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return {
            'python_memory': python_memory,
            'ollama_memory': ollama_memory,
            'total_memory': python_memory + ollama_memory,
            'ollama_process_count': len(ollama_processes)
        }
        
    def start_timer(self, task_name):
        """ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ê°œì„ ëœ ë²„ì „)"""
        memory_info = self.get_total_memory_usage()
        self.metrics[task_name] = {
            'start_time': time.time(),
            'start_python_memory': memory_info['python_memory'],
            'start_ollama_memory': memory_info['ollama_memory'],
            'start_total_memory': memory_info['total_memory'],
            'start_cpu_percent': self.current_process.cpu_percent()  # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ CPU ì‚¬ìš©ë¥ 
        }
        
    def end_timer(self, task_name):
        """ì‘ì—… ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ê²°ê³¼ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        if task_name in self.metrics:
            end_time = time.time()
            memory_info = self.get_total_memory_usage()
            
            self.metrics[task_name].update({
                'end_time': end_time,
                'end_python_memory': memory_info['python_memory'],
                'end_ollama_memory': memory_info['ollama_memory'],
                'end_total_memory': memory_info['total_memory'],
                'duration': end_time - self.metrics[task_name]['start_time'],
                'python_memory_used': memory_info['python_memory'] - self.metrics[task_name]['start_python_memory'],
                'ollama_memory_used': memory_info['ollama_memory'] - self.metrics[task_name]['start_ollama_memory'],
                'total_memory_used': memory_info['total_memory'] - self.metrics[task_name]['start_total_memory'],
                'ollama_process_count': memory_info['ollama_process_count']
            })
            
            return self.metrics[task_name]
        return None
    
    def get_summary(self):
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        summary = {}
        for task, metrics in self.metrics.items():
            if 'duration' in metrics:
                summary[task] = {
                    'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': round(metrics['duration'], 2),
                    'Python ë©”ëª¨ë¦¬ (MB)': round(metrics['python_memory_used'], 2),
                    'Ollama ë©”ëª¨ë¦¬ (MB)': round(metrics['ollama_memory_used'], 2),
                    'ì´ ë©”ëª¨ë¦¬ (MB)': round(metrics['total_memory_used'], 2),
                    'Ollama í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['ollama_process_count'],
                    'ì™„ë£Œì‹œê°„': datetime.fromtimestamp(metrics['end_time']).strftime('%H:%M:%S')
                }
        return summary

# ì „ì—­ í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¶”ì ê¸° ì´ˆê¸°í™”
if "hybrid_tracker" not in st.session_state:
    st.session_state.hybrid_tracker = HybridPerformanceTracker()

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ tracker ìœ ì§€
if "performance_tracker" not in st.session_state:
    st.session_state.performance_tracker = st.session_state.hybrid_tracker.system_tracker

# ì„¸ì…˜ ID ì´ˆê¸°í™”
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

# 1. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜
def process_pdf_and_create_qa_chain(pdf_file, hybrid_tracker):
    """PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ QA ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì )"""
    
    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ì¶”ì  ì‹œì‘
    hybrid_tracker.track_preprocessing_stage("ì „ì²´_PDF_ì²˜ë¦¬")
    
    # 1. PDF ë¡œë”©
    hybrid_tracker.track_preprocessing_stage("PDF_ë¡œë”©")
    with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    hybrid_tracker.end_preprocessing_stage("PDF_ë¡œë”©")
    
    # ë¬¸ì„œ ì •ë³´ ì €ì¥
    doc_count = len(documents)
    total_chars = sum(len(doc.page_content) for doc in documents)

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    hybrid_tracker.track_preprocessing_stage("í…ìŠ¤íŠ¸_ë¶„í• ")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)
    chunk_count = len(docs)
    hybrid_tracker.end_preprocessing_stage("í…ìŠ¤íŠ¸_ë¶„í• ")

    # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    hybrid_tracker.track_preprocessing_stage("ì„ë² ë”©_ëª¨ë¸_ë¡œë”©")
    model_name = "jhgan/ko-sroberta-multitask"
    model_kwargs = {'device': 'cpu'} # CPU ì‚¬ìš©, GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    hybrid_tracker.end_preprocessing_stage("ì„ë² ë”©_ëª¨ë¸_ë¡œë”©")
    
    # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì„ë² ë”© ìƒì„±)
    hybrid_tracker.track_preprocessing_stage("ë²¡í„°_ìŠ¤í† ì–´_ìƒì„±")
    vectorstore = FAISS.from_documents(docs, embeddings)
    hybrid_tracker.end_preprocessing_stage("ë²¡í„°_ìŠ¤í† ì–´_ìƒì„±")

    # 5. QA ì²´ì¸ êµ¬ì„±
    hybrid_tracker.track_preprocessing_stage("QA_ì²´ì¸_êµ¬ì„±")
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

    prompt_template = """
    ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    {context}

    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # LLM ëª¨ë¸ ë¡œë”©
    llm = ChatOllama(model="llama3:8b", temperature=0)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    hybrid_tracker.end_preprocessing_stage("QA_ì²´ì¸_êµ¬ì„±")
    
    # ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ
    hybrid_tracker.end_preprocessing_stage("ì „ì²´_PDF_ì²˜ë¦¬")
    
    # íŒŒì¼ ì •ë¦¬
    os.remove(tmp_file_path)
    
    # ì²˜ë¦¬ í†µê³„ ì €ì¥
    processing_stats = {
        'ë¬¸ì„œ_í˜ì´ì§€_ìˆ˜': doc_count,
        'ì´_ë¬¸ì_ìˆ˜': total_chars,
        'ì²­í¬_ìˆ˜': chunk_count,
        'ê²€ìƒ‰_ë¬¸ì„œ_ìˆ˜': 3
    }
    
    return qa_chain, processing_stats

# 2. Streamlit UI êµ¬ì„±
st.set_page_config(page_title="ë¡œì»¬ PDF QA ì±—ë´‡ (Hybrid Monitoring)", page_icon="ğŸ¤–")
st.title("ğŸ“„ ë¡œì»¬ PDF ê¸°ë°˜ QA ì±—ë´‡ (Ollama + Langsmith)")

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í† ê¸€
show_performance = st.sidebar.checkbox("ğŸ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í™œì„±í™”", value=True)

# Langsmith ìƒíƒœ í‘œì‹œ
langsmith_status = st.session_state.hybrid_tracker.get_langsmith_status()
if langsmith_status['enabled']:
    st.success(f"âœ… Langsmith ì¶”ì  í™œì„±í™”ë¨ (í”„ë¡œì íŠ¸: {langsmith_status['project']})")
else:
    if langsmith_status['available']:
        st.info("ğŸ“Š Langsmith ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ - API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ê³ ê¸‰ ì¶”ì ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ Langsmith ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì»¤ìŠ¤í…€ ì„±ëŠ¥ ì¸¡ì •ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

# --- OpenAI API í‚¤ ì…ë ¥ ë¶€ë¶„ì´ í•„ìš” ì—†ì–´ì¡ŒìŠµë‹ˆë‹¤. ---
st.info("ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ì²˜ë¦¬' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë™ì¼)
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing_stats" not in st.session_state:
    st.session_state.processing_stats = None

# ì‚¬ì´ë“œë°”ì— PDF ì—…ë¡œë” ë°°ì¹˜ (ë™ì¼)
with st.sidebar:
    st.header("PDF ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì§ˆë¬¸í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
    if st.button("PDF ì²˜ë¦¬"):
        if uploaded_file is not None:
            with st.spinner("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ PDFë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... (Langsmith + ì»¤ìŠ¤í…€ ì¶”ì )"):
                # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì ê¸° ì¬ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì²˜ë¦¬ë¥¼ ìœ„í•´)
                st.session_state.hybrid_tracker = HybridPerformanceTracker()
                st.session_state.performance_tracker = st.session_state.hybrid_tracker.system_tracker
                
                # PDF ì²˜ë¦¬ ë° QA ì²´ì¸ ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì )
                qa_chain, processing_stats = process_pdf_and_create_qa_chain(
                    uploaded_file, 
                    st.session_state.hybrid_tracker
                )
                
                st.session_state.qa_chain = qa_chain
                st.session_state.processing_stats = processing_stats
                st.session_state.messages = [{"role": "assistant", "content": "PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. Langsmithì™€ ì»¤ìŠ¤í…€ ì¶”ì ì´ í™œì„±í™”ëœ ìƒíƒœì…ë‹ˆë‹¤!"}]
            
            st.success("PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ê²°ê³¼ í‘œì‹œ
            if show_performance:
                st.subheader("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ ì„±ëŠ¥ ë¶„ì„")
                
                # ì²˜ë¦¬ í†µê³„
                st.write("**ë¬¸ì„œ ì •ë³´:**")
                stats_col1, stats_col2 = st.columns(2)
                with stats_col1:
                    st.metric("í˜ì´ì§€ ìˆ˜", f"{processing_stats['ë¬¸ì„œ_í˜ì´ì§€_ìˆ˜']}ê°œ")
                    st.metric("ì²­í¬ ìˆ˜", f"{processing_stats['ì²­í¬_ìˆ˜']}ê°œ")
                with stats_col2:
                    st.metric("ì´ ë¬¸ì ìˆ˜", f"{processing_stats['ì´_ë¬¸ì_ìˆ˜']:,}ì")
                    st.metric("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜", f"{processing_stats['ê²€ìƒ‰_ë¬¸ì„œ_ìˆ˜']}ê°œ")
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
                hybrid_insights = st.session_state.hybrid_tracker.get_hybrid_insights()
                
                st.write("**ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„:**")
                perf_summary = hybrid_insights['system_performance']
                
                for task, metrics in perf_summary.items():
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric(f"{task}", f"{metrics['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆ")
                    with col2:
                        st.metric("Python ë©”ëª¨ë¦¬", f"{metrics['Python ë©”ëª¨ë¦¬ (MB)']}MB")
                    with col3:
                        st.metric("Ollama ë©”ëª¨ë¦¬", f"{metrics['Ollama ë©”ëª¨ë¦¬ (MB)']}MB")
                    with col4:
                        st.metric("ì´ ë©”ëª¨ë¦¬", f"{metrics['ì´ ë©”ëª¨ë¦¬ (MB)']}MB")
                
                # Langsmith ìƒíƒœ ë° ì¶”ì²œì‚¬í•­
                st.write("**ì¶”ì  ì‹œìŠ¤í…œ ìƒíƒœ:**")
                langsmith_info = hybrid_insights['langsmith_status']
                
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"ğŸ”§ ì»¤ìŠ¤í…€ ì¶”ì : âœ… í™œì„±í™”\nğŸ“Š Langsmith: {'âœ… í™œì„±í™”' if langsmith_info['enabled'] else 'âŒ ë¹„í™œì„±í™”'}")
                with col2:
                    if langsmith_info['enabled']:
                        st.success(f"í”„ë¡œì íŠ¸: {langsmith_info['project']}")
                    else:
                        st.warning("Langsmith API í‚¤ ì„¤ì • í•„ìš”")
                
                # ì„±ëŠ¥ ì¶”ì²œì‚¬í•­
                recommendations = hybrid_insights['recommendations']
                if recommendations:
                    st.write("**ğŸ¯ ì„±ëŠ¥ ìµœì í™” ì¶”ì²œ:**")
                    for rec in recommendations:
                        st.write(f"- {rec}")
                
        else:
            st.error("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ ë° ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ë™ì¼)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ (ì‚¬ì´ë“œë°”)
if show_performance and hasattr(st.session_state, 'hybrid_tracker'):
    with st.sidebar:
        st.header("ğŸ“ˆ í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ")
        
        # Langsmith ìƒíƒœ
        langsmith_status = st.session_state.hybrid_tracker.get_langsmith_status()
        if langsmith_status['enabled']:
            st.success(f"ğŸ“Š Langsmith: âœ… ({langsmith_status['project']})")
        else:
            st.warning("ğŸ“Š Langsmith: âŒ")
        st.success("ğŸ”§ ì»¤ìŠ¤í…€ ì¶”ì : âœ…")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸
        hybrid_insights = st.session_state.hybrid_tracker.get_hybrid_insights()
        perf_summary = hybrid_insights['system_performance']
        
        if perf_summary:
            st.subheader("ì „ì²´ ì‘ì—… ìš”ì•½")
            total_time = sum(metrics['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'] for metrics in perf_summary.values())
            total_python_memory = sum(metrics['Python ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            total_ollama_memory = sum(metrics['Ollama ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            total_memory = sum(metrics['ì´ ë©”ëª¨ë¦¬ (MB)'] for metrics in perf_summary.values())
            
            st.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
            st.metric("Python ë©”ëª¨ë¦¬ ì´í•©", f"{total_python_memory:.2f}MB")
            st.metric("Ollama ë©”ëª¨ë¦¬ ì´í•©", f"{total_ollama_memory:.2f}MB")
            st.metric("ì „ì²´ ë©”ëª¨ë¦¬ ì´í•©", f"{total_memory:.2f}MB")
            
            # ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° ì‘ì—…
            if perf_summary:
                slowest_task = max(perf_summary.items(), key=lambda x: x[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'])
                st.metric("ê°€ì¥ ëŠë¦° ì‘ì—…", f"{slowest_task[0]}: {slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆ")
        
        # í˜„ì¬ ì‹œìŠ¤í…œ ì •ë³´
        st.subheader("ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ì •ë³´")
        memory_info = st.session_state.hybrid_tracker.system_tracker.get_total_memory_usage()
        cpu_percent = psutil.cpu_percent(interval=0.1)  # 0.1ì´ˆë¡œ ë‹¨ì¶•
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Python í”„ë¡œì„¸ìŠ¤", f"{memory_info['python_memory']:.1f}MB")
            st.metric("Ollama í”„ë¡œì„¸ìŠ¤", f"{memory_info['ollama_memory']:.1f}MB")
        with col2:
            st.metric("ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©", f"{memory_info['total_memory']:.1f}MB")
            st.metric("ì‹œìŠ¤í…œ CPU ì‚¬ìš©ë¥ ", f"{cpu_percent:.1f}%")
            
        # Ollama í”„ë¡œì„¸ìŠ¤ ì •ë³´
        if memory_info['ollama_process_count'] > 0:
            st.info(f"ğŸ¤– í™œì„± Ollama í”„ë¡œì„¸ìŠ¤: {memory_info['ollama_process_count']}ê°œ")
        
        # ì„±ëŠ¥ ì¶”ì²œì‚¬í•­
        recommendations = hybrid_insights['recommendations']
        if recommendations:
            st.subheader("ğŸ¯ ìµœì í™” ì¶”ì²œ")
            for rec in recommendations:
                st.write(f"â€¢ {rec}")
        
        # Langsmith ë§í¬ (í™œì„±í™”ëœ ê²½ìš°)
        if langsmith_status['enabled']:
            st.subheader("ğŸ”— Langsmith ëŒ€ì‹œë³´ë“œ")
            st.markdown(f"[Langsmith ì›¹ ëŒ€ì‹œë³´ë“œ ì—´ê¸°](https://smith.langchain.com/)")
            st.caption(f"í”„ë¡œì íŠ¸: {langsmith_status['project']}")

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.qa_chain is not None:
            with st.spinner("í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì ìœ¼ë¡œ LLM ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (Langsmith + ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§)"):
                # í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ì‘ë‹µ ì¶”ì 
                if show_performance:
                    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    metadata = {
                        'document_pages': st.session_state.processing_stats['ë¬¸ì„œ_í˜ì´ì§€_ìˆ˜'],
                        'document_chunks': st.session_state.processing_stats['ì²­í¬_ìˆ˜'],
                        'query_length': len(prompt)
                    }
                    
                    # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì ìœ¼ë¡œ LLM ì¶”ë¡  ì‹¤í–‰
                    combined_result = st.session_state.hybrid_tracker.track_llm_inference(
                        st.session_state.qa_chain, 
                        prompt, 
                        metadata
                    )
                    
                    response = combined_result['response']
                    system_metrics = combined_result['system_metrics']
                    langsmith_enabled = combined_result['langsmith_enabled']
                else:
                    # ì„±ëŠ¥ ì¸¡ì • ì—†ì´ ì‹¤í–‰
                    response = st.session_state.qa_chain({"query": prompt})
                    system_metrics = None
                    langsmith_enabled = False
                
                st.markdown(response["result"])
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                if show_performance and system_metrics:
                    with st.expander("âš¡ í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ì„±ëŠ¥ ì •ë³´"):
                        # ì¶”ì  ì‹œìŠ¤í…œ ìƒíƒœ
                        st.write("**ì¶”ì  ì‹œìŠ¤í…œ:**")
                        track_col1, track_col2 = st.columns(2)
                        with track_col1:
                            st.success("ğŸ”§ ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ ì¶”ì : âœ…")
                        with track_col2:
                            if langsmith_enabled:
                                st.success("ğŸ“Š Langsmith LLM ì¶”ì : âœ…")
                            else:
                                st.warning("ğŸ“Š Langsmith LLM ì¶”ì : âŒ")
                        
                        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­
                        st.write("**ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:**")
                        perf_col1, perf_col2, perf_col3 = st.columns(3)
                        with perf_col1:
                            st.metric("ì‘ë‹µ ì‹œê°„", f"{system_metrics['duration']:.2f}ì´ˆ")
                            st.metric("ì™„ë£Œ ì‹œê°„", datetime.fromtimestamp(system_metrics['end_time']).strftime('%H:%M:%S'))
                        with perf_col2:
                            st.metric("Python ë©”ëª¨ë¦¬", f"{system_metrics['python_memory_used']:.2f}MB")
                            st.metric("Ollama ë©”ëª¨ë¦¬", f"{system_metrics['ollama_memory_used']:.2f}MB")
                        with perf_col3:
                            st.metric("ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©", f"{system_metrics['total_memory_used']:.2f}MB")
                            # ëŒ€ëµì ì¸ ì²˜ë¦¬ ì†ë„ ê³„ì‚°
                            if system_metrics['duration'] > 0:
                                chars_per_sec = len(prompt) / system_metrics['duration']
                                st.metric("ì²˜ë¦¬ ì†ë„", f"{chars_per_sec:.1f} ë¬¸ì/ì´ˆ")
                        
                        # Langsmith ì •ë³´ (í™œì„±í™”ëœ ê²½ìš°)
                        if langsmith_enabled:
                            st.info("ğŸ“Š **Langsmithì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´:**\n- í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ë¡œê·¸\n- í† í° ì‚¬ìš©ëŸ‰\n- ì²´ì¸ ì‹¤í–‰ íë¦„\n- ì‘ë‹µ í’ˆì§ˆ í‰ê°€")
                
                with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                    for i, doc in enumerate(response["source_documents"]):
                        st.write(f"**ë¬¸ì„œ {i+1}:**")
                        st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
                        st.write("---")
        else:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    if st.session_state.qa_chain is not None:
        st.session_state.messages.append({"role": "assistant", "content": response["result"]})
