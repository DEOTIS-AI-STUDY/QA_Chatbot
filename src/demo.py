import streamlit as st
import os
from tempfile import NamedTemporaryFile

# --- ë³€ê²½/ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
# LLMê³¼ ì„ë² ë”© ëª¨ë¸ì„ ë¡œì»¬ë¡œ ëŒë¦¬ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings
# -----------------------------

# LangChain ê´€ë ¨ ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- OpenAI API í‚¤ ì„¤ì • ë¶€ë¶„ì€ ì´ì œ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ---

# 1. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜
def process_pdf_and_create_qa_chain(pdf_file):
    """PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ QA ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    
    with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)

    # --- â¬‡ï¸ 1. ì„ë² ë”© ëª¨ë¸ ë³€ê²½ â¬‡ï¸ ---
    # ê¸°ì¡´ ì½”ë“œ: embeddings = OpenAIEmbeddings()
    
    # ë³€ê²½ëœ ì½”ë“œ: HuggingFaceì—ì„œ ì œê³µí•˜ëŠ” í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    # ëª¨ë¸ì„ ì²˜ìŒ ì‹¤í–‰í•  ë•Œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ë©°, ì•½ 400MB ì •ë„ì˜ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
    model_name = "jhgan/ko-sroberta-multitask"
    model_kwargs = {'device': 'cpu'} # CPU ì‚¬ìš©, GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    # --- â¬†ï¸ ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì™„ë£Œ â¬†ï¸ ---
    
    vectorstore = FAISS.from_documents(docs, embeddings)

    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

    prompt_template = """
    ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    {context}

    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # --- â¬‡ï¸ 2. LLM ëª¨ë¸ ë³€ê²½ â¬‡ï¸ ---
    # ê¸°ì¡´ ì½”ë“œ: llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    # ë³€ê²½ëœ ì½”ë“œ: Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM(eeve-korean)ì„ ì‚¬ìš©
    llm = ChatOllama(model="llama3:8b", temperature=0)
    # --- â¬†ï¸ LLM ëª¨ë¸ ë³€ê²½ ì™„ë£Œ â¬†ï¸ ---

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    
    os.remove(tmp_file_path)
    
    return qa_chain

# 2. Streamlit UI êµ¬ì„±
st.set_page_config(page_title="ë¡œì»¬ PDF QA ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“„ ë¡œì»¬ PDF ê¸°ë°˜ QA ì±—ë´‡ (Ollama)")

# --- OpenAI API í‚¤ ì…ë ¥ ë¶€ë¶„ì´ í•„ìš” ì—†ì–´ì¡ŒìŠµë‹ˆë‹¤. ---
st.info("ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ì²˜ë¦¬' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë™ì¼)
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°”ì— PDF ì—…ë¡œë” ë°°ì¹˜ (ë™ì¼)
with st.sidebar:
    st.header("PDF ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì§ˆë¬¸í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
    if st.button("PDF ì²˜ë¦¬"):
        if uploaded_file is not None:
            with st.spinner("ë¡œì»¬ ëª¨ë¸ë¡œ PDFë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... (ì²˜ìŒì—ëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                st.session_state.qa_chain = process_pdf_and_create_qa_chain(uploaded_file)
                st.session_state.messages = [{"role": "assistant", "content": "PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]
            st.success("PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ ë° ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ë™ì¼)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.qa_chain is not None:
            with st.spinner("ë¡œì»¬ LLMì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                response = st.session_state.qa_chain({"query": prompt})
                st.markdown(response["result"])
                with st.expander("ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                    st.write(response["source_documents"])
        else:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    if st.session_state.qa_chain is not None:
        st.session_state.messages.append({"role": "assistant", "content": response["result"]})
