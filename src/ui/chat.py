"""
ì±„íŒ… ì¸í„°íŽ˜ì´ìŠ¤ UI ì»´í¬ë„ŒíŠ¸
"""
import streamlit as st
from core.models import ModelFactory
from core.rag import create_rag_chain
from core.config import LLM_MODELS
from ui.diagnostics import show_initialization_diagnostics


def render_chat_interface(available_models, model_choice, top_k):
    """ì±„íŒ… ì¸í„°íŽ˜ì´ìŠ¤ ë Œë”ë§"""
    st.subheader("ðŸ’¬ ëŒ€í™”")
    
    # RAG ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ ë° ì´ˆê¸°í™” ë²„íŠ¼
    warning_container = _show_rag_status()
    _render_initialization_button(available_models, model_choice, top_k, warning_container)
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    _display_chat_messages()
    
    # ì‚¬ìš©ìž ìž…ë ¥ ì²˜ë¦¬
    _handle_user_input(top_k)


def _show_rag_status():
    """RAG ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
    warning_container = st.empty()
    
    if st.session_state.qa_chain is not None and st.session_state.rag_initialized:
        st.success(f"âœ… RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. (ëª¨ë¸: {st.session_state.selected_model})")
    elif st.session_state.rag_initialized and st.session_state.qa_chain is None:
        st.error("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì˜¤ë¥˜ê°€ ìžˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        st.info("ìƒíƒœê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ëž˜ 'ìƒíƒœ ë¦¬ì…‹' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    elif not st.session_state.rag_initialized:
        warning_container.warning("âš ï¸ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if st.session_state.qa_chain is False:
            st.error("âŒ ì´ì „ ì´ˆê¸°í™”ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
    
    return warning_container


def _render_initialization_button(available_models, model_choice, top_k, warning_container):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë²„íŠ¼ ë Œë”ë§"""
    if st.button("ðŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"):
        if not available_models:
            st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        _initialize_rag_system(model_choice, top_k, warning_container)


def _initialize_rag_system(model_choice, top_k, warning_container):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    with st.spinner("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
        try:
            current_model = st.session_state.get('selected_model', model_choice)
            
            # ìž„ë² ë”© ëª¨ë¸ ìƒì„±
            st.write("ðŸ“ ìž„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘...")
            embeddings = ModelFactory.create_embedding_model()
            if embeddings is None:
                st.error("âŒ ìž„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                return
            st.write("âœ… ìž„ë² ë”© ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # LLM ëª¨ë¸ ìƒì„±
            st.write("ðŸ¤– LLM ëª¨ë¸ ìƒì„± ì¤‘...")
            llm_model, message = ModelFactory.create_llm_model(current_model)
            if llm_model is None:
                st.error(f"âŒ LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {message}")
                return
            st.write(f"âœ… {message}")
            
            # RAG ì²´ì¸ ìƒì„±
            st.write("ðŸ”— RAG ì²´ì¸ ìƒì„± ì¤‘...")
            qa_chain, success_or_error = create_rag_chain(embeddings, llm_model, top_k)
            
            _handle_initialization_result(qa_chain, success_or_error, current_model, warning_container)
            
        except Exception as e:
            st.session_state.qa_chain = None
            st.session_state.rag_initialized = False
            st.error(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            show_initialization_diagnostics(e, model_choice)


def _handle_initialization_result(qa_chain, success_or_error, current_model, warning_container):
    """ì´ˆê¸°í™” ê²°ê³¼ ì²˜ë¦¬"""
    if success_or_error is True and qa_chain is not None:
        st.session_state.qa_chain = qa_chain
        st.session_state.selected_model = current_model
        st.session_state.rag_initialized = True
        
        # ê²½ê³  ë©”ì‹œì§€ ì¦‰ì‹œ ì œê±°
        warning_container.empty()
        
        # ì´ˆê¸°í™” ë©”ì‹œì§€ ì¶”ê°€
        if not st.session_state.messages:
            st.session_state.messages = []
        st.session_state.messages.append({
            "role": "assistant", 
            "content": f"RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {LLM_MODELS[current_model]['name']}"
        })
        
        st.success("ðŸŽ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        st.info("ì´ì œ ì•„ëž˜ì—ì„œ ì§ˆë¬¸í•˜ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
    else:
        st.session_state.qa_chain = None
        st.session_state.rag_initialized = False
        
        # ì˜¤ë¥˜ ë©”ì‹œì§€ ì²˜ë¦¬
        if success_or_error is True:
            error_msg = "qa_chainì´ Noneìœ¼ë¡œ ë°˜í™˜ë¨ (ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜)"
        elif isinstance(success_or_error, str):
            error_msg = success_or_error
        else:
            error_msg = f"ì•Œ ìˆ˜ ì—†ëŠ” ë°˜í™˜ê°’: {success_or_error}"
            
        st.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}")
        show_initialization_diagnostics(None, st.session_state.get('selected_model'))


def _display_chat_messages():
    """ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


def _handle_user_input(top_k):
    """ì‚¬ìš©ìž ìž…ë ¥ ì²˜ë¦¬"""
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            if st.session_state.qa_chain is not None and st.session_state.rag_initialized:
                _process_query(prompt, top_k)
            else:
                st.warning("ë¨¼ì € RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
                st.info("ìœ„ì˜ 'ðŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                if st.session_state.rag_initialized and st.session_state.qa_chain is None:
                    st.error("âš ï¸ ì‹œìŠ¤í…œ ìƒíƒœ ë¶ˆì¼ì¹˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒíƒœ ë¦¬ì…‹' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


def _process_query(prompt, top_k):
    """ì¿¼ë¦¬ ì²˜ë¦¬"""
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        current_model = st.session_state.get('selected_model')
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì ìœ¼ë¡œ LLM ì¶”ë¡ 
        metadata = {
            'model': LLM_MODELS[current_model]['name'] if current_model else 'Unknown',
            'top_k': top_k,
            'query_length': len(prompt)
        }
        
        combined_result = st.session_state.hybrid_tracker.track_llm_inference(
            st.session_state.qa_chain,
            prompt,
            metadata
        )
        
        response = combined_result['response']
        system_metrics = combined_result['system_metrics']
        
        # ë‹µë³€ í‘œì‹œ
        st.markdown(response["result"])
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        _show_performance_info(system_metrics)
        
        # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
        _show_source_documents(response["source_documents"])
        
        # ë©”ì‹œì§€ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": response["result"]})


def _show_performance_info(system_metrics):
    """ì„±ëŠ¥ ì •ë³´ í‘œì‹œ"""
    if system_metrics:
        with st.expander("âš¡ ì„±ëŠ¥ ì •ë³´"):
            perf_col1, perf_col2, perf_col3 = st.columns(3)
            with perf_col1:
                st.metric("ì‘ë‹µ ì‹œê°„", f"{system_metrics['duration']:.2f}ì´ˆ")
            with perf_col2:
                st.metric("ì´ ë©”ëª¨ë¦¬", f"{system_metrics['total_memory_used']:.2f}MB")
            with perf_col3:
                st.metric("ES í”„ë¡œì„¸ìŠ¤", system_metrics['elasticsearch_process_count'])


def _show_source_documents(source_documents):
    """ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ"""
    with st.expander("ðŸ“„ ì°¸ê³  ë¬¸ì„œ"):
        for i, doc in enumerate(source_documents):
            st.write(f"**ë¬¸ì„œ {i+1}:**")
            st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            st.write(f"*ì¶œì²˜: {doc.metadata.get('filename', 'Unknown')}*")
            st.divider()
