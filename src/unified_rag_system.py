import streamlit as st
import os
import time
import psutil
import uuid
import urllib3
import argparse
from datetime import datetime
from tempfile import NamedTemporaryFile
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë”©
load_dotenv()

# --- Langsmith ì¶”ì ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
try:
    from langsmith import Client
    from langchain.callbacks.tracers import LangChainTracer
    from langchain.callbacks.manager import CallbackManager
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False

# --- ì„ë² ë”© ë° LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    HUGGINGFACE_EMBEDDINGS_AVAILABLE = True
except ImportError as e:
    print(f"HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        HUGGINGFACE_EMBEDDINGS_AVAILABLE = True
    except ImportError as e2:
        print(f"Community HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ë¡œë”© ì‹¤íŒ¨: {e2}")
        HuggingFaceEmbeddings = None
        HUGGINGFACE_EMBEDDINGS_AVAILABLE = False

# ì¡°ê±´ë¶€ import - ì˜¤ë¥˜ ë°œìƒì‹œ Noneìœ¼ë¡œ ì„¤ì •
try:
    from langchain_upstage import ChatUpstage
    UPSTAGE_AVAILABLE = True
except ImportError as e:
    print(f"Upstage ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")
    ChatUpstage = None
    UPSTAGE_AVAILABLE = False

try:
    from langchain_ollama import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError as e:
    print(f"Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")
    ChatOllama = None
    OLLAMA_AVAILABLE = False

# --- ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from elasticsearch import Elasticsearch

# --- LangChain ì²´ì¸ ê´€ë ¨ ---
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ===== ì„¤ì • =====
# Elasticsearch ì„¤ì •
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
INDEX_NAME = os.getenv("INDEX_NAME", "unified_rag")
PDF_DIR = os.getenv("PDF_DIR", "pdf")
LANGCHAIN_ENDPOINT = os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")

# BGE-M3 ì„ë² ë”© ëª¨ë¸ ì„¤ì •
BGE_MODEL_NAME = "BAAI/bge-m3"

# LLM ëª¨ë¸ ì„¤ì •
LLM_MODELS = {
    "upstage": {
        "name": "Upstage Solar LLM",
        "model_id": "solar-1-mini-chat",
        "api_key_env": "UPSTAGE_API_KEY"
    },
    "qwen2": {
        "name": "Qwen2",
        "model_id": "qwen2:7b",
        "api_key_env": None
    },
    "llama3": {
        "name": "Llama3",
        "model_id": "llama3:8b",
        "api_key_env": None
    }
}

# ===== Langsmith ì„¤ì • í•¨ìˆ˜ =====
def setup_langsmith():
    """Langsmith ì¶”ì  ì„¤ì •"""
    if not LANGSMITH_AVAILABLE:
        return None, False
        
    try:
        langsmith_api_key = os.getenv("LANGSMITH_API_KEY") or st.secrets.get("LANGSMITH_API_KEY", "")
        langsmith_project = os.getenv("LANGSMITH_PROJECT", "unified-rag-system")
        langsmith_endpoint = os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")
        
        if langsmith_api_key:
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_ENDPOINT"] = langsmith_endpoint
            os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
            os.environ["LANGCHAIN_PROJECT"] = langsmith_project
            
            tracer = LangChainTracer(project_name=langsmith_project)
            callback_manager = CallbackManager([tracer])
            return callback_manager, True
        else:
            return None, False
    except Exception as e:
        st.warning(f"âš ï¸ Langsmith ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, False

# ===== ì„±ëŠ¥ ì¸¡ì • í´ë˜ìŠ¤ =====
class PerformanceTracker:
    def __init__(self):
        self.metrics = {}
        self.current_process = psutil.Process()
        
    def get_elasticsearch_memory(self):
        """Elasticsearch í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        es_memory = 0
        es_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
            try:
                if 'elasticsearch' in proc.info['name'].lower() or 'java' in proc.info['name'].lower():
                    # Java í”„ë¡œì„¸ìŠ¤ ì¤‘ Elasticsearch ê´€ë ¨ í™•ì¸
                    try:
                        cmdline = proc.cmdline()
                        if any('elasticsearch' in cmd.lower() for cmd in cmdline):
                            es_processes.append(proc)
                            es_memory += proc.memory_info().rss / 1024 / 1024
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return es_memory, len(es_processes)
    
    def get_ollama_memory(self):
        """Ollama í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        ollama_memory = 0
        ollama_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
            try:
                if 'ollama' in proc.info['name'].lower():
                    ollama_processes.append(proc)
                    ollama_memory += proc.memory_info().rss / 1024 / 1024
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return ollama_memory, len(ollama_processes)
    
    def get_total_memory_usage(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        python_memory = self.current_process.memory_info().rss / 1024 / 1024
        es_memory, es_count = self.get_elasticsearch_memory()
        ollama_memory, ollama_count = self.get_ollama_memory()
        
        return {
            'python_memory': python_memory,
            'elasticsearch_memory': es_memory,
            'ollama_memory': ollama_memory,
            'total_memory': python_memory + es_memory + ollama_memory,
            'elasticsearch_process_count': es_count,
            'ollama_process_count': ollama_count
        }
        
    def start_timer(self, task_name):
        """ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        memory_info = self.get_total_memory_usage()
        self.metrics[task_name] = {
            'start_time': time.time(),
            'start_python_memory': memory_info['python_memory'],
            'start_elasticsearch_memory': memory_info['elasticsearch_memory'],
            'start_ollama_memory': memory_info['ollama_memory'],
            'start_total_memory': memory_info['total_memory'],
            'start_cpu_percent': self.current_process.cpu_percent()
        }
        
    def end_timer(self, task_name):
        """ì‘ì—… ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ê²°ê³¼ ë°˜í™˜"""
        if task_name in self.metrics:
            end_time = time.time()
            memory_info = self.get_total_memory_usage()
            
            self.metrics[task_name].update({
                'end_time': end_time,
                'end_python_memory': memory_info['python_memory'],
                'end_elasticsearch_memory': memory_info['elasticsearch_memory'],
                'end_ollama_memory': memory_info['ollama_memory'],
                'end_total_memory': memory_info['total_memory'],
                'duration': end_time - self.metrics[task_name]['start_time'],
                'python_memory_used': memory_info['python_memory'] - self.metrics[task_name]['start_python_memory'],
                'elasticsearch_memory_used': memory_info['elasticsearch_memory'] - self.metrics[task_name]['start_elasticsearch_memory'],
                'ollama_memory_used': memory_info['ollama_memory'] - self.metrics[task_name]['start_ollama_memory'],
                'total_memory_used': memory_info['total_memory'] - self.metrics[task_name]['start_total_memory'],
                'elasticsearch_process_count': memory_info['elasticsearch_process_count'],
                'ollama_process_count': memory_info['ollama_process_count']
            })
            
            return self.metrics[task_name]
        return None
    
    def get_summary(self):
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {}
        for task, metrics in self.metrics.items():
            if 'duration' in metrics:
                summary[task] = {
                    'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': round(metrics['duration'], 2),
                    'Python ë©”ëª¨ë¦¬ (MB)': round(metrics['python_memory_used'], 2),
                    'Elasticsearch ë©”ëª¨ë¦¬ (MB)': round(metrics['elasticsearch_memory_used'], 2),
                    'Ollama ë©”ëª¨ë¦¬ (MB)': round(metrics['ollama_memory_used'], 2),
                    'ì´ ë©”ëª¨ë¦¬ (MB)': round(metrics['total_memory_used'], 2),
                    'ES í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['elasticsearch_process_count'],
                    'Ollama í”„ë¡œì„¸ìŠ¤ ìˆ˜': metrics['ollama_process_count'],
                    'ì™„ë£Œì‹œê°„': datetime.fromtimestamp(metrics['end_time']).strftime('%H:%M:%S')
                }
        return summary

# ===== í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤ =====
class HybridPerformanceTracker:
    def __init__(self):
        self.system_tracker = PerformanceTracker()
        self.langsmith_callback, self.langsmith_enabled = setup_langsmith()
        self.hybrid_metrics = {
            'system_metrics': {},
            'langsmith_sessions': [],
            'combined_insights': {}
        }
        
    def get_langsmith_status(self):
        """Langsmith ìƒíƒœ ë°˜í™˜"""
        return {
            'available': LANGSMITH_AVAILABLE,
            'enabled': self.langsmith_enabled,
            'project': os.getenv("LANGSMITH_PROJECT", "unified-rag-system")
        }
    
    def track_preprocessing_stage(self, stage_name):
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ì¶”ì """
        return self.system_tracker.start_timer(stage_name)
    
    def end_preprocessing_stage(self, stage_name):
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ì¢…ë£Œ"""
        metrics = self.system_tracker.end_timer(stage_name)
        if metrics:
            self.hybrid_metrics['system_metrics'][stage_name] = metrics
        return metrics
    
    def track_llm_inference(self, qa_chain, query, metadata=None):
        """LLM ì¶”ë¡  í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì """
        self.system_tracker.start_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
        
        if metadata is None:
            metadata = {}
        
        enhanced_metadata = {
            **metadata,
            'session_id': st.session_state.get('session_id', 'unknown'),
            'timestamp': datetime.now().isoformat(),
            'query_length': len(query),
            'system': 'unified_rag'
        }
        
        try:
            if self.langsmith_enabled and self.langsmith_callback:
                response = qa_chain(
                    {"query": query}, 
                    callbacks=self.langsmith_callback.handlers,
                    metadata=enhanced_metadata
                )
            else:
                response = qa_chain({"query": query})
            
            system_metrics = self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            
            combined_result = {
                'response': response,
                'system_metrics': system_metrics,
                'langsmith_enabled': self.langsmith_enabled,
                'metadata': enhanced_metadata
            }
            
            return combined_result
            
        except Exception as e:
            self.system_tracker.end_timer("LLM_ì¶”ë¡ _ì‹œìŠ¤í…œ")
            st.error(f"LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def get_system_summary(self):
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìš”ì•½"""
        return self.system_tracker.get_summary()
    
    def get_hybrid_insights(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸"""
        system_summary = self.get_system_summary()
        
        insights = {
            'system_performance': system_summary,
            'langsmith_status': self.get_langsmith_status(),
            'recommendations': self._generate_recommendations(system_summary)
        }
        
        return insights
    
    def _generate_recommendations(self, system_summary):
        """ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if system_summary:
            slowest_task = max(system_summary.items(), 
                             key=lambda x: x[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'], 
                             default=(None, {'ì‹¤í–‰ì‹œê°„ (ì´ˆ)': 0}))
            
            if slowest_task[0] and slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)'] > 10:
                recommendations.append(f"âš ï¸ {slowest_task[0]}ì´ {slowest_task[1]['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆë¡œ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
                
                if 'Elasticsearch' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •ì„ ìµœì í™”í•˜ê±°ë‚˜ ë” ì ì€ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")
                elif 'LLM_ì¶”ë¡ ' in slowest_task[0]:
                    recommendations.append("ğŸ’¡ ë” ì‘ì€ LLM ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ GPUë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            
            total_memory = sum(metrics.get('ì´ ë©”ëª¨ë¦¬ (MB)', 0) for metrics in system_summary.values())
            if total_memory > 8000:
                recommendations.append(f"ğŸ”¥ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {total_memory:.1f}MBë¡œ ë†’ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        return recommendations

# ===== ì„ë² ë”© ë° LLM ëª¨ë¸ íŒ©í† ë¦¬ =====
class ModelFactory:
    @staticmethod
    def create_embedding_model():
        """BGE-M3 ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
        if not HUGGINGFACE_EMBEDDINGS_AVAILABLE:
            st.error("HuggingFace ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        try:
            return HuggingFaceEmbeddings(
                model_name=BGE_MODEL_NAME,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            try:
                return HuggingFaceEmbeddings(
                    model_name="jhgan/ko-sroberta-multitask",
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
            except Exception as e2:
                st.error(f"í´ë°± ì„ë² ë”© ëª¨ë¸ë„ ë¡œë”© ì‹¤íŒ¨: {str(e2)}")
                return None
    
    @staticmethod
    def create_llm_model(model_choice):
        """ì„ íƒëœ LLM ëª¨ë¸ ìƒì„±"""
        if model_choice == "upstage":
            if not UPSTAGE_AVAILABLE:
                st.error("âŒ Upstage ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            api_key = os.getenv("UPSTAGE_API_KEY")
            if not api_key:
                st.error("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            return ChatUpstage(
                api_key=api_key,
                model=LLM_MODELS["upstage"]["model_id"],
                temperature=0
            )
        
        elif model_choice == "qwen2":
            if not OLLAMA_AVAILABLE:
                st.error("âŒ Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            return ChatOllama(
                model=LLM_MODELS["qwen2"]["model_id"],
                temperature=0
            )
        
        elif model_choice == "llama3":
            if not OLLAMA_AVAILABLE:
                st.error("âŒ Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            return ChatOllama(
                model=LLM_MODELS["llama3"]["model_id"],
                temperature=0
            )
        
        else:
            st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_choice}")
            return None

# ===== Elasticsearch ìœ í‹¸ë¦¬í‹° =====
class ElasticsearchManager:
    @staticmethod
    def check_connection():
        """Elasticsearch ì—°ê²° í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ë‹¤ì–‘í•œ ì—°ê²° ë°©ë²• ì‹œë„
        connection_configs = [
            {
                "hosts": [ELASTICSEARCH_URL],
                "verify_certs": False,
                "ssl_show_warn": False,
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True
            },
            {
                "hosts": ["http://localhost:9200"],
                "verify_certs": False,
                "request_timeout": 30
            },
            {
                "hosts": ["http://127.0.0.1:9200"],
                "verify_certs": False,
                "request_timeout": 30
            }
        ]
        
        for i, config in enumerate(connection_configs):
            try:
                es = Elasticsearch(**config)
                if es.ping():
                    cluster_info = es.info()
                    version = cluster_info.get('version', {}).get('number', 'Unknown')
                    return True, f"ì—°ê²° ì„±ê³µ (v{version}) - ë°©ë²• {i+1}"
            except Exception as conn_error:
                continue
        
        return False, "ëª¨ë“  ì—°ê²° ë°©ë²• ì‹¤íŒ¨. Elasticsearchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    
    @staticmethod
    def get_safe_elasticsearch_client():
        """ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        configs = [
            {
                "hosts": [ELASTICSEARCH_URL],
                "verify_certs": False,
                "ssl_show_warn": False,
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True
            },
            {
                "hosts": ["http://localhost:9200"],
                "verify_certs": False,
                "request_timeout": 30
            }
        ]
        
        for config in configs:
            try:
                es = Elasticsearch(**config)
                if es.ping():
                    return es, True, f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ: {config['hosts'][0]}"
            except Exception:
                continue
        
        return None, False, "Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨"
    
    @staticmethod
    def list_pdfs(pdf_dir: str):
        """PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(pdf_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".pdf"):
                files.append(os.path.join(pdf_dir, name))
        return sorted(files)
    
    @staticmethod
    def index_pdfs(pdf_files, embeddings, hybrid_tracker):
        """PDF íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        hybrid_tracker.track_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
        es = Elasticsearch(ELASTICSEARCH_URL)
        if es.indices.exists(index=INDEX_NAME):
            es.indices.delete(index=INDEX_NAME)
        
        all_documents = []
        
        for pdf_path in pdf_files:
            hybrid_tracker.track_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
            
            # PDF ë¡œë”©
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            chunks = splitter.split_documents(docs)
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            for chunk in chunks:
                chunk.metadata.update({
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "category": "PDF"
                })
            
            all_documents.extend(chunks)
            hybrid_tracker.end_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Elasticsearchì— ì €ì¥
        hybrid_tracker.track_preprocessing_stage("Elasticsearch_ì €ì¥")
        try:
            # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            # ë¬¸ì„œ ì €ì¥
            ElasticsearchStore.from_documents(
                all_documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            
            return True, f"ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            return False, f"ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"

# ===== í•µì‹¬ RAG ì‹œìŠ¤í…œ =====
def create_rag_chain(embeddings, llm_model, top_k=3):
    """RAG ì²´ì¸ ìƒì„±"""
    try:
        st.write("ğŸ” Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í™•ì¸ ì¤‘...")
        # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
        if not success:
            error_msg = f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        st.write(f"âœ… Elasticsearch ì—°ê²° ì„±ê³µ: {message}")
        
        st.write("ğŸ” ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì¤‘...")
        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
        if not es_client.indices.exists(index=INDEX_NAME):
            error_msg = f"ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”."
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        # ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        doc_count = es_client.count(index=INDEX_NAME).get("count", 0)
        if doc_count == 0:
            error_msg = f"ì¸ë±ìŠ¤ '{INDEX_NAME}'ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”."
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        st.write(f"âœ… ì¸ë±ìŠ¤ì— {doc_count}ê°œ ë¬¸ì„œ ë°œê²¬")
        
        st.write("ğŸ” Elasticsearch ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        # Elasticsearch ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
        try:
            # ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            vectorstore = ElasticsearchStore(
                embedding=embeddings,
                index_name=INDEX_NAME,
                es_url=ELASTICSEARCH_URL
            )
            st.write("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        except TypeError as type_error:
            # íŒŒë¼ë¯¸í„° ì´ë¦„ ë¬¸ì œì¸ ê²½ìš°
            st.write(f"âš ï¸ íŒŒë¼ë¯¸í„° ì˜¤ë¥˜, ë‹¤ë¥¸ ë°©ë²• ì‹œë„: {str(type_error)}")
            try:
                vectorstore = ElasticsearchStore(
                    embedding=embeddings,
                    index_name=INDEX_NAME,
                    elasticsearch_url=ELASTICSEARCH_URL
                )
                st.write("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ (elasticsearch_url ì‚¬ìš©)")
            except Exception as vs_error2:
                error_msg = f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {str(vs_error2)}"
                st.error(f"âŒ {error_msg}")
                return None, error_msg
        except Exception as vs_error:
            error_msg = f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {str(vs_error)}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        st.write(f"ğŸ” ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì¤‘ (top_k={top_k})...")
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        try:
            retriever = vectorstore.as_retriever(
                search_kwargs={
                    "k": top_k,
                    "fetch_k": min(top_k * 3, 10000)
                }
            )
            st.write("âœ… ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì™„ë£Œ")
        except Exception as ret_error:
            error_msg = f"ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì‹¤íŒ¨: {str(ret_error)}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        st.write("ğŸ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì¤‘...")
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = """
ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
        
        try:
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            st.write("âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ")
        except Exception as prompt_error:
            error_msg = f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì‹¤íŒ¨: {str(prompt_error)}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        st.write("ğŸ” QA ì²´ì¸ ìƒì„± ì¤‘...")
        # QA ì²´ì¸ ìƒì„±
        try:
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm_model,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt},
                return_source_documents=True
            )
            st.write("âœ… QA ì²´ì¸ ìƒì„± ì™„ë£Œ")
        except Exception as qa_error:
            error_msg = f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(qa_error)}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        # ìµœì¢… ê²€ì¦
        if qa_chain is None:
            error_msg = "QA ì²´ì¸ì´ Noneìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        st.write("ğŸ” QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        try:
            test_result = qa_chain({"query": "í…ŒìŠ¤íŠ¸"})
            if test_result is None:
                error_msg = "QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì‘ë‹µì´ Noneì…ë‹ˆë‹¤."
                st.error(f"âŒ {error_msg}")
                return None, error_msg
            st.write("âœ… QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        except Exception as test_error:
            error_msg = f"QA ì²´ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(test_error)}"
            st.error(f"âŒ {error_msg}")
            return None, error_msg
        
        st.write("ğŸ‰ RAG ì²´ì¸ ìƒì„± ì™„ì „ ì„±ê³µ!")
        return qa_chain, True
        
    except Exception as e:
        error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)} (íƒ€ì…: {type(e).__name__})"
        st.error(f"âŒ {error_msg}")
        
        # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶”ê°€
        import traceback
        st.error("ğŸ“‹ ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        st.code(traceback.format_exc())
        
        return None, error_msg

# ===== Streamlit UI =====
def main():
    st.set_page_config(
        page_title="í†µí•© RAG ì‹œìŠ¤í…œ",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸš€ í†µí•© RAG ì‹œìŠ¤í…œ")
    st.markdown("**BGE-M3 ì„ë² ë”© + Elasticsearch + ë©€í‹° LLM + ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**")
    
    # ìƒë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ë°” ì¶”ê°€
    col_nav1, col_nav2, col_nav3 = st.columns([3, 1, 1])
    

    with col_nav2:
        # ì¶”ê°€ ë„¤ë¹„ê²Œì´ì…˜ ê³µê°„ (í•„ìš”ì‹œ í™•ì¥ ê°€ëŠ¥)
        st.empty()
    
    st.divider()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "hybrid_tracker" not in st.session_state:
        st.session_state.hybrid_tracker = HybridPerformanceTracker()
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = None
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "processing_stats" not in st.session_state:
        st.session_state.processing_stats = None
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = None
    if "rag_initialized" not in st.session_state:
        st.session_state.rag_initialized = False
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # Elasticsearch ì—°ê²° ìƒíƒœ
        es_connected, es_message = ElasticsearchManager.check_connection()
        if es_connected:
            st.success(f"âœ… Elasticsearch: {es_message}")
        else:
            st.error(f"âŒ Elasticsearch: {es_message}")
            st.stop()
        
        # Langsmith ìƒíƒœ ì‹¤ì‹œê°„ í™•ì¸
        _, langsmith_enabled = setup_langsmith()
        langsmith_project = os.getenv("LANGSMITH_PROJECT", "unified-rag-system")
        
        if langsmith_enabled:
            st.success(f"âœ… Langsmith: {langsmith_project}")
            
            # LangSmith ê´€ë¦¬ ì„¹ì…˜
            with st.expander("ğŸ“Š LangSmith ê´€ë¦¬(ì†Œìœ ìë§Œ)"):
                langsmith_url = "https://smith.langchain.com"
                
                st.markdown(f"""
                **í”„ë¡œì íŠ¸:** `{langsmith_project}`  
                **ìƒíƒœ:** ğŸŸ¢ í™œì„±í™”ë¨
                """)
                
                col_ls1, col_ls2 = st.columns(2)
                with col_ls1:
                    if st.button("ğŸ“ˆ ëŒ€ì‹œë³´ë“œ", key="langsmith_dashboard"):
                        st.markdown(f'<meta http-equiv="refresh" content="0; url={langsmith_url}">', unsafe_allow_html=True)
                        st.info(f"LangSmith ëŒ€ì‹œë³´ë“œë¡œ ì´ë™: {langsmith_url}")
                
                with col_ls2:
                    if st.button("ğŸ”— ë§í¬ ë³µì‚¬", key="copy_langsmith_link"):
                        st.code(langsmith_url)
                        st.success("ë§í¬ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                st.markdown("**ì£¼ìš” ê¸°ëŠ¥:**")
                st.write("â€¢ ì‹¤ì‹œê°„ ì¶”ë¡  ì¶”ì ")
                st.write("â€¢ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„")
                st.write("â€¢ ì˜¤ë¥˜ ë””ë²„ê¹…")
                st.write("â€¢ ë¹„ìš© ëª¨ë‹ˆí„°ë§")
        else:
            st.info("ğŸ“Š Langsmith: ë¹„í™œì„±í™”")
            
            with st.expander("ğŸ“Š LangSmith ì„¤ì •"):
                # API í‚¤ í™•ì¸
                api_key = os.getenv("LANGSMITH_API_KEY") or st.secrets.get("LANGSMITH_API_KEY", "")
                if api_key:
                    st.warning("âš ï¸ API í‚¤ëŠ” ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ LangSmith ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    st.info("ğŸ’¡ `pip install langsmith` ëª…ë ¹ìœ¼ë¡œ LangSmithë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
                else:
                    st.markdown("""
                    **LangSmith í™œì„±í™” ë°©ë²•:**
                    1. LangSmith API í‚¤ ë°œê¸‰
                    2. .env íŒŒì¼ì— ì¶”ê°€:
                    ```
                    LANGSMITH_API_KEY=your_api_key
                    LANGSMITH_PROJECT=your_project_name
                    ```
                    3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘
                    """)
                
                if st.button("ğŸŒ LangSmith ì›¹ì‚¬ì´íŠ¸", key="langsmith_website"):
                    st.markdown('<meta http-equiv="refresh" content="0; url=https://smith.langchain.com">', unsafe_allow_html=True)
                    st.info("LangSmith ì›¹ì‚¬ì´íŠ¸: https://smith.langchain.com")
        
        st.divider()
        
        # LLM ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– LLM ëª¨ë¸ ì„ íƒ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í•„í„°ë§
        available_models = {}
        if UPSTAGE_AVAILABLE:
            available_models["upstage"] = LLM_MODELS["upstage"]
        if OLLAMA_AVAILABLE:
            available_models["qwen2"] = LLM_MODELS["qwen2"]
            available_models["llama3"] = LLM_MODELS["llama3"]
            
        if not available_models:
            st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”: langchain-upstage, langchain-ollama")
            model_choice = None
        else:
            model_choice = st.selectbox(
                "ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
                options=list(available_models.keys()),
                format_func=lambda x: available_models[x]["name"],
                index=0,
                key="model_selector"
            )
            
            # ëª¨ë¸ì´ ë³€ê²½ë˜ë©´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” í•„ìš”
            if st.session_state.selected_model != model_choice:
                st.session_state.selected_model = model_choice
                st.session_state.qa_chain = None
                st.session_state.rag_initialized = False
                
            # ëª¨ë¸ ìƒíƒœ í‘œì‹œ
            if model_choice == "upstage":
                api_key = os.getenv("UPSTAGE_API_KEY")
                if api_key and api_key != "your_upstage_api_key":
                    st.success("âœ… Upstage API í‚¤ ì„¤ì •ë¨")
                else:
                    st.warning("âš ï¸ UPSTAGE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            elif model_choice in ["qwen2", "llama3"]:
                st.info("â„¹ï¸ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        
        # ê²€ìƒ‰ ì„¤ì •
        st.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        top_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", 1, 10, 3)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í† ê¸€
        show_performance = st.checkbox("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§", value=True)
        
        # ë””ë²„ê¹… ì •ë³´ (ê°œë°œ ì¤‘ì—ë§Œ í‘œì‹œ)
        with st.expander("ğŸ”§ ë””ë²„ê·¸ ì •ë³´"):
            st.write(f"qa_chain: {st.session_state.qa_chain}")
            st.write(f"qa_chain type: {type(st.session_state.qa_chain)}")
            st.write(f"qa_chain is None: {st.session_state.qa_chain is None}")
            st.write(f"qa_chain == False: {st.session_state.qa_chain == False}")
            st.write(f"bool(qa_chain): {bool(st.session_state.qa_chain)}")
            st.write(f"rag_initialized: {st.session_state.get('rag_initialized', False)}")
            st.write(f"selected_model: {st.session_state.get('selected_model', 'None')}")
            st.write(f"current_model_choice: {model_choice if 'model_choice' in locals() else 'None'}")
            
            # ìƒíƒœ ë¦¬ì…‹ ë²„íŠ¼
            if st.button("ğŸ”„ ìƒíƒœ ë¦¬ì…‹", key="reset_debug"):
                st.session_state.qa_chain = None
                st.session_state.rag_initialized = False
                st.session_state.selected_model = None
                st.success("ìƒíƒœê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        
        st.divider()
        
        # PDF ê´€ë¦¬
        st.subheader("ğŸ“„ PDF ê´€ë¦¬")
        
        # PDF ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "PDF íŒŒì¼ ì—…ë¡œë“œ",
            type="pdf",
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("ğŸ“¥ íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹±"):
            # PDF ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(PDF_DIR, exist_ok=True)
            
            # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì €ì¥
            uploaded_paths = []
            for uploaded_file in uploaded_files:
                file_path = os.path.join(PDF_DIR, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                uploaded_paths.append(file_path)
            
            with st.spinner("PDF íŒŒì¼ ì¸ë±ì‹± ì¤‘..."):
                # ì„ë² ë”© ëª¨ë¸ ìƒì„±
                embeddings = ModelFactory.create_embedding_model()
                
                # PDF ì¸ë±ì‹±
                success, message = ElasticsearchManager.index_pdfs(
                    uploaded_paths, 
                    embeddings, 
                    st.session_state.hybrid_tracker
                )
                
                if success:
                    st.success(message)
                else:
                    st.error(message)
        
        # ê¸°ì¡´ PDF íŒŒì¼ ëª©ë¡
        existing_pdfs = ElasticsearchManager.list_pdfs(PDF_DIR)
        if existing_pdfs:
            st.write("**ê¸°ì¡´ PDF íŒŒì¼:**")
            for pdf in existing_pdfs:
                st.write(f"â€¢ {os.path.basename(pdf)}")
            
            if st.button("ğŸ”„ ê¸°ì¡´ íŒŒì¼ ì¬ì¸ë±ì‹±"):
                with st.spinner("ê¸°ì¡´ PDF íŒŒì¼ ì¬ì¸ë±ì‹± ì¤‘..."):
                    embeddings = ModelFactory.create_embedding_model()
                    success, message = ElasticsearchManager.index_pdfs(
                        existing_pdfs, 
                        embeddings, 
                        st.session_state.hybrid_tracker
                    )
                    
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
    
    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ’¬ ëŒ€í™”")
        
        # RAG ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
        if st.session_state.qa_chain is not None and st.session_state.rag_initialized:
            st.success(f"âœ… RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ëª¨ë¸: {st.session_state.selected_model})")
        elif st.session_state.rag_initialized and st.session_state.qa_chain is None:
            st.error("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
            st.info("ìƒíƒœê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë˜ 'ìƒíƒœ ë¦¬ì…‹' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        else:
            st.warning("âš ï¸ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if st.session_state.qa_chain is False:
                st.error("âŒ ì´ì „ ì´ˆê¸°í™”ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        
        # RAG ì²´ì¸ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"):
            if not available_models:
                st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            with st.spinner("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
                try:
                    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©
                    current_model = st.session_state.get('selected_model', model_choice)
                    
                    # ì„ë² ë”© ëª¨ë¸ ìƒì„±
                    st.write("ğŸ“ ì„ë² ë”© ëª¨ë¸ ìƒì„± ì¤‘...")
                    embeddings = ModelFactory.create_embedding_model()
                    if embeddings is None:
                        st.error("âŒ ì„ë² ë”© ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                        return
                    st.write("âœ… ì„ë² ë”© ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                    
                    # LLM ëª¨ë¸ ìƒì„±
                    st.write("ğŸ¤– LLM ëª¨ë¸ ìƒì„± ì¤‘...")
                    llm_model = ModelFactory.create_llm_model(current_model)
                    if llm_model is None:
                        st.error("âŒ LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                        return
                    st.write("âœ… LLM ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                    
                    # RAG ì²´ì¸ ìƒì„±
                    st.write("ğŸ”— RAG ì²´ì¸ ìƒì„± ì¤‘...")
                    qa_chain, success_or_error = create_rag_chain(embeddings, llm_model, top_k)
                    
                    # ê²°ê³¼ ë¶„ì„
                    st.write(f"ğŸ” RAG ì²´ì¸ ìƒì„± ê²°ê³¼ ë¶„ì„:")
                    st.write(f"   - qa_chain ê°’: {qa_chain}")
                    st.write(f"   - qa_chain íƒ€ì…: {type(qa_chain)}")
                    st.write(f"   - success_or_error ê°’: {success_or_error}")
                    st.write(f"   - success_or_error íƒ€ì…: {type(success_or_error)}")
                    
                    if success_or_error is True and qa_chain is not None:
                        st.session_state.qa_chain = qa_chain
                        st.session_state.selected_model = current_model
                        st.session_state.rag_initialized = True
                        
                        # ì´ˆê¸°í™” ë©”ì‹œì§€ ì¶”ê°€
                        if not st.session_state.messages:
                            st.session_state.messages = []
                        st.session_state.messages.append({
                            "role": "assistant", 
                            "content": f"RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {LLM_MODELS[current_model]['name']}"
                        })
                        
                        st.success("ğŸ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                        st.info("ì´ì œ ì•„ë˜ì—ì„œ ì§ˆë¬¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        st.session_state.qa_chain = None
                        st.session_state.rag_initialized = False
                        
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ ì²˜ë¦¬
                        if success_or_error is True:
                            error_msg = "qa_chainì´ Noneìœ¼ë¡œ ë°˜í™˜ë¨ (ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜)"
                        elif isinstance(success_or_error, str):
                            error_msg = success_or_error
                        else:
                            error_msg = f"ì•Œ ìˆ˜ ì—†ëŠ” ë°˜í™˜ê°’: {success_or_error}"
                            
                        st.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}")
                        
                        # ìë™ ì§„ë‹¨ ì‹œìŠ¤í…œ
                        st.info("ï¿½ ìë™ ì§„ë‹¨ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
                        with st.expander("ï¿½ğŸ“‹ ìƒì„¸ ì§„ë‹¨ ê²°ê³¼", expanded=True):
                            # 1. Elasticsearch ì¸ë±ìŠ¤ ë¬¸ì„œ í™•ì¸
                            st.write("**1. Elasticsearch ì¸ë±ìŠ¤ í™•ì¸**")
                            try:
                                es_client, es_success, es_msg = ElasticsearchManager.get_safe_elasticsearch_client()
                                if es_success:
                                    try:
                                        if es_client.indices.exists(index=INDEX_NAME):
                                            doc_count = es_client.count(index=INDEX_NAME).get("count", 0)
                                            if doc_count > 0:
                                                st.success(f"âœ… ì¸ë±ìŠ¤ '{INDEX_NAME}'ì— {doc_count}ê°œ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤.")
                                            else:
                                                st.error(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”.")
                                                st.info("ğŸ’¡ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ 'ê¸°ì¡´ íŒŒì¼ ì¬ì¸ë±ì‹±' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                                        else:
                                            st.error(f"âŒ ì¸ë±ìŠ¤ '{INDEX_NAME}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                            st.info("ğŸ’¡ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
                                    except Exception as idx_e:
                                        st.error(f"âŒ ì¸ë±ìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(idx_e)}")
                                else:
                                    st.error(f"âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {es_msg}")
                            except Exception as es_e:
                                st.error(f"âŒ Elasticsearch ì§„ë‹¨ ì‹¤íŒ¨: {str(es_e)}")
                            
                            st.divider()
                            
                            # 2. LLM ëª¨ë¸ ê°€ìš©ì„± í™•ì¸
                            st.write("**2. LLM ëª¨ë¸ ê°€ìš©ì„± í™•ì¸**")
                            try:
                                test_llm = ModelFactory.create_llm_model(current_model)
                                if test_llm is not None:
                                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
                                    try:
                                        if current_model == "upstage":
                                            # Upstage API í…ŒìŠ¤íŠ¸
                                            test_response = test_llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
                                            st.success(f"âœ… {LLM_MODELS[current_model]['name']} ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                                        else:
                                            # Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ping)
                                            try:
                                                import requests
                                                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                                                if response.status_code == 200:
                                                    models = response.json().get("models", [])
                                                    model_names = [m.get("name", "") for m in models]
                                                    target_model = LLM_MODELS[current_model]["model_id"]
                                                    if any(target_model in name for name in model_names):
                                                        st.success(f"âœ… Ollamaì—ì„œ {target_model} ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                                    else:
                                                        st.error(f"âŒ Ollamaì— {target_model} ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                                                        st.info(f"ğŸ’¡ í„°ë¯¸ë„ì—ì„œ 'ollama pull {target_model}' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                                                        st.code(f"ollama pull {target_model}")
                                                else:
                                                    st.error("âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
                                            except requests.exceptions.RequestException:
                                                st.error("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                                st.info("ğŸ’¡ í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ìœ¼ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
                                                st.code("ollama serve")
                                    except Exception as test_e:
                                        st.warning(f"âš ï¸ ëª¨ë¸ ìƒì„±ì€ ëì§€ë§Œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨: {str(test_e)}")
                                        if current_model in ["qwen2", "llama3"]:
                                            st.info("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ê³  ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                                else:
                                    st.error(f"âŒ {LLM_MODELS[current_model]['name']} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
                            except Exception as llm_e:
                                st.error(f"âŒ LLM ëª¨ë¸ ì§„ë‹¨ ì‹¤íŒ¨: {str(llm_e)}")
                            
                            st.divider()
                            
                            # 3. API í‚¤ ë° í™˜ê²½ë³€ìˆ˜ í™•ì¸
                            st.write("**3. API í‚¤ ë° í™˜ê²½ë³€ìˆ˜ í™•ì¸**")
                            if current_model == "upstage":
                                api_key = os.getenv("UPSTAGE_API_KEY")
                                if api_key and api_key != "your_upstage_api_key":
                                    masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "****"
                                    st.success(f"âœ… UPSTAGE_API_KEY ì„¤ì •ë¨ ({masked_key})")
                                else:
                                    st.error("âŒ UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                    st.info("ğŸ’¡ .env íŒŒì¼ì— UPSTAGE_API_KEY=your_actual_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                                    st.code("echo 'UPSTAGE_API_KEY=your_actual_key' >> .env")
                            else:
                                st.success(f"âœ… {LLM_MODELS[current_model]['name']}ëŠ” API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            
                            # í™˜ê²½ë³€ìˆ˜ ìƒíƒœ
                            st.write("**í™˜ê²½ë³€ìˆ˜ ìƒíƒœ:**")
                            env_vars = {
                                "ELASTICSEARCH_URL": ELASTICSEARCH_URL,
                                "INDEX_NAME": INDEX_NAME,
                                "PDF_DIR": PDF_DIR
                            }
                            for var, value in env_vars.items():
                                st.write(f"â€¢ {var}: `{value}`")
                            
                            st.divider()
                            
                            # 4. ì¶”ì²œ í•´ê²° ë°©ë²•
                            st.write("**4. ì¶”ì²œ í•´ê²° ë°©ë²•**")
                            st.info("ğŸ“ **ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•:**")
                            st.write("1ï¸âƒ£ PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
                            st.write("2ï¸âƒ£ Elasticsearchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (Docker Compose ì‚¬ìš©)")
                            st.write("3ï¸âƒ£ ì„ íƒí•œ LLM ëª¨ë¸ ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
                            st.write("4ï¸âƒ£ í•„ìš”í•œ ê²½ìš° API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
                            st.write("5ï¸âƒ£ 'ìƒíƒœ ë¦¬ì…‹' í›„ ë‹¤ì‹œ ì´ˆê¸°í™”")
                            
                            # ë¹ ë¥¸ ì•¡ì…˜ ë²„íŠ¼ë“¤
                            st.write("**ë¹ ë¥¸ ì•¡ì…˜:**")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                if st.button("ğŸ”„ ìƒíƒœ ë¦¬ì…‹", key="quick_reset"):
                                    st.session_state.qa_chain = None
                                    st.session_state.rag_initialized = False
                                    st.session_state.selected_model = None
                                    st.success("ìƒíƒœ ë¦¬ì…‹ ì™„ë£Œ!")
                            with col2:
                                if st.button("ğŸ“Š ES ìƒíƒœ", key="check_es"):
                                    es_client, es_success, es_msg = ElasticsearchManager.get_safe_elasticsearch_client()
                                    if es_success:
                                        info = es_client.info()
                                        st.json({"status": "connected", "version": info.get("version", {})})
                                    else:
                                        st.error(es_msg)
                            with col3:
                                if st.button("ğŸ¤– ëª¨ë¸ í…ŒìŠ¤íŠ¸", key="test_model"):
                                    test_model = ModelFactory.create_llm_model(current_model)
                                    if test_model:
                                        st.success(f"{current_model} ëª¨ë¸ ìƒì„± ì„±ê³µ!")
                                    else:
                                        st.error(f"{current_model} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨!")
                        
                except Exception as e:
                    st.session_state.qa_chain = None
                    st.session_state.rag_initialized = False
                    st.error(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    
                    # ì˜ˆì™¸ ìƒí™©ì—ì„œë„ ì§„ë‹¨ ì •ë³´ ì œê³µ
                    with st.expander("ğŸ” ì˜¤ë¥˜ ì§„ë‹¨ ì •ë³´", expanded=True):
                        st.write("**ì˜¤ë¥˜ ìƒì„¸ ì •ë³´:**")
                        st.code(str(e))
                        
                        st.write("**ê°€ëŠ¥í•œ ì›ì¸:**")
                        if "elasticsearch" in str(e).lower():
                            st.write("â€¢ Elasticsearch ì—°ê²° ë¬¸ì œ")
                            st.info("ğŸ’¡ Elasticsearchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: `docker-compose up elasticsearch`")
                        elif "ollama" in str(e).lower() or "connection" in str(e).lower():
                            st.write("â€¢ Ollama ì„œë²„ ì—°ê²° ë¬¸ì œ")
                            st.info("ğŸ’¡ Ollama ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: `ollama serve`")
                        elif "api" in str(e).lower() or "key" in str(e).lower():
                            st.write("â€¢ API í‚¤ ê´€ë ¨ ë¬¸ì œ")
                            st.info("ğŸ’¡ .env íŒŒì¼ì˜ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                        else:
                            st.write("â€¢ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                            st.info("ğŸ’¡ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ì„¸ìš”")
                        
                        if hasattr(e, '__traceback__'):
                            import traceback
                            with st.expander("ìƒì„¸ íŠ¸ë ˆì´ìŠ¤ë°±"):
                                st.code(traceback.format_exc())
        
        # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                if st.session_state.qa_chain is not None and st.session_state.rag_initialized:
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        # í˜„ì¬ ëª¨ë¸ í™•ì¸
                        current_model = st.session_state.get('selected_model')
                        
                        # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì ìœ¼ë¡œ LLM ì¶”ë¡ 
                        if show_performance:
                            metadata = {
                                'model': LLM_MODELS[current_model]['name'] if current_model else 'Unknown',
                                'top_k': top_k,
                                'query_length': len(prompt)
                            }
                            
                            combined_result = st.session_state.hybrid_tracker.track_llm_inference(
                                st.session_state.qa_chain,
                                prompt,
                                metadata
                            )
                            
                            response = combined_result['response']
                            system_metrics = combined_result['system_metrics']
                        else:
                            response = st.session_state.qa_chain({"query": prompt})
                            system_metrics = None
                        
                        # ë‹µë³€ í‘œì‹œ
                        st.markdown(response["result"])
                        
                        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                        if show_performance and system_metrics:
                            with st.expander("âš¡ ì„±ëŠ¥ ì •ë³´"):
                                perf_col1, perf_col2, perf_col3 = st.columns(3)
                                with perf_col1:
                                    st.metric("ì‘ë‹µ ì‹œê°„", f"{system_metrics['duration']:.2f}ì´ˆ")
                                with perf_col2:
                                    st.metric("ì´ ë©”ëª¨ë¦¬", f"{system_metrics['total_memory_used']:.2f}MB")
                                with perf_col3:
                                    st.metric("ES í”„ë¡œì„¸ìŠ¤", system_metrics['elasticsearch_process_count'])
                        
                        # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
                        with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ"):
                            for i, doc in enumerate(response["source_documents"]):
                                st.write(f"**ë¬¸ì„œ {i+1}:**")
                                st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
                                st.write(f"*ì¶œì²˜: {doc.metadata.get('filename', 'Unknown')}*")
                                st.divider()
                else:
                    st.warning("ë¨¼ì € RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
                    st.info("ìœ„ì˜ 'ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                    if st.session_state.rag_initialized and st.session_state.qa_chain is None:
                        st.error("âš ï¸ ì‹œìŠ¤í…œ ìƒíƒœ ë¶ˆì¼ì¹˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒíƒœ ë¦¬ì…‹' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            
            if st.session_state.qa_chain is not None and st.session_state.rag_initialized:
                st.session_state.messages.append({"role": "assistant", "content": response["result"]})
    
    with col2:
        if show_performance:
            st.subheader("ğŸ“ˆ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ")
            
            # í˜„ì¬ ì‹œìŠ¤í…œ ì •ë³´
            memory_info = st.session_state.hybrid_tracker.system_tracker.get_total_memory_usage()
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            st.metric("ì‹œìŠ¤í…œ CPU", f"{cpu_percent:.1f}%")
            st.metric("Python ë©”ëª¨ë¦¬", f"{memory_info['python_memory']:.1f}MB")
            st.metric("Elasticsearch", f"{memory_info['elasticsearch_memory']:.1f}MB")
            st.metric("Ollama", f"{memory_info['ollama_memory']:.1f}MB")
            st.metric("ì´ ë©”ëª¨ë¦¬", f"{memory_info['total_memory']:.1f}MB")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸
            hybrid_insights = st.session_state.hybrid_tracker.get_hybrid_insights()
            
            # ì„±ëŠ¥ ìš”ì•½
            perf_summary = hybrid_insights['system_performance']
            if perf_summary:
                st.subheader("ì‘ì—… ìš”ì•½")
                for task, metrics in perf_summary.items():
                    st.write(f"**{task}**: {metrics['ì‹¤í–‰ì‹œê°„ (ì´ˆ)']}ì´ˆ")
            
            # ì¶”ì²œì‚¬í•­
            recommendations = hybrid_insights['recommendations']
            if recommendations:
                st.subheader("ğŸ¯ ìµœì í™” ì¶”ì²œ")
                for rec in recommendations:
                    st.write(f"â€¢ {rec}")
            

if __name__ == "__main__":
    main()