"""
Elasticsearch ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
"""
import os
import urllib3
import json
from typing import List, Tuple, Optional, Dict, Any
from elasticsearch import Elasticsearch
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain.schema import Document
from core.config import (
    ELASTICSEARCH_URL, 
    ELASTICSEARCH_HOST, 
    ELASTICSEARCH_PORT, 
    ELASTICSEARCH_SCHEME,
    ELASTICSEARCH_USERNAME,
    ELASTICSEARCH_PASSWORD,
    INDEX_NAME
)


class ElasticsearchManager:
    @staticmethod
    def keyword_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ(ë‹¨ìˆœ í…ìŠ¤íŠ¸) ê¸°ë°˜ ê²€ìƒ‰"""
        config = ElasticsearchManager.get_connection_config()
        es = Elasticsearch(**config)
        if not es.indices.exists(index=INDEX_NAME):
            return []
        body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["page_content", "metadata.filename", "metadata.source"],
                    "type": "best_fields"
                }
            },
            "size": top_k
        }
        try:
            res = es.search(index=INDEX_NAME, body=body)
            hits = res.get("hits", {}).get("hits", [])
            results = []
            for hit in hits:
                doc = {
                    "score": hit.get("_score"),
                    "content": hit.get("_source", {}).get("page_content"),
                    "metadata": hit.get("_source", {}).get("metadata", {})
                }
                results.append(doc)
            return results
        except Exception as e:
            print(f"[ElasticsearchManager] í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    """Elasticsearch ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_connection_config() -> Dict[str, Any]:
        """Elasticsearch ì—°ê²° ì„¤ì • ìƒì„±"""
        config = {
            "hosts": [ELASTICSEARCH_URL],
            "verify_certs": False,
            "ssl_show_warn": False,
            "request_timeout": 30,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # ì¸ì¦ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if ELASTICSEARCH_USERNAME and ELASTICSEARCH_PASSWORD:
            config["basic_auth"] = (ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD)
        
        return config
    
    @staticmethod
    def check_connection() -> Tuple[bool, str]:
        """Elasticsearch ì—°ê²° í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ì£¼ ì—°ê²° ì„¤ì •
        main_config = ElasticsearchManager.get_connection_config()
        
        # ëŒ€ì²´ ì—°ê²° ë°©ë²•ë“¤
        fallback_configs = [
            {
                "hosts": ["http://localhost:9200"],
                "verify_certs": False,
                "request_timeout": 30
            },
            {
                "hosts": ["http://127.0.0.1:9200"],
                "verify_certs": False,
                "request_timeout": 30
            }
        ]
        
        # ëª¨ë“  ì„¤ì •ì„ í•©ì³ì„œ ì‹œë„
        all_configs = [main_config] + fallback_configs
        
        for i, config in enumerate(all_configs):
            try:
                es = Elasticsearch(**config)
                if es.ping():
                    cluster_info = es.info()
                    version = cluster_info.get('version', {}).get('number', 'Unknown')
                    cluster_name = cluster_info.get('cluster_name', 'Unknown')
                    host_info = config["hosts"][0]
                    return True, f"ì—°ê²° ì„±ê³µ ({cluster_name} v{version}) - {host_info}"
            except Exception as e:
                if i == 0:  # ì£¼ ì—°ê²° ì‹¤íŒ¨ ì‹œì—ë§Œ ì—ëŸ¬ ì •ë³´ ê¸°ë¡
                    last_error = str(e)
                continue
        
        error_msg = f"ëª¨ë“  ì—°ê²° ë°©ë²• ì‹¤íŒ¨"
        if 'last_error' in locals():
            error_msg += f" (ì£¼ ì—°ê²° ì˜¤ë¥˜: {last_error})"
        error_msg += ". Elasticsearchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
        
        return False, error_msg
    
    @staticmethod
    def get_safe_elasticsearch_client() -> Tuple[Elasticsearch, bool, str]:
        """ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        configs = [
            {
                "hosts": [ELASTICSEARCH_URL],
                "verify_certs": False,
                "ssl_show_warn": False,
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True
            },
            {
                "hosts": ["http://localhost:9200"],
                "verify_certs": False,
                "request_timeout": 30
            }
        ]
        
        for config in configs:
            try:
                es = Elasticsearch(**config)
                if es.ping():
                    return es, True, f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ: {config['hosts'][0]}"
            except Exception:
                continue
        
        return None, False, "Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨"
    
    @staticmethod
    def list_pdfs(pdf_dir: str) -> List[str]:
        """PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(pdf_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".pdf"):
                files.append(os.path.join(pdf_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_txt_files(txt_dir: str) -> List[str]:
        """TXT íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(txt_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".txt"):
                files.append(os.path.join(txt_dir, name))
        return sorted(files)
    
    @staticmethod
    def delete_documents_by_category(categories: List[str]) -> Tuple[bool, str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë“¤ì„ ì„ íƒì ìœ¼ë¡œ ì‚­ì œ"""
        try:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return True, "ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
            # ì‚­ì œí•  ì¹´í…Œê³ ë¦¬ë“¤ì— ëŒ€í•œ ì¿¼ë¦¬ ìƒì„±
            delete_query = {
                "query": {
                    "terms": {
                        "metadata.category.keyword": categories
                    }
                }
            }
            
            # ì‚­ì œ ì „ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_before = es.count(index=INDEX_NAME).get("count", 0)
            
            # ì„ íƒì  ì‚­ì œ ì‹¤í–‰
            delete_response = es.delete_by_query(
                index=INDEX_NAME,
                body=delete_query,
                refresh=True
            )
            
            deleted_count = delete_response.get("deleted", 0)
            count_after = es.count(index=INDEX_NAME).get("count", 0)
            
            return True, f"ì¹´í…Œê³ ë¦¬ {categories} ë¬¸ì„œ {deleted_count}ê°œ ì‚­ì œë¨ (ì „ì²´: {count_before} â†’ {count_after})"
            
        except Exception as e:
            return False, f"ì„ íƒì  ì‚­ì œ ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def get_existing_files_by_category(category: str) -> List[str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì¡´ íŒŒì¼ëª… ëª©ë¡ ë°˜í™˜"""
        try:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return []
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ëª… ì§‘ê³„ ì¿¼ë¦¬
            agg_query = {
                "size": 0,
                "query": {
                    "term": {
                        "metadata.category.keyword": category
                    }
                },
                "aggs": {
                    "filenames": {
                        "terms": {
                            "field": "metadata.filename.keyword",
                            "size": 1000
                        }
                    }
                }
            }
            
            response = es.search(index=INDEX_NAME, body=agg_query)
            buckets = response.get("aggregations", {}).get("filenames", {}).get("buckets", [])
            
            return [bucket["key"] for bucket in buckets]
            
        except Exception as e:
            print(f"ê¸°ì¡´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    @staticmethod
    def list_json_files(json_dir: str) -> List[str]:
        """JSON íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(json_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".json"):
                files.append(os.path.join(json_dir, name))
        return sorted(files)
    
    @staticmethod
    def index_pdfs(pdf_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """PDF íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹± (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""
        hybrid_tracker.track_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì™„ì „ ì‚­ì œ
        if not incremental:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            if es.indices.exists(index=INDEX_NAME):
                es.indices.delete(index=INDEX_NAME)
        else:
            # ê¸°ì¡´ PDF ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë§Œ ì‚­ì œ
            success, message = ElasticsearchManager.delete_documents_by_category(["PDF"])
            if not success:
                hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ PDF ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        
        for pdf_path in pdf_files:
            hybrid_tracker.track_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
            
            # PDF ë¡œë”©
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            chunks = splitter.split_documents(docs)
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            for chunk in chunks:
                chunk.metadata.update({
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "category": "PDF"
                })
            
            all_documents.extend(chunks)
            hybrid_tracker.end_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Elasticsearchì— ì €ì¥
        hybrid_tracker.track_preprocessing_stage("Elasticsearch_ì €ì¥")
        try:
            # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            # ë¬¸ì„œ ì €ì¥
            ElasticsearchStore.from_documents(
                all_documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            
            return True, f"PDF ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("PDF_ì¸ë±ì‹±_ì‹œì‘")
            return False, f"PDF ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def index_txt_files(txt_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """TXT íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹± (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""
        hybrid_tracker.track_preprocessing_stage("TXT_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì™„ì „ ì‚­ì œ
        if not incremental:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            if es.indices.exists(index=INDEX_NAME):
                es.indices.delete(index=INDEX_NAME)
        else:
            # ê¸°ì¡´ TXT ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë§Œ ì‚­ì œ
            success, message = ElasticsearchManager.delete_documents_by_category(["TXT"])
            if not success:
                hybrid_tracker.end_preprocessing_stage("TXT_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ TXT ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        
        for txt_path in txt_files:
            hybrid_tracker.track_preprocessing_stage(f"TXT_ì²˜ë¦¬_{os.path.basename(txt_path)}")
            
            try:
                # TXT ë¡œë”©
                loader = TextLoader(txt_path, encoding='utf-8')
                docs = loader.load()
                
                # í…ìŠ¤íŠ¸ ë¶„í• 
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000, 
                    chunk_overlap=200
                )
                chunks = splitter.split_documents(docs)
                
                # ë©”íƒ€ë°ì´í„° ë³´ê°•
                for chunk in chunks:
                    chunk.metadata.update({
                        "source": txt_path,
                        "filename": os.path.basename(txt_path),
                        "category": "TXT"
                    })
                
                all_documents.extend(chunks)
                
            except Exception as e:
                print(f"TXT íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({txt_path}): {e}")
                continue
                
            hybrid_tracker.end_preprocessing_stage(f"TXT_ì²˜ë¦¬_{os.path.basename(txt_path)}")
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("TXT_ì¸ë±ì‹±_ì‹œì‘")
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Elasticsearchì— ì €ì¥
        hybrid_tracker.track_preprocessing_stage("Elasticsearch_ì €ì¥")
        try:
            # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            # ë¬¸ì„œ ì €ì¥
            ElasticsearchStore.from_documents(
                all_documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("TXT_ì¸ë±ì‹±_ì‹œì‘")
            
            return True, f"TXT ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("TXT_ì¸ë±ì‹±_ì‹œì‘")
            return False, f"TXT ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def index_json_files(json_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """JSON íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹± (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""
        hybrid_tracker.track_preprocessing_stage("JSON_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì™„ì „ ì‚­ì œ
        if not incremental:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            if es.indices.exists(index=INDEX_NAME):
                es.indices.delete(index=INDEX_NAME)
        else:
            # ê¸°ì¡´ JSON ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë§Œ ì‚­ì œ
            success, message = ElasticsearchManager.delete_documents_by_category(["JSON"])
            if not success:
                hybrid_tracker.end_preprocessing_stage("JSON_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ JSON ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        
        for json_path in json_files:
            hybrid_tracker.track_preprocessing_stage(f"JSON_ì²˜ë¦¬_{os.path.basename(json_path)}")
            
            try:
                # JSON ë¡œë”©
                with open(json_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                # JSON êµ¬ì¡°ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = ElasticsearchManager._extract_text_from_json(json_data)
                
                if text_content:
                    # Document ê°ì²´ ìƒì„±
                    doc = Document(
                        page_content=text_content,
                        metadata={
                            "source": json_path,
                            "filename": os.path.basename(json_path),
                            "category": "JSON"
                        }
                    )
                    
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000, 
                        chunk_overlap=200
                    )
                    chunks = splitter.split_documents([doc])
                    
                    all_documents.extend(chunks)
                
            except Exception as e:
                print(f"JSON íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({json_path}): {e}")
                continue
                
            hybrid_tracker.end_preprocessing_stage(f"JSON_ì²˜ë¦¬_{os.path.basename(json_path)}")
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("JSON_ì¸ë±ì‹±_ì‹œì‘")
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Elasticsearchì— ì €ì¥
        hybrid_tracker.track_preprocessing_stage("Elasticsearch_ì €ì¥")
        try:
            # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            # ë¬¸ì„œ ì €ì¥
            ElasticsearchStore.from_documents(
                all_documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("JSON_ì¸ë±ì‹±_ì‹œì‘")
            
            return True, f"JSON ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("JSON_ì¸ë±ì‹±_ì‹œì‘")
            return False, f"JSON ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def _extract_text_from_json(json_data: Any, max_depth: int = 10) -> str:
        """JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if max_depth <= 0:
            return ""
        
        text_parts = []
        
        if isinstance(json_data, dict):
            for key, value in json_data.items():
                # í‚¤ë„ í…ìŠ¤íŠ¸ë¡œ í¬í•¨
                text_parts.append(str(key))
                # ê°’ ì¶”ì¶œ
                if isinstance(value, (dict, list)):
                    text_parts.append(ElasticsearchManager._extract_text_from_json(value, max_depth - 1))
                else:
                    text_parts.append(str(value))
        elif isinstance(json_data, list):
            for item in json_data:
                if isinstance(item, (dict, list)):
                    text_parts.append(ElasticsearchManager._extract_text_from_json(item, max_depth - 1))
                else:
                    text_parts.append(str(item))
        else:
            text_parts.append(str(json_data))
        
        return " ".join(filter(None, text_parts))
    
    @staticmethod
    def index_all_files(data_dir: str, embeddings, hybrid_tracker, file_types: List[str] = None, incremental: bool = True) -> Tuple[bool, str]:
        """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ íƒ€ì…ì„ ì¸ë±ì‹± (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""
        if file_types is None:
            file_types = ['pdf', 'txt', 'json']
        
        hybrid_tracker.track_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì™„ì „ ì‚­ì œ
        if not incremental:
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            if es.indices.exists(index=INDEX_NAME):
                es.indices.delete(index=INDEX_NAME)
        else:
            # ì„ íƒëœ íŒŒì¼ íƒ€ì…ì˜ ì¹´í…Œê³ ë¦¬ë§Œ ì‚­ì œ
            categories_to_delete = [file_type.upper() for file_type in file_types]
            success, message = ElasticsearchManager.delete_documents_by_category(categories_to_delete)
            if not success:
                hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        processed_files = 0
        
        # PDF íŒŒì¼ ì²˜ë¦¬
        if 'pdf' in file_types:
            pdf_dir = os.path.join(data_dir, 'pdf')
            pdf_files = ElasticsearchManager.list_pdfs(pdf_dir)
            for pdf_path in pdf_files:
                hybrid_tracker.track_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
                try:
                    loader = PyPDFLoader(pdf_path)
                    docs = loader.load()
                    
                    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                    chunks = splitter.split_documents(docs)
                    
                    for chunk in chunks:
                        chunk.metadata.update({
                            "source": pdf_path,
                            "filename": os.path.basename(pdf_path),
                            "category": "PDF"
                        })
                    
                    all_documents.extend(chunks)
                    processed_files += 1
                except Exception as e:
                    print(f"PDF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({pdf_path}): {e}")
                hybrid_tracker.end_preprocessing_stage(f"PDF_ì²˜ë¦¬_{os.path.basename(pdf_path)}")
        
        # TXT íŒŒì¼ ì²˜ë¦¬
        if 'txt' in file_types:
            txt_dir = os.path.join(data_dir, 'txt')
            txt_files = ElasticsearchManager.list_txt_files(txt_dir)
            for txt_path in txt_files:
                hybrid_tracker.track_preprocessing_stage(f"TXT_ì²˜ë¦¬_{os.path.basename(txt_path)}")
                try:
                    loader = TextLoader(txt_path, encoding='utf-8')
                    docs = loader.load()
                    
                    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                    chunks = splitter.split_documents(docs)
                    
                    for chunk in chunks:
                        chunk.metadata.update({
                            "source": txt_path,
                            "filename": os.path.basename(txt_path),
                            "category": "TXT"
                        })
                    
                    all_documents.extend(chunks)
                    processed_files += 1
                except Exception as e:
                    print(f"TXT íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({txt_path}): {e}")
                hybrid_tracker.end_preprocessing_stage(f"TXT_ì²˜ë¦¬_{os.path.basename(txt_path)}")
        
        # JSON íŒŒì¼ ì²˜ë¦¬
        if 'json' in file_types:
            json_dir = os.path.join(data_dir, 'json')
            json_files = ElasticsearchManager.list_json_files(json_dir)
            for json_path in json_files:
                hybrid_tracker.track_preprocessing_stage(f"JSON_ì²˜ë¦¬_{os.path.basename(json_path)}")
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    
                    text_content = ElasticsearchManager._extract_text_from_json(json_data)
                    
                    if text_content:
                        doc = Document(
                            page_content=text_content,
                            metadata={
                                "source": json_path,
                                "filename": os.path.basename(json_path),
                                "category": "JSON"
                            }
                        )
                        
                        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                        chunks = splitter.split_documents([doc])
                        
                        all_documents.extend(chunks)
                        processed_files += 1
                except Exception as e:
                    print(f"JSON íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({json_path}): {e}")
                hybrid_tracker.end_preprocessing_stage(f"JSON_ì²˜ë¦¬_{os.path.basename(json_path)}")
        
        if not all_documents:
            # íŒŒì¼ì´ ì—†ì–´ë„ ì¹´í…Œê³ ë¦¬ ì‚­ì œëŠ” ì´ë¯¸ ì™„ë£Œë¨
            hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
            
            # í˜„ì¬ ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
                if success:
                    cnt = es_client.count(index=INDEX_NAME).get("count", 0)
                    return True, f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ, í˜„ì¬ ë¬¸ì„œ ìˆ˜: {cnt}"
                else:
                    return True, "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ"
            except:
                return True, "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ"
        
        # Elasticsearchì— ì €ì¥
        hybrid_tracker.track_preprocessing_stage("Elasticsearch_ì €ì¥")
        try:
            # ì•ˆì „í•œ Elasticsearch í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            # ë¬¸ì„œ ì €ì¥
            ElasticsearchStore.from_documents(
                all_documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
            
            return True, f"ì „ì²´ íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ, ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            hybrid_tracker.end_preprocessing_stage("Elasticsearch_ì €ì¥")
            hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
            return False, f"ì „ì²´ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
