"""
Elasticsearch ì¸ë±ì‹± ì „ìš© ìœ í‹¸ë¦¬í‹°
"""
import os
import json
import re
from typing import List, Tuple, Any
from elasticsearch import Elasticsearch
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import ElasticsearchStore
from langchain.schema import Document
from docx import Document as DocxDocument
from core.config import ELASTICSEARCH_URL, INDEX_NAME
from .elasticsearch import ElasticsearchManager, get_optimized_text_splitter


class ElasticsearchIndexer:
    """Elasticsearch ì¸ë±ì‹± ì „ìš© í´ë˜ìŠ¤"""
    
    @staticmethod
    def delete_documents_by_category(categories: List[str]) -> Tuple[bool, str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë“¤ì„ ì„ íƒì ìœ¼ë¡œ ì‚­ì œ"""
        try:
            from utils.elasticsearch import ElasticsearchManager
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return True, "ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
            # ì‚­ì œí•  ì¹´í…Œê³ ë¦¬ë“¤ì— ëŒ€í•œ ì¿¼ë¦¬ ìƒì„±
            delete_query = {
                "query": {
                    "terms": {
                        "metadata.category.keyword": categories
                    }
                }
            }
            
            # ì‚­ì œ ì „ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_before = es.count(index=INDEX_NAME).get("count", 0)
            
            # ì„ íƒì  ì‚­ì œ ì‹¤í–‰
            delete_response = es.delete_by_query(
                index=INDEX_NAME,
                body=delete_query,
                refresh=True
            )
            
            deleted_count = delete_response.get("deleted", 0)
            count_after = es.count(index=INDEX_NAME).get("count", 0)
            
            return True, f"ì¹´í…Œê³ ë¦¬ {categories} ë¬¸ì„œ {deleted_count}ê°œ ì‚­ì œë¨ (ì „ì²´: {count_before} â†’ {count_after})"
            
        except Exception as e:
            return False, f"ì„ íƒì  ì‚­ì œ ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def get_existing_files_by_category(category: str) -> List[str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì¡´ íŒŒì¼ëª… ëª©ë¡ ë°˜í™˜"""
        try:
            from utils.elasticsearch import ElasticsearchManager
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return []
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ëª… ì§‘ê³„ ì¿¼ë¦¬
            agg_query = {
                "size": 0,
                "query": {
                    "term": {
                        "metadata.category.keyword": category
                    }
                },
                "aggs": {
                    "filenames": {
                        "terms": {
                            "field": "metadata.filename.keyword",
                            "size": 1000
                        }
                    }
                }
            }
            
            response = es.search(index=INDEX_NAME, body=agg_query)
            buckets = response.get("aggregations", {}).get("filenames", {}).get("buckets", [])
            
            return [bucket["key"] for bucket in buckets]
            
        except Exception as e:
            print(f"ê¸°ì¡´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    # íŒŒì¼ ëª©ë¡ ì¡°íšŒ í•¨ìˆ˜ë“¤ (ì¸ë±ì‹±ì—ì„œ ì‚¬ìš©)
    @staticmethod
    def list_pdfs(pdf_dir: str) -> List[str]:
        """PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(pdf_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".pdf"):
                files.append(os.path.join(pdf_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_txt_files(txt_dir: str) -> List[str]:
        """TXT íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(txt_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".txt"):
                files.append(os.path.join(txt_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_docx_files(docx_dir: str) -> List[str]:
        """DOCX íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(docx_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith((".docx", ".doc")):
                files.append(os.path.join(docx_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_json_files(json_dir: str) -> List[str]:
        """JSON íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(json_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".json"):
                files.append(os.path.join(json_dir, name))
        return sorted(files)
    
    @staticmethod
    def index_pdfs(pdf_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """PDF íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        return ElasticsearchIndexer._index_files_by_type(
            files=pdf_files,
            file_type="PDF",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=lambda path: PyPDFLoader(path).load()
        )
    
    @staticmethod
    def index_txt_files(txt_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """TXT íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        return ElasticsearchIndexer._index_files_by_type(
            files=txt_files,
            file_type="TXT",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=lambda path: TextLoader(path, encoding='utf-8').load()
        )
    
    @staticmethod
    def index_json_files(json_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """JSON íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        def json_loader(path):
            with open(path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            text_content = ElasticsearchIndexer._extract_text_from_json(json_data)
            if text_content:
                return [Document(
                    page_content=text_content,
                    metadata={"source": path, "filename": os.path.basename(path), "category": "JSON"}
                )]
            return []
        
        return ElasticsearchIndexer._index_files_by_type(
            files=json_files,
            file_type="JSON",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=json_loader
        )
    
    @staticmethod
    def index_docx_files(docx_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """DOCX íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        def docx_loader(path):
            try:
                # DocParser ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
                from utils.docparser import DocParser
                parser = DocParser(path)
                text_content = parser.get_text()
                
                docs = [Document(
                    page_content=text_content,
                    metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
                )]
                
                # í‘œ ë‚´ìš© ì¶”ê°€
                table_blocks = parser.get_tables()
                for table_content in table_blocks:
                    docs.append(Document(
                        page_content=table_content,
                        metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
                    ))
                
                return docs
            except ImportError:
                # DocParserê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ python-docx ì‚¬ìš©
                return ElasticsearchIndexer._docx_basic_loader(path)
        
        return ElasticsearchIndexer._index_files_by_type(
            files=docx_files,
            file_type="DOCX",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=docx_loader
        )
    
    @staticmethod
    def _docx_basic_loader(path: str) -> List[Document]:
        """ê¸°ë³¸ DOCX ë¡œë” (python-docx ì‚¬ìš©)"""
        doc = DocxDocument(path)
        text_content = []
        
        # ë‹¨ë½ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_content.append(paragraph.text)
        
        # í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text.strip():
                        text_content.append(cell.text)
        
        if text_content:
            full_text = "\n".join(text_content)
            return [Document(
                page_content=full_text,
                metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
            )]
        return []
    
    @staticmethod
    def _index_files_by_type(files: List[str], file_type: str, embeddings, hybrid_tracker, 
                           incremental: bool, loader_func) -> Tuple[bool, str]:
        """íŒŒì¼ íƒ€ì…ë³„ í†µí•© ì¸ë±ì‹± ë¡œì§"""
        stage_name = f"{file_type}_ì¸ë±ì‹±_ì‹œì‘"
        hybrid_tracker.track_preprocessing_stage(stage_name)
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if not incremental:
            ElasticsearchIndexer._delete_entire_index()
        else:
            success, message = ElasticsearchIndexer.delete_documents_by_category([file_type])
            if not success:
                hybrid_tracker.end_preprocessing_stage(stage_name)
                return False, f"ê¸°ì¡´ {file_type} ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        
        # íŒŒì¼ë³„ ì²˜ë¦¬
        for file_path in files:
            file_stage = f"{file_type}_ì²˜ë¦¬_{os.path.basename(file_path)}"
            hybrid_tracker.track_preprocessing_stage(file_stage)
            
            try:
                docs = loader_func(file_path)
                
                # í…ìŠ¤íŠ¸ ë¶„í•  (í‘œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ)
                splitter = get_optimized_text_splitter()
                for doc in docs:
                    if "í‘œ" in doc.page_content or len(doc.page_content.split('\n')) > 5:
                        # í‘œì´ê±°ë‚˜ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ë¶„í• í•˜ì§€ ì•ŠìŒ
                        all_documents.append(doc)
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë¶„í• 
                        chunks = splitter.split_documents([doc])
                        all_documents.extend(chunks)
                
                # ë©”íƒ€ë°ì´í„° ë³´ê°•
                for doc in all_documents[-len(docs):]:  # ë°©ê¸ˆ ì¶”ê°€ëœ ë¬¸ì„œë“¤ë§Œ
                    doc.metadata.update({
                        "source": file_path,
                        "filename": os.path.basename(file_path),
                        "category": file_type
                    })
                
            except Exception as e:
                print(f"{file_type} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                continue
            finally:
                hybrid_tracker.end_preprocessing_stage(file_stage)
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage(stage_name)
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # PDF íŒŒì¼ì˜ ê²½ìš° ì²˜ë¦¬ ê²°ê³¼ë¥¼ export
        if file_type.upper() == "PDF":
            export_path = ElasticsearchIndexer._export_pdf_processing_data(all_documents, "data")
            if export_path:
                print(f"ğŸ“Š PDF ì²˜ë¦¬ ê²°ê³¼ê°€ exportë˜ì—ˆìŠµë‹ˆë‹¤: {export_path}")
        
        # Elasticsearchì— ì €ì¥
        return ElasticsearchIndexer._save_to_elasticsearch(all_documents, embeddings, hybrid_tracker, stage_name, file_type)
    
    @staticmethod
    def _delete_entire_index():
        """ì „ì²´ ì¸ë±ìŠ¤ ì‚­ì œ"""
        config = ElasticsearchManager.get_connection_config()
        es = Elasticsearch(**config)
        if es.indices.exists(index=INDEX_NAME):
            es.indices.delete(index=INDEX_NAME)
    
    @staticmethod
    def _save_to_elasticsearch(documents: List[Document], embeddings, hybrid_tracker, stage_name: str, file_type: str) -> Tuple[bool, str]:
        """Elasticsearchì— ë¬¸ì„œ ì €ì¥"""
        save_stage = "Elasticsearch_ì €ì¥"
        hybrid_tracker.track_preprocessing_stage(save_stage)
        
        try:
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            mapping = {
                "mappings":{
                    "properties":{
                        "text":{
                            "type":"text",
                            "analyzer":"nori"
                        }
                    }
                }
            }

            if es_client.indices.exists(index=INDEX_NAME):
                es_client.indices.delete(index=INDEX_NAME)
            es_client.indices.create(index=INDEX_NAME, body=mapping)

            ElasticsearchStore.from_documents(
                documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            return True, f"{file_type} ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            return False, f"{file_type} ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
        finally:
            hybrid_tracker.end_preprocessing_stage(save_stage)
            hybrid_tracker.end_preprocessing_stage(stage_name)
    
    @staticmethod
    def _export_pdf_processing_data(data, export_dir: str = "data") -> str:
        """PDF ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ exportí•˜ëŠ” í•¨ìˆ˜ - ë‹¤ì–‘í•œ ë°ì´í„° í˜•íƒœ ì§€ì›"""
        try:
            import uuid
            from datetime import datetime
            
            # export ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(export_dir, exist_ok=True)
            
            # export íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_filename = f"pdf_indexing_export_{timestamp}.json"
            export_path = os.path.join(export_dir, export_filename)
            
            # ë°ì´í„° í˜•íƒœ íŒë³„ ë° ì²˜ë¦¬
            processed_documents = []
            
            if isinstance(data, list) and data:
                # ì²« ë²ˆì§¸ ìš”ì†Œë¡œ ë°ì´í„° í˜•íƒœ íŒë³„
                first_item = data[0]
                
                if hasattr(first_item, 'page_content'):
                    # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (_index_files_by_typeì—ì„œ í˜¸ì¶œ)
                    processed_documents = data
                elif isinstance(first_item, dict) and 'content' in first_item:
                    # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (index_all_filesì—ì„œ í˜¸ì¶œ)
                    for item in data:
                        # ë”•ì…”ë„ˆë¦¬ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜
                        content = item.get('content', '')
                        metadata = item.get('metadata', {})
                        
                        # Document-like ê°ì²´ ìƒì„± (page_contentì™€ metadata ì†ì„±ì„ ê°€ì§„ ê°ì²´)
                        class DocumentLike:
                            def __init__(self, page_content, metadata):
                                self.page_content = page_content
                                self.metadata = metadata
                        
                        processed_documents.append(DocumentLike(content, metadata))
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•íƒœì…ë‹ˆë‹¤: {type(first_item)}")
            else:
                print("âŒ ë¹ˆ ë°ì´í„° ë˜ëŠ” ì˜ëª»ëœ í˜•íƒœì…ë‹ˆë‹¤.")
                return ""
            
            # ë¬¸ì„œ ë¶„ì„
            export_data = {
                "export_info": {
                    "export_time": datetime.now().isoformat(),
                    "total_documents": len(processed_documents),
                    "file_type": "PDF",
                    "description": "PDF ì¸ë±ì‹± ê³¼ì •ì—ì„œ vector DBì— ì €ì¥ë  ë¬¸ì„œë“¤"
                },
                "documents": []
            }
            
            # í‘œ ê´€ë ¨ í†µê³„
            table_documents = 0
            mixed_documents = 0
            total_table_markers = 0
            total_markdown_lines = 0
            
            for doc in processed_documents:
                # ë¬¸ì„œ ID ìƒì„±
                doc_id = str(uuid.uuid4())
                
                # í‘œ ê´€ë ¨ ë¶„ì„
                content = doc.page_content
                has_table = ElasticsearchIndexer._is_table_content(content)
                table_markers = content.count('**[í‘œ')
                markdown_table_lines = len([line for line in content.split('\n') if '|' in line and line.strip()])
                
                if has_table:
                    table_documents += 1
                if table_markers > 0:
                    mixed_documents += 1
                
                total_table_markers += table_markers
                total_markdown_lines += markdown_table_lines
                
                # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
                doc_info = {
                    "document_id": doc_id,
                    "filename": doc.metadata.get('filename', 'unknown'),
                    "source": doc.metadata.get('source', ''),
                    "content_length": len(content),
                    "line_count": len(content.split('\n')),
                    "has_table": has_table,
                    "table_markers_found": table_markers,
                    "markdown_table_lines": markdown_table_lines,
                    "metadata": doc.metadata,
                    "content": content
                }
                
                # í˜ì´ì§€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if 'page' in doc.metadata:
                    doc_info["page"] = doc.metadata['page']
                
                # í‘œ ê°œìˆ˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if 'table_count' in doc.metadata:
                    doc_info["table_count"] = doc.metadata['table_count']
                
                export_data["documents"].append(doc_info)
            
            # í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
            export_data["export_info"].update({
                "total_table_documents": table_documents,
                "total_mixed_documents": mixed_documents,
                "table_detection_ratio": f"{table_documents}/{len(processed_documents)} ({table_documents/len(processed_documents)*100:.1f}%)" if processed_documents else "0/0 (0%)"
            })
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š PDF ì²˜ë¦¬ ë°ì´í„° export ì™„ë£Œ: {export_path}")
            print(f"   - ì´ ë¬¸ì„œ: {len(processed_documents)}ê°œ")
            print(f"   - í‘œ í¬í•¨ ë¬¸ì„œ: {table_documents}ê°œ")
            print(f"   - ì¸ë¼ì¸ í‘œ ë§ˆì»¤: {total_table_markers}ê°œ")
            print(f"   - ë§ˆí¬ë‹¤ìš´ í‘œ ë¼ì¸: {total_markdown_lines}ê°œ")
            
            return export_path
            
        except Exception as e:
            print(f"âŒ PDF ë°ì´í„° export ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return ""
    
    @staticmethod
    def _is_table_content(content: str) -> bool:
        """í‘œ ë‚´ìš©ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ - ê°œì„ ëœ ë²„ì „"""
        if not content or len(content.strip()) < 10:
            return False
        
        # 1. ì¸ë¼ì¸ í‘œ ë§ˆì»¤ê°€ ìˆëŠ” ê²½ìš° (**[í‘œ N]**)
        if re.search(r'\*\*\[í‘œ\s*\d*\]\*\*', content):
            return True
        
        # 2. mixed íƒ€ì… ë¬¸ì„œì¸ì§€ í™•ì¸ (ë©”íƒ€ë°ì´í„°ì—ì„œ)
        if 'type' in content and 'mixed' in content:
            return True
        
        # 3. ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ì´ í¬í•¨ëœ ê²½ìš°
        lines = content.split('\n')
        pipe_lines = [line for line in lines if '|' in line and line.strip()]
        
        if len(pipe_lines) >= 3:  # ìµœì†Œ 3ì¤„ ì´ìƒ
            # í—¤ë”ì™€ êµ¬ë¶„ì„ ì´ ìˆëŠ”ì§€ í™•ì¸
            has_separator = any('---' in line or '===' in line for line in pipe_lines[:5])
            if has_separator:
                # íŒŒì´í”„ ë¼ì¸ ë¹„ìœ¨ í™•ì¸
                pipe_ratio = len(pipe_lines) / len(lines) if lines else 0
                if pipe_ratio >= 0.15:  # 15% ì´ìƒì´ í…Œì´ë¸” ë¼ì¸
                    return True
        
        # 4. í‘œ ë§ˆì»¤ì™€ í…Œì´ë¸” êµ¬ì¡°ê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš°
        if re.search(r'\*\*\[í‘œ\]?\*\*', content) and len(pipe_lines) >= 2:
            return True
        
        # 5. í…Œì´ë¸” ê´€ë ¨ í‚¤ì›Œë“œì™€ êµ¬ì¡°ì  íŠ¹ì§• ê²°í•© í™•ì¸
        table_keywords = ['í‘œ', 'êµ¬ë¶„', 'í•­ëª©', 'ë¶„ë¥˜', 'ê¸°ì¤€', 'ì¡°ê±´', 'ì—…ë¬´', 'ì ˆì°¨', 'ë°©ë²•', 'ë‚´ìš©']
        keyword_found = any(keyword in content for keyword in table_keywords)
        
        if keyword_found and len(pipe_lines) >= 2:
            # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ í…Œì´ë¸” íŒ¨í„´ ê°ì§€
            table_pattern = r'\|[^|]*\|[^|]*\|'
            if re.search(table_pattern, content):
                return True
        
        # 6. êµ¬ì¡°í™”ëœ ì •ë³´ íŒ¨í„´ ê°ì§€ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        structured_lines = [line for line in lines if re.search(r'[â€¢Â·â–ª\-]\s*', line) or ':' in line]
        if len(structured_lines) >= 3 and len(structured_lines) / len(lines) >= 0.25:
            return True
        
        # 7. í˜ì´ì§€ ì •ë³´ì™€ í•¨ê»˜ ìˆëŠ” í‘œ ë°ì´í„°
        if "í˜ì´ì§€" in content and len(pipe_lines) >= 2:
            return True
        
        # 8. í‘œ ì œëª©ì´ ìˆëŠ” ê²½ìš°
        if re.search(r'í‘œ\s*\d+', content) and len(pipe_lines) >= 1:
            return True
        
        return False
    
    @staticmethod
    def _extract_text_from_json(json_data: Any, max_depth: int = 10) -> str:
        """JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if max_depth <= 0:
            return ""
        
        text_parts = []
        
        if isinstance(json_data, dict):
            for key, value in json_data.items():
                text_parts.append(str(key))
                if isinstance(value, (dict, list)):
                    text_parts.append(ElasticsearchIndexer._extract_text_from_json(value, max_depth - 1))
                else:
                    text_parts.append(str(value))
        elif isinstance(json_data, list):
            for item in json_data:
                if isinstance(item, (dict, list)):
                    text_parts.append(ElasticsearchIndexer._extract_text_from_json(item, max_depth - 1))
                else:
                    text_parts.append(str(item))
        else:
            text_parts.append(str(json_data))
        
        return " ".join(filter(None, text_parts))
    
    @staticmethod
    def index_all_files(data_dir: str, embeddings, hybrid_tracker, file_types: List[str] = None, incremental: bool = True) -> Tuple[bool, str]:
        """ëª¨ë“  íŒŒì¼ íƒ€ì…ì„ í•œ ë²ˆì— ì¸ë±ì‹±"""
        if file_types is None:
            file_types = ['pdf', 'txt', 'json', 'docx']
        
        hybrid_tracker.track_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if not incremental:
            ElasticsearchIndexer._delete_entire_index()
        else:
            categories_to_delete = [ft.upper() for ft in file_types]
            success, message = ElasticsearchIndexer.delete_documents_by_category(categories_to_delete)
            if not success:
                hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        processed_files = 0
        
        # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬ ë§¤í•‘
        file_processors = {
            'pdf': (ElasticsearchIndexer.list_pdfs, lambda path: PyPDFLoader(path).load()),
            'txt': (ElasticsearchIndexer.list_txt_files, lambda path: TextLoader(path, encoding='utf-8').load()),
            'json': (ElasticsearchIndexer.list_json_files, ElasticsearchIndexer._process_json_file),
            'docx': (ElasticsearchIndexer.list_docx_files, ElasticsearchIndexer._process_docx_file)
        }
        
        for file_type in file_types:
            if file_type not in file_processors:
                continue
                
            list_func, loader_func = file_processors[file_type]
            type_dir = os.path.join(data_dir, file_type)
            files = list_func(type_dir)
            
            # PDF íŒŒì¼ì¸ ê²½ìš° exportë¥¼ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ì´ˆê¸°í™”
            pdf_processing_data = [] if file_type.lower() == 'pdf' else None
            
            for file_path in files:
                stage_name = f"{file_type.upper()}_ì²˜ë¦¬_{os.path.basename(file_path)}"
                hybrid_tracker.track_preprocessing_stage(stage_name)
                
                try:
                    docs = loader_func(file_path)
                    splitter = get_optimized_text_splitter()
                    
                    for doc in docs:
                        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
                        base_metadata = {
                            "source": file_path,
                            "filename": os.path.basename(file_path)
                        }
                        
                        # JSON íŒŒì¼ì˜ ê²½ìš° ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë³´ì¡´
                        if file_type.lower() == 'json' and 'category' in doc.metadata:
                            base_metadata["file_type"] = file_type.upper()
                            # ê¸°ì¡´ categoryëŠ” ìœ ì§€
                        else:
                            base_metadata["category"] = file_type.upper()
                        
                        doc.metadata.update(base_metadata)
                        
                        # PDF íŒŒì¼ì¸ ê²½ìš° export ë°ì´í„° ìˆ˜ì§‘
                        if file_type.lower() == 'pdf' and pdf_processing_data is not None:
                            is_table = ElasticsearchIndexer._is_table_content(doc.page_content)
                            pdf_processing_data.append({
                                "content": doc.page_content,
                                "metadata": doc.metadata,
                                "is_table": is_table,
                                "content_length": len(doc.page_content),
                                "filename": os.path.basename(file_path)
                            })
                        
                        # í‘œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ë¶„í• 
                        if "í‘œ" not in doc.page_content:
                            chunks = splitter.split_documents([doc])
                            all_documents.extend(chunks)
                        else:
                            all_documents.append(doc)
                    
                    processed_files += 1
                    
                except Exception as e:
                    print(f"{file_type.upper()} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                finally:
                    hybrid_tracker.end_preprocessing_stage(stage_name)
            
            # PDF íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ export ì‹¤í–‰
            if file_type.lower() == 'pdf' and pdf_processing_data:
                try:
                    ElasticsearchIndexer._export_pdf_processing_data(pdf_processing_data, data_dir)
                    print(f"ğŸ“Š PDF ì²˜ë¦¬ ë°ì´í„°ê°€ /data ë””ë ‰í† ë¦¬ì— exportë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"âš ï¸ PDF export ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
            try:
                es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
                cnt = es_client.count(index=INDEX_NAME).get("count", 0) if success else 0
                return True, f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ, í˜„ì¬ ë¬¸ì„œ ìˆ˜: {cnt}"
            except:
                return True, "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ"
        
        # Elasticsearchì— ì €ì¥
        result, message = ElasticsearchIndexer._save_to_elasticsearch(
            all_documents, embeddings, hybrid_tracker, "ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘", "ì „ì²´"
        )
        
        if result:
            return True, f"ì „ì²´ íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ, ì´ ë¬¸ì„œ : {message.split('ë¬¸ì„œ ìˆ˜: ')[1] if 'ë¬¸ì„œ ìˆ˜: ' in message else ''}"
        else:
            return False, message
    
    @staticmethod
    def _process_json_file(path: str) -> List[Document]:
        """JSON íŒŒì¼ ì²˜ë¦¬ - ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì„¸ë¶„í™”ëœ ë¬¸ì„œ ë¶„í• """
        with open(path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        documents = []
        
        # converted_index.json í˜•íƒœì˜ êµ¬ì¡°í™”ëœ JSONì¸ì§€ í™•ì¸
        if isinstance(json_data, list) and len(json_data) > 0:
            for file_entry in json_data:
                if isinstance(file_entry, dict) and 'filename' in file_entry and 'data' in file_entry:
                    # êµ¬ì¡°í™”ëœ JSON ì²˜ë¦¬
                    filename = file_entry.get('filename', 'unknown')
                    data_items = file_entry.get('data', [])
                    
                    # ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì¸µ êµ¬ì¡° ì²˜ë¦¬
                    for item in data_items:
                        if isinstance(item, dict) and item.get('content', '').strip():
                            # ê° í•­ëª©ì„ ê°œë³„ ë¬¸ì„œë¡œ ìƒì„± (ìµœëŒ€í•œ ì„¸ë¶„í™”)
                            title = item.get('title', '').strip()
                            heading = item.get('heading', '').strip()
                            section = item.get('section', '').strip()
                            content = item.get('content', '').strip()
                            
                            if not content:
                                continue
                            
                            # í‚¤ì›Œë“œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ì—ì„œ í™œìš©)
                            keywords = ElasticsearchIndexer._extract_keywords(content)
                            
                            # ë©”íƒ€ë°ì´í„° êµ¬ì„± - ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í’ë¶€í•œ ì •ë³´
                            metadata = {
                                'source_filename': filename,
                                'document_type': 'structured_json',
                                'title': title if title else 'ê¸°íƒ€',
                                'heading': heading if heading else '',
                                'section': section if section else '',
                                'category': ElasticsearchIndexer._extract_category(title, heading),
                                'topic': ElasticsearchIndexer._extract_topic(title, heading, section),
                                'content_type': ElasticsearchIndexer._classify_content_type(content),
                                'has_table': item.get('hasTable', False),
                                'keywords': ', '.join(keywords) if keywords else '',  # ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ë¬¸ìì—´
                                'keyword_count': len(keywords),  # í‚¤ì›Œë“œ ê°œìˆ˜
                                'content_length': len(content),  # ë‚´ìš© ê¸¸ì´
                                'hierarchy_depth': len([x for x in [title, heading, section] if x.strip()]),  # ê³„ì¸µ ê¹Šì´
                                'title_clean': title.strip('[]') if title else '',  # ëŒ€ê´„í˜¸ ì œê±°í•œ ì œëª©
                                'is_procedure': 'ì ˆì°¨' in content or 'ë‹¨ê³„' in content,  # ì ˆì°¨ ë¬¸ì„œ ì—¬ë¶€
                                'has_numbers': bool(re.search(r'\d+', content)),  # ìˆ«ì í¬í•¨ ì—¬ë¶€
                                'urgency_level': 'high' if any(word in content.lower() for word in ['ê¸´ê¸‰', 'ì¦‰ì‹œ', 'ì£¼ì˜', 'ê²½ê³ ']) else 'normal'  # ê¸´ê¸‰ë„
                            }
                            
                            # ê²€ìƒ‰ ì¹œí™”ì  ë¬¸ì„œ ë‚´ìš© êµ¬ì„± - ë©”íƒ€ë°ì´í„° í™œìš©
                            document_parts = []
                            
                            # ì œëª© ê³„ì¸µ êµ¬ì¡° ì¶”ê°€ (ê²€ìƒ‰ ê°€ì¤‘ì¹˜ í–¥ìƒ)
                            if title:
                                document_parts.append(f"ëŒ€ë¶„ë¥˜: {title}")
                                document_parts.append(f"ì¹´í…Œê³ ë¦¬: {title.strip('[]')}")  # ëŒ€ê´„í˜¸ ì œê±°í•œ ë²„ì „
                            if heading:
                                document_parts.append(f"ì†Œë¶„ë¥˜: {heading}")
                            if section:
                                document_parts.append(f"ì„¸ë¶€í•­ëª©: {section}")
                            
                            # í‚¤ì›Œë“œ ê°•í™” (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
                            if keywords:
                                document_parts.append(f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords)}")
                                document_parts.append(f"ê²€ìƒ‰ì–´: {' '.join(keywords)}")  # í‚¤ì›Œë“œ ë°˜ë³µìœ¼ë¡œ ê²€ìƒ‰ ê°•í™”
                            
                            # ë¬¸ì„œ íŠ¹ì„± í‘œì‹œ
                            doc_features = []
                            if item.get('hasTable', False):
                                doc_features.extend(["í‘œ í¬í•¨", "ë°ì´í„°í‘œ", "í‘œí˜•íƒœì •ë³´"])
                            if metadata['is_procedure']:
                                doc_features.extend(["ì ˆì°¨ì•ˆë‚´", "ë‹¨ê³„ë³„ê°€ì´ë“œ"])
                            if metadata['has_numbers']:
                                doc_features.extend(["ìˆ˜ì¹˜ì •ë³´", "ì •ëŸ‰ë°ì´í„°"])
                            if doc_features:
                                document_parts.append(f"ë¬¸ì„œíŠ¹ì„±: {', '.join(doc_features)}")
                            
                            # ì›ë³¸ ë‚´ìš©
                            document_parts.append(f"ë‚´ìš©: {content}")
                            
                            # ê²€ìƒ‰ìš© ìš”ì•½ ìƒì„±
                            summary = ElasticsearchIndexer._generate_summary(content, title, heading, section)
                            if summary:
                                document_parts.append(f"ìš”ì•½: {summary}")
                            
                            document_content = '\n\n'.join(document_parts)
                            documents.append(Document(page_content=document_content, metadata=metadata))
                    
                    # ëŒ€ë¶„ë¥˜ë³„ í†µí•© ë¬¸ì„œë„ ìƒì„± (ìƒìœ„ ë ˆë²¨ ê²€ìƒ‰ìš©)
                    title_groups = {}
                    for item in data_items:
                        if isinstance(item, dict):
                            title = item.get('title', '').strip()
                            if title and item.get('content', '').strip():
                                if title not in title_groups:
                                    title_groups[title] = []
                                title_groups[title].append(item)
                    
                    for title, items in title_groups.items():
                        if len(items) > 1:  # ì—¬ëŸ¬ í•­ëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ í†µí•© ë¬¸ì„œ ìƒì„±
                            content_parts = []
                            all_keywords = set()
                            has_any_table = False  # í‘œ í¬í•¨ ì—¬ë¶€ ì²´í¬
                            
                            for item in items:
                                content = item.get('content', '').strip()
                                if content:
                                    heading = item.get('heading', '').strip()
                                    section = item.get('section', '').strip()
                                    
                                    # í‘œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                                    if item.get('hasTable', False):
                                        has_any_table = True
                                    
                                    part_header = []
                                    if heading:
                                        part_header.append(heading)
                                    if section:
                                        part_header.append(section)
                                    
                                    if part_header:
                                        content_parts.append(f"[{' - '.join(part_header)}]\n{content}")
                                    else:
                                        content_parts.append(content)
                                    
                                    # í‚¤ì›Œë“œ ìˆ˜ì§‘
                                    item_keywords = ElasticsearchIndexer._extract_keywords(content)
                                    all_keywords.update(item_keywords)
                            
                            if content_parts:
                                consolidated_content = '\n\n'.join(content_parts)
                                
                                # í†µí•© ë¬¸ì„œì˜ í’ë¶€í•œ ë©”íƒ€ë°ì´í„°
                                metadata = {
                                    'source_filename': filename,
                                    'document_type': 'consolidated_json',
                                    'title': title,
                                    'title_clean': title.strip('[]'),
                                    'category': ElasticsearchIndexer._extract_category(title, ''),
                                    'keywords': ', '.join(sorted(all_keywords)),
                                    'keyword_count': len(all_keywords),
                                    'item_count': len(items),
                                    'has_table': has_any_table,
                                    'content_length': len(consolidated_content),
                                    'is_comprehensive': len(items) > 3,  # í¬ê´„ì  ë¬¸ì„œ ì—¬ë¶€
                                    'urgency_level': 'high' if any('ê¸´ê¸‰' in str(item.get('content', '')) for item in items) else 'normal'
                                }
                                
                                # í†µí•© ë¬¸ì„œ ë‚´ìš© êµ¬ì„± (ê²€ìƒ‰ ìµœì í™”)
                                doc_content_parts = [
                                    f"ëŒ€ë¶„ë¥˜: {title}",
                                    f"ì¹´í…Œê³ ë¦¬: {title.strip('[]')}",
                                    f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(sorted(all_keywords))}",
                                    f"ë¬¸ì„œíŠ¹ì„±: {'í‘œí¬í•¨ ' if has_any_table else ''}í¬ê´„ì ë¬¸ì„œ ì´{len(items)}ê°œí•­ëª©",
                                    f"í†µí•© ë‚´ìš©:\n{consolidated_content}"
                                ]
                                
                                document_content = '\n\n'.join(doc_content_parts)
                                documents.append(Document(page_content=document_content, metadata=metadata))
                else:
                    # ì¼ë°˜ JSON ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
                    text_content = ElasticsearchIndexer._extract_text_from_json(file_entry)
                    if text_content:
                        documents.append(Document(page_content=text_content, metadata={}))
        else:
            # ì¼ë°˜ JSON ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            text_content = ElasticsearchIndexer._extract_text_from_json(json_data)
            if text_content:
                documents.append(Document(page_content=text_content, metadata={}))
        
        return documents
    
    @staticmethod
    def _extract_category(title: str, heading: str) -> str:
        """ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        text = f"{title} {heading}".lower()
        
        # ë” ì •í™•í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        if 'ê¸°ë³¸ì—…ë¬´' in text or 'ì²˜ë¦¬' in text or 'ì ˆì°¨' in text:
            return 'ì—…ë¬´ì ˆì°¨'
        elif 'ì†Œë¹„ì' in text or 'ê°€ì´ë“œ' in text or 'ì¢…ë¥˜' in text:
            return 'ì´ìš©ê°€ì´ë“œ'
        elif 'ë°œê¸‰' in text or 'ì‹ ì²­' in text:
            return 'ì¹´ë“œë°œê¸‰'
        elif 'ì´ìš©í•œë„' in text or 'í•œë„' in text:
            return 'ì´ìš©í•œë„'
        elif 'ì—°ì²´' in text or 'ì¶”ì‹¬' in text or 'ë²•ì ì¡°ì¹˜' in text:
            return 'ì—°ì²´ê´€ë¦¬'
        elif 'ë¶€ê°€ì„œë¹„ìŠ¤' in text or 'í˜œíƒ' in text or 'í¬ì¸íŠ¸' in text:
            return 'ë¶€ê°€ì„œë¹„ìŠ¤'
        elif 'ìˆ˜ìˆ˜ë£Œ' in text or 'ì—°íšŒë¹„' in text or 'ìš”ê¸ˆ' in text:
            return 'ìˆ˜ìˆ˜ë£Œì •ë³´'
        elif 'ë³´ì•ˆ' in text or 'ë¶„ì‹¤' in text or 'ë„ë‚œ' in text:
            return 'ë³´ì•ˆê´€ë¦¬'
        elif 'í•´ì™¸' in text or 'êµ­ì™¸' in text:
            return 'í•´ì™¸ì´ìš©'
        elif 'í˜„ê¸ˆì„œë¹„ìŠ¤' in text or 'ëŒ€ì¶œ' in text or 'ì¹´ë“œë¡ ' in text:
            return 'ëŒ€ì¶œì„œë¹„ìŠ¤'
        else:
            # titleì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„ (ëŒ€ê´„í˜¸ ì œê±°)
            if '[' in title and ']' in title:
                category_match = title.strip('[]').strip()
                if category_match and len(category_match) < 20:
                    return category_match
            return 'ê¸°íƒ€'
    
    @staticmethod
    def _extract_topic(title: str, heading: str, section: str) -> str:
        """ì„¸ë¶€ ì£¼ì œ ì¶”ì¶œ"""
        parts = [p for p in [title, heading, section] if p.strip()]
        return ' - '.join(parts) if parts else 'ê¸°íƒ€'
    
    @staticmethod
    def _classify_content_type(content: str) -> str:
        """ë‚´ìš© ìœ í˜• ë¶„ë¥˜"""
        content_lower = content.lower()
        
        if 'ì ˆì°¨' in content_lower or 'ë‹¨ê³„' in content_lower:
            return 'ì ˆì°¨ì•ˆë‚´'
        elif 'ê¸°ì¤€' in content_lower or 'ì¡°ê±´' in content_lower:
            return 'ê¸°ì¤€ì •ë³´'
        elif 'ë°©ë²•' in content_lower or 'í™œìš©' in content_lower:
            return 'ì´ìš©ë°©ë²•'
        elif 'êµ¬ë¶„' in content_lower or 'ì¢…ë¥˜' in content_lower:
            return 'ë¶„ë¥˜ì •ë³´'
        elif 'ì£¼ì˜' in content_lower or 'ê¸ˆì§€' in content_lower:
            return 'ì£¼ì˜ì‚¬í•­'
        else:
            return 'ì¼ë°˜ì •ë³´'
    
    @staticmethod
    def _extract_keywords(content: str) -> List[str]:
        """ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        import re
        
        # ê¸ˆìœµ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ íŒ¨í„´ (ë” í¬ê´„ì )
        keyword_patterns = [
            r'ì‹ ìš©ì¹´ë“œ|ì²´í¬ì¹´ë“œ|ì§ë¶ˆì¹´ë“œ|ì„ ë¶ˆì¹´ë“œ|ê°€ì¡±ì¹´ë“œ|ë²•ì¸ì¹´ë“œ',
            r'ì¹´ë“œë°œê¸‰|ë°œê¸‰ê¸°ì¤€|ë°œê¸‰ì ˆì°¨|ë°œê¸‰ì¡°ê±´|ë°œê¸‰ì‹ ì²­',
            r'ì´ìš©í•œë„|ê²°ì œëŠ¥ë ¥|ì‹ ìš©ë“±ê¸‰|ì‹ ìš©í‰ì |ì‹ ìš©ë„',
            r'ì—°ì²´|ì¶”ì‹¬|ë²•ì ì¡°ì¹˜|ì—°ì²´ì •ë³´|ì±„ê¶Œì¶”ì‹¬',
            r'ë¶€ê°€ì„œë¹„ìŠ¤|í¬ì¸íŠ¸|í• ë¶€|ì ë¦½|í˜œíƒ|ë¦¬ì›Œë“œ',
            r'ì—°íšŒë¹„|ìˆ˜ìˆ˜ë£Œ|ì´ì|ìš”ê¸ˆ|ë¹„ìš©|ëŒ€ê¸ˆ',
            r'VISA|Master|AMEX|JCB|BCì¹´ë“œ',
            r'ê°€ì²˜ë¶„ì†Œë“|ê¸ˆìœµê±°ë˜|ì‹ ìš©ì •ë³´|ê°œì¸ì‹ ìš©í‰ê°€',
            r'í˜„ê¸ˆì„œë¹„ìŠ¤|ì¹´ë“œë¡ |ëŒ€ì¶œ|ì—¬ì‹ |í•œë„',
            r'ë¶„ì‹¤|ë„ë‚œ|ì¬ë°œê¸‰|ì •ì§€|í•´ì§€|ì·¨ì†Œ',
            r'ê°€ë§¹ì |ê²°ì œ|ì¼ì‹œë¶ˆ|ë¦¬ë³¼ë¹™|ë¬´ì´ìí• ë¶€',
            r'í•´ì™¸|êµ­ì™¸|êµ­ë‚´|ì˜¨ë¼ì¸|ì˜¤í”„ë¼ì¸'
        ]
        
        keywords = set()
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for pattern in keyword_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.update(matches)
        
        # ì¤‘ìš”í•œ ìˆ«ìë‚˜ ë¹„ìœ¨ ì •ë³´
        number_patterns = re.findall(r'\d+(?:\.\d+)?%|\d+ë§Œì›|\d+ê°œì›”|\d+ì¼|\d+ë…„', content)
        keywords.update(number_patterns)
        
        # ì ˆì°¨ë‚˜ ë‹¨ê³„ ê´€ë ¨ í‚¤ì›Œë“œ
        if 'ì ˆì°¨' in content or 'ë‹¨ê³„' in content or 'ë°©ë²•' in content:
            procedure_words = re.findall(r'[â‘ -â‘³]|[ê°€-í£]\)|[1-9]\.|ì²«ì§¸|ë‘˜ì§¸|ì…‹ì§¸', content)
            keywords.update(procedure_words[:5])
        
        # ê´„í˜¸ ì•ˆì˜ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        bracket_content = re.findall(r'[ã€Œã€\[]([^ã€ã€\]]+)[ã€ã€\]]', content)
        for bc in bracket_content:
            if 2 < len(bc) < 15:  # ì ì ˆí•œ ê¸¸ì´ì˜ ë‚´ìš©ë§Œ
                keywords.add(bc)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬, ê¸¸ì´ ì œí•œ
        result = sorted([k for k in keywords if len(k) > 1 and len(k) < 15])
        return result[:12]  # ìµœëŒ€ 12ê°œ í‚¤ì›Œë“œ
    
    @staticmethod
    def _generate_summary(content: str, title: str, heading: str, section: str) -> str:
        """ë‚´ìš© ìš”ì•½ ìƒì„±"""
        # ì²« ë¬¸ì¥ì´ë‚˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        sentences = content.split('.')
        if sentences:
            first_sentence = sentences[0].strip()
            if len(first_sentence) > 10 and len(first_sentence) < 200:
                return first_sentence
        
        # ì œëª© ì •ë³´ ê¸°ë°˜ ìš”ì•½
        context_parts = [p for p in [title, heading, section] if p.strip()]
        if context_parts:
            return f"{' - '.join(context_parts)}ì— ëŒ€í•œ ì •ë³´"
        
        return ""
    
    @staticmethod
    def _process_docx_file(path: str) -> List[Document]:
        """DOCX íŒŒì¼ ì²˜ë¦¬"""
        try:
            from utils.docparser import DocParser
            parser = DocParser(path)
            text_content = parser.get_text()
            
            docs = [Document(page_content=text_content, metadata={})]
            
            # í‘œ ë‚´ìš© ì¶”ê°€
            table_blocks = parser.get_tables()
            for table_content in table_blocks:
                docs.append(Document(page_content=table_content, metadata={}))
            
            return docs
        except ImportError:
            return ElasticsearchIndexer._docx_basic_loader(path)
