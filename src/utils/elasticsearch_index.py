"""
Elasticsearch ì¸ë±ì‹± ì „ìš© ìœ í‹¸ë¦¬í‹°
"""
import os
import json
from typing import List, Tuple, Any
from elasticsearch import Elasticsearch
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import ElasticsearchStore
from langchain.schema import Document
from docx import Document as DocxDocument
from core.config import ELASTICSEARCH_URL, INDEX_NAME
from .elasticsearch import ElasticsearchManager, get_optimized_text_splitter


class ElasticsearchIndexer:
    """Elasticsearch ì¸ë±ì‹± ì „ìš© í´ë˜ìŠ¤"""
    
    @staticmethod
    def delete_documents_by_category(categories: List[str]) -> Tuple[bool, str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë“¤ì„ ì„ íƒì ìœ¼ë¡œ ì‚­ì œ"""
        try:
            from utils.elasticsearch import ElasticsearchManager
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return True, "ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
            # ì‚­ì œí•  ì¹´í…Œê³ ë¦¬ë“¤ì— ëŒ€í•œ ì¿¼ë¦¬ ìƒì„±
            delete_query = {
                "query": {
                    "terms": {
                        "metadata.category.keyword": categories
                    }
                }
            }
            
            # ì‚­ì œ ì „ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_before = es.count(index=INDEX_NAME).get("count", 0)
            
            # ì„ íƒì  ì‚­ì œ ì‹¤í–‰
            delete_response = es.delete_by_query(
                index=INDEX_NAME,
                body=delete_query,
                refresh=True
            )
            
            deleted_count = delete_response.get("deleted", 0)
            count_after = es.count(index=INDEX_NAME).get("count", 0)
            
            return True, f"ì¹´í…Œê³ ë¦¬ {categories} ë¬¸ì„œ {deleted_count}ê°œ ì‚­ì œë¨ (ì „ì²´: {count_before} â†’ {count_after})"
            
        except Exception as e:
            return False, f"ì„ íƒì  ì‚­ì œ ì˜¤ë¥˜: {str(e)}"
    
    @staticmethod
    def get_existing_files_by_category(category: str) -> List[str]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì¡´ íŒŒì¼ëª… ëª©ë¡ ë°˜í™˜"""
        try:
            from utils.elasticsearch import ElasticsearchManager
            config = ElasticsearchManager.get_connection_config()
            es = Elasticsearch(**config)
            
            if not es.indices.exists(index=INDEX_NAME):
                return []
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ëª… ì§‘ê³„ ì¿¼ë¦¬
            agg_query = {
                "size": 0,
                "query": {
                    "term": {
                        "metadata.category.keyword": category
                    }
                },
                "aggs": {
                    "filenames": {
                        "terms": {
                            "field": "metadata.filename.keyword",
                            "size": 1000
                        }
                    }
                }
            }
            
            response = es.search(index=INDEX_NAME, body=agg_query)
            buckets = response.get("aggregations", {}).get("filenames", {}).get("buckets", [])
            
            return [bucket["key"] for bucket in buckets]
            
        except Exception as e:
            print(f"ê¸°ì¡´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    # íŒŒì¼ ëª©ë¡ ì¡°íšŒ í•¨ìˆ˜ë“¤ (ì¸ë±ì‹±ì—ì„œ ì‚¬ìš©)
    @staticmethod
    def list_pdfs(pdf_dir: str) -> List[str]:
        """PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(pdf_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".pdf"):
                files.append(os.path.join(pdf_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_txt_files(txt_dir: str) -> List[str]:
        """TXT íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(txt_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".txt"):
                files.append(os.path.join(txt_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_docx_files(docx_dir: str) -> List[str]:
        """DOCX íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(docx_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith((".docx", ".doc")):
                files.append(os.path.join(docx_dir, name))
        return sorted(files)
    
    @staticmethod
    def list_json_files(json_dir: str) -> List[str]:
        """JSON íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        try:
            names = os.listdir(json_dir)
        except FileNotFoundError:
            return []
        
        files = []
        for name in names:
            if name.lower().endswith(".json"):
                files.append(os.path.join(json_dir, name))
        return sorted(files)
    
    @staticmethod
    def index_pdfs(pdf_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """PDF íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        return ElasticsearchIndexer._index_files_by_type(
            files=pdf_files,
            file_type="PDF",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=lambda path: PyPDFLoader(path).load()
        )
    
    @staticmethod
    def index_txt_files(txt_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """TXT íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        return ElasticsearchIndexer._index_files_by_type(
            files=txt_files,
            file_type="TXT",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=lambda path: TextLoader(path, encoding='utf-8').load()
        )
    
    @staticmethod
    def index_json_files(json_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """JSON íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        def json_loader(path):
            with open(path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            text_content = ElasticsearchIndexer._extract_text_from_json(json_data)
            if text_content:
                return [Document(
                    page_content=text_content,
                    metadata={"source": path, "filename": os.path.basename(path), "category": "JSON"}
                )]
            return []
        
        return ElasticsearchIndexer._index_files_by_type(
            files=json_files,
            file_type="JSON",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=json_loader
        )
    
    @staticmethod
    def index_docx_files(docx_files: List[str], embeddings, hybrid_tracker, incremental: bool = True) -> Tuple[bool, str]:
        """DOCX íŒŒì¼ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±"""
        def docx_loader(path):
            try:
                # DocParser ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
                from utils.docparser import DocParser
                parser = DocParser(path)
                text_content = parser.get_text()
                
                docs = [Document(
                    page_content=text_content,
                    metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
                )]
                
                # í‘œ ë‚´ìš© ì¶”ê°€
                table_blocks = parser.get_tables()
                for table_content in table_blocks:
                    docs.append(Document(
                        page_content=table_content,
                        metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
                    ))
                
                return docs
            except ImportError:
                # DocParserê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ python-docx ì‚¬ìš©
                return ElasticsearchIndexer._docx_basic_loader(path)
        
        return ElasticsearchIndexer._index_files_by_type(
            files=docx_files,
            file_type="DOCX",
            embeddings=embeddings,
            hybrid_tracker=hybrid_tracker,
            incremental=incremental,
            loader_func=docx_loader
        )
    
    @staticmethod
    def _docx_basic_loader(path: str) -> List[Document]:
        """ê¸°ë³¸ DOCX ë¡œë” (python-docx ì‚¬ìš©)"""
        doc = DocxDocument(path)
        text_content = []
        
        # ë‹¨ë½ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_content.append(paragraph.text)
        
        # í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text.strip():
                        text_content.append(cell.text)
        
        if text_content:
            full_text = "\n".join(text_content)
            return [Document(
                page_content=full_text,
                metadata={"source": path, "filename": os.path.basename(path), "category": "DOCX"}
            )]
        return []
    
    @staticmethod
    def _index_files_by_type(files: List[str], file_type: str, embeddings, hybrid_tracker, 
                           incremental: bool, loader_func) -> Tuple[bool, str]:
        """íŒŒì¼ íƒ€ì…ë³„ í†µí•© ì¸ë±ì‹± ë¡œì§"""
        stage_name = f"{file_type}_ì¸ë±ì‹±_ì‹œì‘"
        hybrid_tracker.track_preprocessing_stage(stage_name)
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if not incremental:
            ElasticsearchIndexer._delete_entire_index()
        else:
            success, message = ElasticsearchIndexer.delete_documents_by_category([file_type])
            if not success:
                hybrid_tracker.end_preprocessing_stage(stage_name)
                return False, f"ê¸°ì¡´ {file_type} ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        
        # íŒŒì¼ë³„ ì²˜ë¦¬
        for file_path in files:
            file_stage = f"{file_type}_ì²˜ë¦¬_{os.path.basename(file_path)}"
            hybrid_tracker.track_preprocessing_stage(file_stage)
            
            try:
                docs = loader_func(file_path)
                
                # í…ìŠ¤íŠ¸ ë¶„í•  (í‘œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ)
                splitter = get_optimized_text_splitter()
                for doc in docs:
                    if "í‘œ" in doc.page_content or len(doc.page_content.split('\n')) > 5:
                        # í‘œì´ê±°ë‚˜ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ë¶„í• í•˜ì§€ ì•ŠìŒ
                        all_documents.append(doc)
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë¶„í• 
                        chunks = splitter.split_documents([doc])
                        all_documents.extend(chunks)
                
                # ë©”íƒ€ë°ì´í„° ë³´ê°•
                for doc in all_documents[-len(docs):]:  # ë°©ê¸ˆ ì¶”ê°€ëœ ë¬¸ì„œë“¤ë§Œ
                    doc.metadata.update({
                        "source": file_path,
                        "filename": os.path.basename(file_path),
                        "category": file_type
                    })
                
            except Exception as e:
                print(f"{file_type} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                continue
            finally:
                hybrid_tracker.end_preprocessing_stage(file_stage)
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage(stage_name)
            return False, "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # Elasticsearchì— ì €ì¥
        return ElasticsearchIndexer._save_to_elasticsearch(all_documents, embeddings, hybrid_tracker, stage_name, file_type)
    
    @staticmethod
    def _delete_entire_index():
        """ì „ì²´ ì¸ë±ìŠ¤ ì‚­ì œ"""
        config = ElasticsearchManager.get_connection_config()
        es = Elasticsearch(**config)
        if es.indices.exists(index=INDEX_NAME):
            es.indices.delete(index=INDEX_NAME)
    
    @staticmethod
    def _save_to_elasticsearch(documents: List[Document], embeddings, hybrid_tracker, stage_name: str, file_type: str) -> Tuple[bool, str]:
        """Elasticsearchì— ë¬¸ì„œ ì €ì¥"""
        save_stage = "Elasticsearch_ì €ì¥"
        hybrid_tracker.track_preprocessing_stage(save_stage)
        
        try:
            es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
            if not success:
                raise Exception(f"Elasticsearch ì—°ê²° ì‹¤íŒ¨: {message}")
            
            ElasticsearchStore.from_documents(
                documents,
                embedding=embeddings,
                es_url=ELASTICSEARCH_URL,
                index_name=INDEX_NAME
            )
            
            es_client.indices.refresh(index=INDEX_NAME)
            cnt = es_client.count(index=INDEX_NAME).get("count", 0)
            
            return True, f"{file_type} ì¸ë±ì‹± ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {cnt}"
            
        except Exception as e:
            return False, f"{file_type} ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"
        finally:
            hybrid_tracker.end_preprocessing_stage(save_stage)
            hybrid_tracker.end_preprocessing_stage(stage_name)
    
    @staticmethod
    def _extract_text_from_json(json_data: Any, max_depth: int = 10) -> str:
        """JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if max_depth <= 0:
            return ""
        
        text_parts = []
        
        if isinstance(json_data, dict):
            for key, value in json_data.items():
                text_parts.append(str(key))
                if isinstance(value, (dict, list)):
                    text_parts.append(ElasticsearchIndexer._extract_text_from_json(value, max_depth - 1))
                else:
                    text_parts.append(str(value))
        elif isinstance(json_data, list):
            for item in json_data:
                if isinstance(item, (dict, list)):
                    text_parts.append(ElasticsearchIndexer._extract_text_from_json(item, max_depth - 1))
                else:
                    text_parts.append(str(item))
        else:
            text_parts.append(str(json_data))
        
        return " ".join(filter(None, text_parts))
    
    @staticmethod
    def index_all_files(data_dir: str, embeddings, hybrid_tracker, file_types: List[str] = None, incremental: bool = True) -> Tuple[bool, str]:
        """ëª¨ë“  íŒŒì¼ íƒ€ì…ì„ í•œ ë²ˆì— ì¸ë±ì‹±"""
        if file_types is None:
            file_types = ['pdf', 'txt', 'json', 'docx']
        
        hybrid_tracker.track_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if not incremental:
            ElasticsearchIndexer._delete_entire_index()
        else:
            categories_to_delete = [ft.upper() for ft in file_types]
            success, message = ElasticsearchIndexer.delete_documents_by_category(categories_to_delete)
            if not success:
                hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
                return False, f"ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {message}"
            print(f"ğŸ“š {message}")
        
        all_documents = []
        processed_files = 0
        
        # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬ ë§¤í•‘
        file_processors = {
            'pdf': (ElasticsearchIndexer.list_pdfs, lambda path: PyPDFLoader(path).load()),
            'txt': (ElasticsearchIndexer.list_txt_files, lambda path: TextLoader(path, encoding='utf-8').load()),
            'json': (ElasticsearchIndexer.list_json_files, ElasticsearchIndexer._process_json_file),
            'docx': (ElasticsearchIndexer.list_docx_files, ElasticsearchIndexer._process_docx_file)
        }
        
        for file_type in file_types:
            if file_type not in file_processors:
                continue
                
            list_func, loader_func = file_processors[file_type]
            type_dir = os.path.join(data_dir, file_type)
            files = list_func(type_dir)
            
            for file_path in files:
                stage_name = f"{file_type.upper()}_ì²˜ë¦¬_{os.path.basename(file_path)}"
                hybrid_tracker.track_preprocessing_stage(stage_name)
                
                try:
                    docs = loader_func(file_path)
                    splitter = get_optimized_text_splitter()
                    
                    for doc in docs:
                        doc.metadata.update({
                            "source": file_path,
                            "filename": os.path.basename(file_path),
                            "category": file_type.upper()
                        })
                        
                        # í‘œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ë¶„í• 
                        if "í‘œ" not in doc.page_content:
                            chunks = splitter.split_documents([doc])
                            all_documents.extend(chunks)
                        else:
                            all_documents.append(doc)
                    
                    processed_files += 1
                    
                except Exception as e:
                    print(f"{file_type.upper()} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                finally:
                    hybrid_tracker.end_preprocessing_stage(stage_name)
        
        if not all_documents:
            hybrid_tracker.end_preprocessing_stage("ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘")
            try:
                es_client, success, message = ElasticsearchManager.get_safe_elasticsearch_client()
                cnt = es_client.count(index=INDEX_NAME).get("count", 0) if success else 0
                return True, f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ, í˜„ì¬ ë¬¸ì„œ ìˆ˜: {cnt}"
            except:
                return True, "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì‚­ì œ ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: 0ê°œ"
        
        # Elasticsearchì— ì €ì¥
        result, message = ElasticsearchIndexer._save_to_elasticsearch(
            all_documents, embeddings, hybrid_tracker, "ì „ì²´_íŒŒì¼_ì¸ë±ì‹±_ì‹œì‘", "ì „ì²´"
        )
        
        if result:
            return True, f"ì „ì²´ íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ. ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ, ì´ ë¬¸ì„œ : {message.split('ë¬¸ì„œ ìˆ˜: ')[1] if 'ë¬¸ì„œ ìˆ˜: ' in message else ''}"
        else:
            return False, message
    
    @staticmethod
    def _process_json_file(path: str) -> List[Document]:
        """JSON íŒŒì¼ ì²˜ë¦¬"""
        with open(path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        text_content = ElasticsearchIndexer._extract_text_from_json(json_data)
        if text_content:
            return [Document(page_content=text_content, metadata={})]
        return []
    
    @staticmethod
    def _process_docx_file(path: str) -> List[Document]:
        """DOCX íŒŒì¼ ì²˜ë¦¬"""
        try:
            from utils.docparser import DocParser
            parser = DocParser(path)
            text_content = parser.get_text()
            
            docs = [Document(page_content=text_content, metadata={})]
            
            # í‘œ ë‚´ìš© ì¶”ê°€
            table_blocks = parser.get_tables()
            for table_content in table_blocks:
                docs.append(Document(page_content=table_content, metadata={}))
            
            return docs
        except ImportError:
            return ElasticsearchIndexer._docx_basic_loader(path)
